[
  {
    "id": "1lre90u",
    "title": "Open Source Gemini Data Cleaning CLI Tool",
    "selftext": "We made an open source Gemini data cleaning CLI that uses schematic reasoning to clean and ML prep data at a rate of about **10,000 cells for 10 cents.**\n\n  \n[https://github.com/Mohammad-R-Rashid/dbclean](https://github.com/Mohammad-R-Rashid/dbclean)\n\nor\n\n[dbclean.dev](http://dbclean.dev)  \n  \nYou can follow the docs on github or the website. When we made this tool me made sure to make it SUPER cheap for indie devs.\n\nYou can read more about our logic for making this tool here:\n\n[https://medium.com/@mohammad.rashid7337/heres-what-nobody-tells-you-about-messy-data-31f3bff57d2c](https://medium.com/@mohammad.rashid7337/heres-what-nobody-tells-you-about-messy-data-31f3bff57d2c)  \n",
    "url": "https://www.reddit.com/r/datacleaning/comments/1lre90u/open_source_gemini_data_cleaning_cli_tool/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1751620327.0,
    "author": "16GB_of_ram",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1lre90u/open_source_gemini_data_cleaning_cli_tool/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1lk53bj",
    "title": "Offering Affordable & Accurate Data Cleaning Services | Excel, CSV, Google Sheets, SQL",
    "selftext": "Hey everyone!\n\nI'm offering **reliable and affordable data cleaning services** for anyone looking to clean up messy datasets, fix formatting issues, or prepare data for analysis or reporting.\n\n# üîß What I Can Help With:\n\n* Removing duplicates, blanks, and errors\n* Standardizing column formats (dates, names, numbers, etc.)\n* Data validation and normalization\n* Merging and splitting data columns\n* Cleaning CSV, Excel, Google Sheets, and SQL datasets\n* Preparing data for dashboards or reports\n\n# üõ† Tools & Skills:\n\n* Excel (Advanced functions, Power Query, VBA)\n* Google Sheets\n* SQL (MySQL/PostgreSQL)\n* Python (Pandas, NumPy) ‚Äì if needed for complex cleaning\n\n# üíº Who I Work With:\n\n* Small businesses\n* Researchers\n* Students\n* Freelancers or startups needing fast turnarounds\n\n# üí∞ Rates:\n\n* Flat rate or hourly ‚Äì depends on project size (starting as low as **$10/project**)\n* Free initial assessment of your dataset\n\n# ‚úÖ Why Choose Me?\n\n* Fast turnaround\n* 100% confidentiality\n* Clean, well-documented deliverables\n* Available for one-time or ongoing tasks\n\nIf you‚Äôve got messy data and need it cleaned quickly and professionally, feel free to DM me or drop a comment here. I'm happy to look at your file and provide a free quote.\n\nThanks for reading!  \nLet‚Äôs turn your messy data into clean, useful insights. üöÄ",
    "url": "https://www.reddit.com/r/datacleaning/comments/1lk53bj/offering_affordable_accurate_data_cleaning/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1750856569.0,
    "author": "Every_Value_5692",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1lk53bj/offering_affordable_accurate_data_cleaning/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ldazh2",
    "title": "[D] Why Is Data Processing, Especially Labeling, So Expensive? So Many Contractors Seem Like Scammers",
    "selftext": "",
    "url": "/r/MachineLearning/comments/1ldaof1/d_why_is_data_processing_especially_labeling_so/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1750124817.0,
    "author": "Worried-Variety3397",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1ldazh2/d_why_is_data_processing_especially_labeling_so/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1lbybar",
    "title": "Trying to extract structured info from 2k+ logs (free text) - NLP or regex?",
    "selftext": "I‚Äôve been tasked to ‚Äúautomate/analyse‚Äù part of a backlog issue at work. We‚Äôve got thousands of inspection records from pipeline checks and all the data is written in long free-text notes by inspectors. For example:\n\n> TP14 - pitting 1mm, RWT 6.2mm. GREEN\n> PS6 has scaling, metal to metal contact. ORANGE\n\nThere are over 3000 of these. No structure, no dropdowns, just text. Right now someone has to read each one and manually pull out stuff like the location (TP14, PS6), what type of problem it is (scaling or pitting), how bad it is (GREEN, ORANGE, RED), and then write a recommendation to fix it.\n\nSo far I‚Äôve tried:\n\n- Regex works for ‚ÄúTP\\d+‚Äù and basic stuff but not great when there‚Äôs ranges like ‚ÄúTP2 to TP4‚Äù or multiple mixed items\n\n- spaCy picks up some keywords but not very consistent\n\nMy questions:\n\n1. Am I overthinking this? Should I just use more regex and call it a day?\n\n2. Is there a better way to preprocess these texts before GPT\n\n3. Is it time to cut my losses and just tell them it can't be done (please I wanna solve this)\n\nApologies if I sound dumb, I‚Äôm more of a mechanical background so this whole NLP thing is new territory. Appreciate any advice (or corrections) if I‚Äôm barking up the wrong tree.",
    "url": "https://www.reddit.com/r/datacleaning/comments/1lbybar/trying_to_extract_structured_info_from_2k_logs/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1749987592.0,
    "author": "airgonawt",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1lbybar/trying_to_extract_structured_info_from_2k_logs/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mxwayki",
        "body": "As far as I can see from the example the text you want to extract does have a structure. So you could use regex grouping to structure your pattern (https://www.regular-expressions.info/brackets.html). You can later access the individual groups to access the value. If for example there are multiple options within group you can add these options using the OR operator. If I'm missing an important detail feel free to ask.",
        "score": 1,
        "created_utc": 1749989365.0,
        "author": "tartochehi",
        "is_submitter": false,
        "parent_id": "t3_1lbybar",
        "depth": 0
      },
      {
        "id": "mxwgtir",
        "body": "The most difficult challenge has been distinguishing between a defect description and a recommendation or status update when they share the same keywords.\n \nFor example:\n\n \"As Per\" and Justification Phrases. The phrase \"as per\" was extremely difficult because it can either introduce a purely informational line that should be excluded (e.g., As per review 19/10/2022...) or provide justification within a valid defect description (e.g., ...rejected as per acceptance criteria).\n\nSo that's just one of those things I find hard to clean in the data.",
        "score": 1,
        "created_utc": 1749991909.0,
        "author": "airgonawt",
        "is_submitter": true,
        "parent_id": "t1_mxwayki",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1l4ocj5",
    "title": "Introducing DataPen: Your Free, Secure, and Easy Data Transformation Tool!",
    "selftext": "https://preview.redd.it/m7mcb2ik2a5f1.png?width=2400&format=png&auto=webp&s=f87dbd9fff1558bc054fdea5ec779a7bb9ab3d92\n\n  \nTired of messy CSV files? Data Clean is a **100% free**, web-based app for marketers and data analysts. It helps you clean, map, and transform your data in just 3 simple steps: upload, transform, export.\n\n**What DataPen can do:**\n\n* Remove special characters.\n* Standardize cases.\n* Map old values to new ones.\n* Format dates, numbers, and phone numbers.\n* Find and replace values.\n* Validate de-duplication on columns and remove duplicate rows.\n\nYour data stays **100% secure** on your device; we store nothing. Try DataPen today and simplify your data cleaning process!\n\n[https://datapen.in](https://datapen.in)",
    "url": "https://www.reddit.com/r/datacleaning/comments/1l4ocj5/introducing_datapen_your_free_secure_and_easy/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1749202971.0,
    "author": "santhosh-sivan",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1l4ocj5/introducing_datapen_your_free_secure_and_easy/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1l35cne",
    "title": "Do you also waste hours cleaning Excel files and building dashboards manually?",
    "selftext": "I‚Äôve been working on a side project and I‚Äôd love feedback from people who work with data regularly.\n\n\n\nEvery time I get a client file (Excel or CSV), I end up spending¬†hours¬†on the same stuff: removing duplicates, fixing phone numbers, standardizing columns, applying simple filters‚Ä¶ then trying to extract KPIs or build charts manually.\n\n\n\nI‚Äôm testing an idea for a tool where you upload your file, describe what you want (in plain English), and it¬†cleans the data or builds a dashboard¬†for you automatically using GPT.\n\n\n\nExamples:\n\n‚Äì ‚ÄúRemove rows where email contains ‚Äòtest‚Äô‚Äù\n\n‚Äì ‚ÄúFormat phone numbers to international format‚Äù\n\n‚Äì ‚ÄúShow a bar chart of revenue by region‚Äù\n\n\n\nMy questions:\n\n‚Äì Would this save you time?\n\n‚Äì Would you¬†trust GPT¬†with these kinds of tasks?\n\n‚Äì What feature would be a must-have for you?\n\n\n\nIf this sounds familiar, I‚Äôd love to hear your take. I‚Äôm not selling anything ‚Äì just genuinely trying to see if this is worth building further.",
    "url": "https://i.redd.it/r4ieew11vw4f1.png",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 0,
    "created_utc": 1749043098.0,
    "author": "Nizthracian",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1l35cne/do_you_also_waste_hours_cleaning_excel_files_and/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1kqoke6",
    "title": "Auto-Analyst 3.0‚Ää‚Äî‚ÄäAI Data Scientist. New Web UI and more reliable system",
    "selftext": "",
    "url": "https://medium.com/firebird-technologies/auto-analyst-3-0-ai-data-scientist-new-web-ui-and-more-reliable-system-c194cced2e93",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1747692083.0,
    "author": "phicreative1997",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1kqoke6/autoanalyst_30_ai_data_scientist_new_web_ui_and/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1kmmlme",
    "title": "Looking for a tutor that is proficient in data analysis in particular using pbi",
    "selftext": "Hi there, I‚Äôm looking for someone that could help me understand data analysis as a beginner. Willing to pay for tutoring. ",
    "url": "https://www.reddit.com/r/datacleaning/comments/1kmmlme/looking_for_a_tutor_that_is_proficient_in_data/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1747247228.0,
    "author": "Due_Duck4877",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1kmmlme/looking_for_a_tutor_that_is_proficient_in_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jiuiam",
    "title": "Looking for testers for my AI data cleaning tool that's currently in beta! The tool 1- Identifies naming inconsistencies/abbreviations and converts to a single consistent format and 2- extracts specific data from text strings and converts it to structured, analyzable data.",
    "selftext": "If you have five minutes to spare I'd be so appreciative of the help! Let me know and I'll share the link.",
    "url": "https://www.reddit.com/r/datacleaning/comments/1jiuiam/looking_for_testers_for_my_ai_data_cleaning_tool/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1742832814.0,
    "author": "Good_Guarantee6297",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1jiuiam/looking_for_testers_for_my_ai_data_cleaning_tool/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mkvvd4x",
        "body": "Hey, I'm also building a tool in a similar realm called [DataFlowMapper](https://www.dataflowmapper.com) that's in early access. Mine focuses on visual mapping and transformation logic for CSV/Excel/JSON with AI assistance. Sounds like our tools might be complementary - yours focusing on more standardization, mine on the transformation workflow. I'd be happy to exchange feedback and access since it sounds like we both have some knowledge working with data.",
        "score": 1,
        "created_utc": 1743527480.0,
        "author": "skrufters",
        "is_submitter": false,
        "parent_id": "t3_1jiuiam",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1itrzv6",
    "title": "Preprocessing steps",
    "selftext": "If I have a synthetic dataset for prediction and it contains alot of categorical data what is the suitable way to handle them for a model is one hot encoding a good solution for all of them or I can use model like xgboost or what is the guidelines for preprocessing cycle in this case \nI tried one hot encoding for some , label encoding for other features , imputed nulls with mode , another way I dropped them then tried rf model but the error was high ",
    "url": "https://www.reddit.com/r/datacleaning/comments/1itrzv6/preprocessing_steps/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1740032623.0,
    "author": "itsme5189",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1itrzv6/preprocessing_steps/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ik5ce0",
    "title": "What am I missing?",
    "selftext": "What other data cleaning skills should I work on before applying to jobs? Don‚Äôt hold back, tear this ish down.",
    "url": "https://i.redd.it/pih9ft265she1.jpeg",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1738961441.0,
    "author": "SingerEast1469",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1ik5ce0/what_am_i_missing/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1i0j0vl",
    "title": "Recreating a database from old exports. Can this be cleaned with Python?",
    "selftext": "https://preview.redd.it/oacbjt53jsce1.png?width=1129&format=png&auto=webp&s=660adecf6840abb3c509dc685900d29fbef7e792\n\nI'm recreating an old database from the exported data. Many of the tables have \"dirty\" data. For example, one of the table exports for Descriptions split the description into several lines. There are over 650k lines, so correcting the export manually will take a *very* long time. I've attempted to clean the data with Python, but haven't succeeded. Is there a way to clean this kind of data with Python? And, more importantly, how?! Any tips are greatly appreciated!!",
    "url": "https://www.reddit.com/r/datacleaning/comments/1i0j0vl/recreating_a_database_from_old_exports_can_this/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1736788591.0,
    "author": "keep_ur_temper",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1i0j0vl/recreating_a_database_from_old_exports_can_this/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m6zfi3e",
        "body": "You could try uploading it (or a sample of it) to ChatGPT and ask it to use its code interpreter to clean it? (And/or write you a script to run yourself).",
        "score": 1,
        "created_utc": 1736801774.0,
        "author": "ebullient",
        "is_submitter": false,
        "parent_id": "t3_1i0j0vl",
        "depth": 0
      },
      {
        "id": "mixxlxd",
        "body": "try excel formula,such as: =textjoin(\",\",TRUE,FILTER(D:D,B:B=\"10522\"))",
        "score": 1,
        "created_utc": 1742546120.0,
        "author": "Shoddy-Moose4330",
        "is_submitter": false,
        "parent_id": "t3_1i0j0vl",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1hwnjjt",
    "title": "Is Data Cleaning the Hardest Part of Data Analysis?",
    "selftext": "I've been observing my sister as she works on a data analysis project, and data cleaning is taking up most of her time. She‚Äôs struggling with it, and I‚Äôm curious‚Äîdo you also find data cleaning the hardest part of data analysis? How do you handle the challenges of data cleaning efficiently? or is this a problem for every one",
    "url": "https://www.reddit.com/r/datacleaning/comments/1hwnjjt/is_data_cleaning_the_hardest_part_of_data_analysis/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1736352163.0,
    "author": "ElegantSuccotash7367",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1hwnjjt/is_data_cleaning_the_hardest_part_of_data_analysis/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m62pwu0",
        "body": "I‚Äôm by no means an expert. I‚Äôve done data analysis in grad school and occasionally at work. Data cleaning is always the toughest part. The math maths fine, makes sense, is reasonably automated. \n\nBut for any of that to work, you have to make sure you‚Äôre not putting garbage into your model.",
        "score": 1,
        "created_utc": 1736355528.0,
        "author": "IAmScience",
        "is_submitter": false,
        "parent_id": "t3_1hwnjjt",
        "depth": 0
      },
      {
        "id": "m65jua0",
        "body": "wipe numerous treatment cough aspiring rhythm snow cable gray history\n\n *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*",
        "score": 1,
        "created_utc": 1736386677.0,
        "author": "darrenphillipjones",
        "is_submitter": false,
        "parent_id": "t3_1hwnjjt",
        "depth": 0
      },
      {
        "id": "m6vyos5",
        "body": "Honestly, it's just part of the process, and it can be frustrating, but once you get through it, everything else becomes so much easier, your sister could benefit from breaking things down into smaller tasks or using tools that can speed things up.",
        "score": 1,
        "created_utc": 1736753427.0,
        "author": "AleaIT-Solutions",
        "is_submitter": false,
        "parent_id": "t3_1hwnjjt",
        "depth": 0
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1hl63i9",
    "title": "Expert Data Cleaning Services | Boost Your Data Quality and Accuracy!",
    "selftext": "Is your data messy and incomplete? Let me help you clean it up and transform it into reliable, accurate insights! As a certified Data Analytics expert, I specialize in data cleaning using advanced tools like **Python**, **Excel**, and **Power BI**.\n\nI can help you:\n\n* Remove duplicates and errors\n* Fill missing values\n* Standardize data formats\n* Clean and organize large datasets for analysis\n\nWith my **Data Cleaning** services, you‚Äôll get high-quality data ready for analysis, helping you make smarter business decisions. **Get in touch now** for a free consultation or quote!\n\nContact - [truedatamate@gmail.com](mailto:truedatamate@gmail.com)\n\n\\#DataCleaning #DataAnalytics #Excel #PowerBI #Python #DataTransformation #CleanData #DataInsights #BigData #BusinessIntelligence #DataScience #DataAnalysis #Freelancer #AI #DataExperts #MachineLearning,CleanUpData",
    "url": "https://www.reddit.com/r/datacleaning/comments/1hl63i9/expert_data_cleaning_services_boost_your_data/",
    "score": 0,
    "upvote_ratio": 0.38,
    "num_comments": 0,
    "created_utc": 1735015825.0,
    "author": "Different_Ad_9433",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1hl63i9/expert_data_cleaning_services_boost_your_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hbrum6",
    "title": "Need Help with Mapping Vague Model data(in CSV) to a JSON File with Specific Boat Manufacturers and Models?",
    "selftext": "Hi everyone,\n\nI'm working on a data-cleaning project and need some guidance. I have two datasets:\n\n**Real Data(JSON):** This file contains a structured list of boat manufacturers and their respective models.\n\n\\[Link\\] [drive.google.com/file/d/1G5xL1ruUeZDazGDgM2RzRmctZeJV5ltv/view?usp=drive\\_link](http://drive.google.com/file/d/1G5xL1ruUeZDazGDgM2RzRmctZeJV5ltv/view?usp=drive_link)\n\n**Unmapped Data (CSV):** This file contains less structured and often vague information about boats, including incomplete or inconsistent manufacturer and model details.\n\n\\[Link\\] [drive.google.com/file/d/18yHZztu3P7Rd-rXusdvh2wob2e7Q1vaz/view](http://drive.google.com/file/d/18yHZztu3P7Rd-rXusdvh2wob2e7Q1vaz/view)\n\n**Goal:**  \nI want to map the data in the CSV file to the JSON file as accurately as possible, so I can standardize the vague entries in the CSV to match the structured data in the JSON.\n\n**Challenges:**  \nThe CSV data is inconsistent; manufacturer names might be misspelled, abbreviated, or slightly different from the ones in the JSON.\n\nSome model details in the CSV are partial or unclear.\n\nThere are many entries, so manual mapping isn‚Äôt feasible.\n\n**What I‚Äôve Tried:**  \n\\- Experimenting with fuzzy string matching (fuzzywuzzy or rapidfuzz libraries).  \n\\- Looking for exact matches but finding the results too limited.\n\n**What I Need Help With:**  \n\\- What‚Äôs the best approach to clean and map this data programmatically?  \n\\- Are there any specific tools, libraries, or techniques that can handle such mapping efficiently?  \n\\- Any advice on dealing with edge cases, like multiple possible matches or missing data?\n\nI‚Äôd appreciate any insights, code snippets, or resources that could help me solve this problem.\n\nThanks in advance!",
    "url": "https://www.reddit.com/r/datacleaning/comments/1hbrum6/need_help_with_mapping_vague_model_datain_csv_to/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1733918227.0,
    "author": "urbangareeb_in",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1hbrum6/need_help_with_mapping_vague_model_datain_csv_to/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mixz005",
        "body": "It is recommended to use LLMs such as GPT, deepseek, or Claude, provided you have some coding skills to implement loop calls for LLMs. Additionally, including examples in your prompt can help the LLM understand better.",
        "score": 1,
        "created_utc": 1742547017.0,
        "author": "Shoddy-Moose4330",
        "is_submitter": false,
        "parent_id": "t3_1hbrum6",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1gu4voi",
    "title": "ow can i implement a new  lemmatizing function from scratch ",
    "selftext": "hello good people i am a student at computer science  engineering and i have homework at  data retrieval field \n\nusing Python and i am not   that much with this kind of programming language   \n\nbut the main thing i want to say is  how I should implement a steeming function from scratch   without using nltk library  because my doctor wants us to build it   in the homework could anyone tell me where should i start and what I should do i searched everywhere in the google and   with no benefits  everything talks about the function in the nltk library \n\nwhat should i do?\n\nthanks for any help   \n\nsorry for my bad  English ",
    "url": "https://www.reddit.com/r/datacleaning/comments/1gu4voi/ow_can_i_implement_a_new_lemmatizing_function/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 0,
    "created_utc": 1731937100.0,
    "author": "QusayAbozed",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1gu4voi/ow_can_i_implement_a_new_lemmatizing_function/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gkvxha",
    "title": "DATA CLEANING HELP",
    "selftext": "Ive just started DATA SCIENCE. Like ive done Numpy, Pandas, Seaborn, Sklearn and some other libraries... and ive also done Machine learning(learned algos). And now i wanna start doing project. Whenever i sit to do project, i get stuck by DATA CLEANING PROCESS! So, anyone could you share how to go ahead in this situation, if youve any good resource related to data cleaning please help me with that too...! THANKS!",
    "url": "https://www.reddit.com/r/datacleaning/comments/1gkvxha/data_cleaning_help/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1730890902.0,
    "author": "Wrong_Today_7855",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1gkvxha/data_cleaning_help/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lw49tjo",
        "body": "What kind of data do you want to clean exactly?\nThat would help advising you.",
        "score": 1,
        "created_utc": 1731089707.0,
        "author": "SMTP-Service_net",
        "is_submitter": false,
        "parent_id": "t3_1gkvxha",
        "depth": 0
      },
      {
        "id": "lyc93b7",
        "body": "What issues are you facing specifically? There's a whole lot of variety within data cleaning.",
        "score": 1,
        "created_utc": 1732236882.0,
        "author": "Impressive_Run8512",
        "is_submitter": false,
        "parent_id": "t3_1gkvxha",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1gd5q86",
    "title": "Need a mentor ",
    "selftext": "Hi guys! Urgent need a mentor who can give me tasks from Data cleaning to visualization. I never studied data analytics formely, just studied from YouTube. Need help, I am counting on this reddit community.",
    "url": "https://www.reddit.com/r/datacleaning/comments/1gd5q86/need_a_mentor/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 0,
    "created_utc": 1730014689.0,
    "author": "Turbulent_Way_87",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1gd5q86/need_a_mentor/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gc1pzs",
    "title": "Tips cleaning this dictionary?",
    "selftext": "I don't know if this is the right place for this but I need help cleaning this old dictionary, it is the only dictionary my native language has as of now. I want to make an app from it.\n\nI discovered this pdf from an internet Archive as I had been looking for it for a while. This seems to be a digitized version of the physical copy.\n\nThe text can be copied but one letter doesn't copy properly, it is mistaken for other letters like V and U, which is the ∆≤ letter I have pointed an arrow to. These days that letter is written with a ≈¥.\n\nThe dictionary goes from Tumbuka to Tonga to English and then flips at some point to go from English to Tonga to Tumbuka.\n\nI only want the Tumbuka to English pairs and vice-versa ignoring the Tonga so I make a mobile app more easily.\n\nHere is a link to the [dictionary](https://drive.google.com/file/d/1oNds1W4f_duYN3E24Qly_q6hpJbmJpI5/view?usp=drivesdk)",
    "url": "https://i.redd.it/9nvctllf8ywd1.jpeg",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1729882371.0,
    "author": "DangoLawaka",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1gc1pzs/tips_cleaning_this_dictionary/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "ltqhnlp",
        "body": "Great question.  Your best bet will be to re-do the OCR with a custom engine that can be trained to recognize the non-latin characters.  Then hopefully you won't have to do any data cleaning at all.\n\nOCR isn't really my field, but I believe it's fairly straightforward to create custom training data for `pytesseract`.  Be warned that you're in for a fair bit of work.  Like anything involving fine tuning of python libraries, this is going to be more like programming than you're probably hoping.  There may be other point-and-click OCR engines that can be taught new characters, but as I say, it's not my field.",
        "score": 1,
        "created_utc": 1729885160.0,
        "author": "DSJustice",
        "is_submitter": false,
        "parent_id": "t3_1gc1pzs",
        "depth": 0
      },
      {
        "id": "ltqi9vh",
        "body": "I was trying to avoid too much work but maybe I have no choice then, I'll look into it, thanks!",
        "score": 1,
        "created_utc": 1729885354.0,
        "author": "DangoLawaka",
        "is_submitter": true,
        "parent_id": "t1_ltqhnlp",
        "depth": 1
      },
      {
        "id": "ltqj8f5",
        "body": "If you're not a python coder, perhaps it's worth checking other ocr solutions to see which ones are trainable.  Collecting and cropping a few examples of your custom characters shouldn't actually be *that* much work.  :-)",
        "score": 1,
        "created_utc": 1729885654.0,
        "author": "DSJustice",
        "is_submitter": false,
        "parent_id": "t1_ltqi9vh",
        "depth": 2
      },
      {
        "id": "ltqkkaq",
        "body": "I am quite comfortable with python actually, it was my language in school. So I'll give it a go before trying the other solutions",
        "score": 2,
        "created_utc": 1729886067.0,
        "author": "DangoLawaka",
        "is_submitter": true,
        "parent_id": "t1_ltqj8f5",
        "depth": 3
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "1gb2jyf",
    "title": "FREE email data cleaning (no catch)",
    "selftext": "Hi all,\n\nIt‚Äôs time for us to give back to the Reddit communities we love so much.\n\nNormally when creating an account on [Listcleaner.net](http://Listcleaner.net) you get 100 free cleaning credits to try our¬†[email cleaning service](https://listcleaner.net/).\n\nRight now we want to give 25 users of the r/datacleaning subreddit not 100, but 1000 credits to clean your email data, when creating an account.\n\nYou DO NOT have to buy anything, and the only contact information required to create your account on [Listcleaner.net](http://Listcleaner.net) is your email address.\n\nAfter creating an account, please tell us via DM your Listcleaner accounts username or email address and we will add the credits to your account.\n\nThe credits can be used on our website and via our API.\n\nHappy email cleaning!  \nThe¬†[Listcleaner.net](https://listcleaner.net/)¬†team\n\n",
    "url": "https://www.reddit.com/r/datacleaning/comments/1gb2jyf/free_email_data_cleaning_no_catch/",
    "score": 0,
    "upvote_ratio": 0.33,
    "num_comments": 1,
    "created_utc": 1729776276.0,
    "author": "SMTP-Service_net",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1gb2jyf/free_email_data_cleaning_no_catch/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mjt87bu",
        "body": "Hi, DM sent with email. Thx.",
        "score": 1,
        "created_utc": 1742985767.0,
        "author": "Key-Put2663",
        "is_submitter": false,
        "parent_id": "t3_1gb2jyf",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1fv2ce9",
    "title": "formating the dates with pandas",
    "selftext": "[how would you format theese dates with python and pandas I really could not do it](https://preview.redd.it/2s1h8tblvhsd1.png?width=205&format=png&auto=webp&s=5b1d2c5e9f683f69099c865a280df4636d4dd102)\n\n",
    "url": "https://www.reddit.com/r/datacleaning/comments/1fv2ce9/formating_the_dates_with_pandas/",
    "score": 0,
    "upvote_ratio": 0.33,
    "num_comments": 2,
    "created_utc": 1727940993.0,
    "author": "UNdefinedUSEr4",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1fv2ce9/formating_the_dates_with_pandas/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lq4c089",
        "body": "A really short drive through Google land [led me to this](https://towardsdatascience.com/clean-a-messy-date-column-with-mixed-formats-in-pandas-1a88808edbf7).",
        "score": 1,
        "created_utc": 1727954656.0,
        "author": "andartico",
        "is_submitter": false,
        "parent_id": "t3_1fv2ce9",
        "depth": 0
      },
      {
        "id": "lqdk6qa",
        "body": "Here‚Äôs my train of thought\n\n1. If the entry contains a string value. You could make a column that checks if the value is translatable to a numeric integer. If yes, it contains a string. If not, it is an integer like ISO date or unix epoch timestamp.\n\n2. Now that you have strings vs numbers, you can then break down reading each type. Eg, for numbers, you can make a new column that tells you if the date starts with the same few numbers as our unix epoch (from, say, the year 2000 onwards). If it starts with those digits, parse as unix epoch. If not, parse as something else\n\n3. If it‚Äôs a numeric and not a unix epoch, parse as ISO YYYYMMDD first, see if it makes sense, and then try other ISOs like MMDDYYYY, etc.. now you‚Äôve handled all the numbers, the potentially more difficult part will be strings.\n\n4. For the columns that are strings, maybe use a function to find out the most common order of character types, such as day-month-year, month-day-year, etc.. and use the frequencies of those to prioritise how you parse. Even a simple rule such as (Python pseudocode):\n\nDef parsing_function(input):\n\nTry:\n    Output= parse(input, as_MMDDYYYY)\n    If Output > 2025: # this is probably wrong\n        #try something else, etc etc\n\n\nThe point is you use your knowledge about the context of the dataset to figure out some soft rules that will help you deal with the bulk of the data, and handle the outliers and errors later. What you want is to get started, and to do so you should find the biggest and most obvious thing you can work on and worry about the small details and errors later",
        "score": 1,
        "created_utc": 1728082725.0,
        "author": "teetaps",
        "is_submitter": false,
        "parent_id": "t3_1fv2ce9",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1fcuvg2",
    "title": "How do y'all find datasets for cleaning practice? ",
    "selftext": "I've been trying to find datasets to practice my cleaning skills and I find datasets already clean. \nAlso if there's a way to find datasets to clean above a million rows that'll be so helpful! ",
    "url": "https://www.reddit.com/r/datacleaning/comments/1fcuvg2/how_do_yall_find_datasets_for_cleaning_practice/",
    "score": 5,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1725902299.0,
    "author": "Lunatic_Duck",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1fcuvg2/how_do_yall_find_datasets_for_cleaning_practice/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lmrbkel",
        "body": "kaggle may have something for you",
        "score": 1,
        "created_utc": 1726145726.0,
        "author": "Environmental_Ad5755",
        "is_submitter": false,
        "parent_id": "t3_1fcuvg2",
        "depth": 0
      },
      {
        "id": "lqnmot6",
        "body": "I think these websites might be helpful to you  \nKaggle\n\nGoogle Cloud Public Datasets\n\nOpenStreetMap (OSM) Data",
        "score": 1,
        "created_utc": 1728239864.0,
        "author": "Educational-Long-468",
        "is_submitter": false,
        "parent_id": "t3_1fcuvg2",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1fcvbi2",
    "title": "what's the most common dirty data problem?",
    "selftext": "when working with dirty data, what data issues have you run into the most? what's important to look out for? do your tools look out for these things or do you have to manually build out these checks? ",
    "url": "https://www.reddit.com/r/datacleaning/comments/1fcvbi2/whats_the_most_common_dirty_data_problem/",
    "score": 0,
    "upvote_ratio": 0.33,
    "num_comments": 6,
    "created_utc": 1725903416.0,
    "author": "Less_Big6922",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1fcvbi2/whats_the_most_common_dirty_data_problem/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lmdmani",
        "body": "Product data‚Ä¶ the SKUs being duplicated or referring to the wrong product, lack of metadata, insufficient documentation from ERP and a general sense of panic and dread that makes me wake up at 3am with acid reflux and wondering why I don‚Äôt dedicate the rest of my professional life to selling strawberry jam instead of dealing with this insanity day after day after day.  \nTools I use: Alcohol and cannabis.  \nThings I do: Look, nobody is gonna pay for you this or give you a medal or anything line that‚Ä¶BUT the only thing that has ever worked for me consistently is to show up at the front counter, order a bunch or random stuff, then track my receipt down in the DWH to match reality to data rows. It has saved me so much heartache and done wonders to sanitize the data.",
        "score": 4,
        "created_utc": 1725934551.0,
        "author": "alexmrv",
        "is_submitter": false,
        "parent_id": "t3_1fcvbi2",
        "depth": 0
      },
      {
        "id": "lmxv8y9",
        "body": "The people creating the data don't have standards - there's no requirements for creating a part in part master, there's no formula for how a part is named. There's no guideline on how a part is categorized, so budgeting, timing, inventory, and procurement all have bad data.\n\nBiggest problem - we can't make mass upload changes. It all has to be done manually. We can manually select a bunch of parts that are all getting the same change, but I can't upload something and do a mass change. So it's awful to do any cleaning. And we're limited because of regulations, not capabilities, so it's not gonna change",
        "score": 2,
        "created_utc": 1726240671.0,
        "author": "cait_Cat",
        "is_submitter": false,
        "parent_id": "t3_1fcvbi2",
        "depth": 0
      },
      {
        "id": "lnqmzeq",
        "body": "so true",
        "score": 1,
        "created_utc": 1726670248.0,
        "author": "Less_Big6922",
        "is_submitter": true,
        "parent_id": "t1_lmdmani",
        "depth": 1
      },
      {
        "id": "lnqnkgv",
        "body": "and I agree, that's the most practical and effective way to deal with that",
        "score": 1,
        "created_utc": 1726670441.0,
        "author": "Less_Big6922",
        "is_submitter": true,
        "parent_id": "t1_lmdmani",
        "depth": 1
      },
      {
        "id": "lnqnd01",
        "body": "I empathize",
        "score": 1,
        "created_utc": 1726670373.0,
        "author": "Less_Big6922",
        "is_submitter": true,
        "parent_id": "t1_lmxv8y9",
        "depth": 1
      }
    ],
    "comments_extracted": 5
  },
  {
    "id": "1eeius6",
    "title": "Tool to write data cleaning scripts in python from natural language. Thoughts & feedback? (Roasting is accepted & appreciated here)",
    "selftext": "",
    "url": "https://v.redd.it/rcsml8gdzbfd1",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1722203732.0,
    "author": "redrred753",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1eeius6/tool_to_write_data_cleaning_scripts_in_python/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ee9j0r",
    "title": "Need Help, Suggestions, and Feedbacks",
    "selftext": "Hi Guys,\n\nLet's keep it short, \n\nI want to learn data cleaning using Power Query/Power Bi and Pandas (Python)\n\nBut the problem is that I've no mentor or someone who can check my cleaned and processed data. Like I don't even know if I am cleaning the data appropriately or not.\n\nPlease tell me guys how this subreddit can be helpful.\n\nPlease help. I'm desperate for help!",
    "url": "https://www.reddit.com/r/datacleaning/comments/1ee9j0r/need_help_suggestions_and_feedbacks/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1722179174.0,
    "author": "Turbulent_Way_87",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1ee9j0r/need_help_suggestions_and_feedbacks/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lkhe52m",
        "body": "[removed]",
        "score": 1,
        "created_utc": 1724929042.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_1ee9j0r",
        "depth": 0
      },
      {
        "id": "lsedlyu",
        "body": "Thanks man!",
        "score": 1,
        "created_utc": 1729188002.0,
        "author": "Turbulent_Way_87",
        "is_submitter": true,
        "parent_id": "t1_lkhe52m",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1dx6eg0",
    "title": "[PROMO] The NASA Breath Diagnostics Challenge - $55,000 in prizes",
    "selftext": "[https://bitgrit.net/competition/22](https://bitgrit.net/competition/22)  \n  \nThe challenge tasks solvers to leverage their expertise to develop a classification model that can accurately discriminate between the breath of COVID-positive and COVID-negative individuals, using existing data. The ultimate goal is to improve the accuracy of the NASA E-Nose device as a potential clinical tool that would provide diagnostic results based on the molecular composition of human breath",
    "url": "https://www.reddit.com/r/datacleaning/comments/1dx6eg0/promo_the_nasa_breath_diagnostics_challenge_55000/",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 0,
    "created_utc": 1720320458.0,
    "author": "bitgrit_Team",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1dx6eg0/promo_the_nasa_breath_diagnostics_challenge_55000/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1d1pg6i",
    "title": "Cleaning rows with typos",
    "selftext": "I have a table in Excel filled with typos. For example:\nRow1: obi LLC, US, SC, 29418, Charlestone, id5\nRow2: obi company, US, SC, 29418, Charlestone, id4\nRow3: obi gmbh, US, SC, 29418, Charlestone, id3\nRow4: obi, US, SC, 29418, Charlestone, id2\nRow5: Obi LLC, US, SC, 59418, Charlestone, id1\nRow6: Starbucks, US, SC, 1111, Budapest, id9\nRow7: Starbucks kft, HU, BP, 1111, Budapest, id8\nRow8: Starbucks, HU, BP, 1111, Budapest, id7\n\nThe correct rows here are row1 and row8 because their values occur most frequently in the table. I want to create a new table with only the correct directions. The expectation is to assign the standardized value to each row based on its relationship. It's important to consider not only the name but also the name/country/state/zip code/city combination.\nFuzzy matching wouldn't work, because I don't have a list with the correct data.\nI initially tried using VBA, but I only managed to list the one row that occurred most frequently (in this case row 1). I can copy my code if necessary.\nHave you ever cleaned such messy data? What would you recommend?\nThank you for your advice ",
    "url": "https://www.reddit.com/r/datacleaning/comments/1d1pg6i/cleaning_rows_with_typos/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1716809637.0,
    "author": "Dorixix",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1d1pg6i/cleaning_rows_with_typos/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "l5wjl62",
        "body": "https://towardsdatascience.com/embedding-for-spelling-correction-92c93f835d79",
        "score": 2,
        "created_utc": 1716828408.0,
        "author": "emcbride09",
        "is_submitter": false,
        "parent_id": "t3_1d1pg6i",
        "depth": 0
      },
      {
        "id": "l5x1uyf",
        "body": "Using python, I would print a list of all the values that didn‚Äôt match the accepted ones to get a good look at what I was working with. Then do some type of mapping function to give them all the right values. Could be very tedious based on what I see there, but oh so satisfying when completed!",
        "score": 2,
        "created_utc": 1716835145.0,
        "author": "BscCS",
        "is_submitter": false,
        "parent_id": "t3_1d1pg6i",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1coy0fb",
    "title": "I was so tired of cleaning crappy data, so I made a tool",
    "selftext": "Hey guys, I think this might be very relevant in this sub. Lately, I was working on a tool to clean any textual data. In a nutshell it can convert inconsistent data like this (see all names are different and hard to analyse):\n\n[See first column](https://preview.redd.it/zoxorz73inzc1.png?width=1224&format=png&auto=webp&s=e9b951fb893bdbc15cb6d6e65340f64f78cdb8cb)\n\nInto something like this:\n\n[See generated columns](https://preview.redd.it/9cwsmsm4inzc1.png?width=2258&format=png&auto=webp&s=545b0347c1609773204198a0d28801ab9dca134d)\n\n[https://data-cleaning.com](https://data-cleaning.com)  \n  \nI'm actively looking for feedback and whether this meets someones needs / needs to be changed for your specific case. Please let me know what you think!",
    "url": "https://www.reddit.com/r/datacleaning/comments/1coy0fb/i_was_so_tired_of_cleaning_crappy_data_so_i_made/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1715369593.0,
    "author": "Ok_Maize_3709",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1coy0fb/i_was_so_tired_of_cleaning_crappy_data_so_i_made/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1cibbfh",
    "title": "help how to organize this column ? ",
    "selftext": "I have a column named ' informations ' and it has the information of used cars, and this column has an attribute and her value seperated by a comma ( , ) but in the same cell i have multiple attribute and the values like this one : \n\n,Puissance fiscale,4,Bo√Æte de vitesse,Manuelle,Carburant,Essence,Ann√©e,2013,Kilom√©trage,120000,Model,I20,Couleur,bleu,Marque de voiture,Hyundai,Cylindr√©e,1.2\n\nas you can that is a single cell ine the 1st line in the column named informations \n\nPuissance fiscale has 4 as a value   \nboite de vitesse has manuelle as a value   \nETC \n\nNB: i have around 9000 line and not everyline have the same structure as this ",
    "url": "https://www.reddit.com/r/datacleaning/comments/1cibbfh/help_how_to_organize_this_column/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 5,
    "created_utc": 1714642054.0,
    "author": "Environmental_Ad5755",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1cibbfh/help_how_to_organize_this_column/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "l281b0k",
        "body": "What are you using to clean the data, what format are you expecting? You only mention this column in fetail, but assuming you are going to produce a json file, then the column informations can be transformed into a dict with key values\n\n{\n\n\"Informations\":\n\n{\n\"Something in french\": 4,\n\"Some other thing\": \"value\"\nEtc\n}\n}",
        "score": 1,
        "created_utc": 1714644097.0,
        "author": "lrojas",
        "is_submitter": false,
        "parent_id": "t3_1cibbfh",
        "depth": 0
      },
      {
        "id": "lqnn9mf",
        "body": "To organize your 'informations' column in Pandas, you can split the attributes and values within each cell using a regular expression, ensuring that attributes are correctly paired with their values. First, load the dataset, and define a function to split the data by commas, then convert it into a dictionary where each attribute is a key and its corresponding value is the dictionary value. You can then expand this dictionary into separate columns using `pd.DataFrame()`, ensuring that each attribute becomes a column. This method allows you to handle different structures across rows and organizes your data for easier analysis.",
        "score": 1,
        "created_utc": 1728240048.0,
        "author": "Educational-Long-468",
        "is_submitter": false,
        "parent_id": "t3_1cibbfh",
        "depth": 0
      },
      {
        "id": "l28388r",
        "body": "The data is scrapped with python selenium and now they are in a local data base ( postgresql ) but for this manipulation i extracted them in a csv untill i figure out how to do it in a csv then i will connect ti the data base to organize them there. And yes key values in a dict that's the idea",
        "score": 1,
        "created_utc": 1714645389.0,
        "author": "Environmental_Ad5755",
        "is_submitter": true,
        "parent_id": "t1_l281b0k",
        "depth": 1
      },
      {
        "id": "l2839gc",
        "body": "Btw thank's for replying üôèüôè",
        "score": 1,
        "created_utc": 1714645410.0,
        "author": "Environmental_Ad5755",
        "is_submitter": true,
        "parent_id": "t1_l281b0k",
        "depth": 1
      },
      {
        "id": "l283c4c",
        "body": "No problem",
        "score": 1,
        "created_utc": 1714645458.0,
        "author": "lrojas",
        "is_submitter": false,
        "parent_id": "t1_l2839gc",
        "depth": 2
      }
    ],
    "comments_extracted": 5
  },
  {
    "id": "1ci7mjm",
    "title": "Decoding data classification: A simplified yet comprehensive handbook",
    "selftext": " In today's data-driven world, where data breaches are a constant threat, safeguarding your organization's sensitive information is paramount. Learn how to implement robust data classification processes and explore top tools for securing your data from our blog.  \n\n\nExplore now: https://www.infovision.com/blog/decoding-data-classification-simplified-yet-comprehensive-handbook  \n\n\n[\\#CyberThreats](https://www.instagram.com/explore/tags/cyberthreats/)  \n[\\#DataClassification](https://www.instagram.com/explore/tags/dataclassification/)  \n[\\#DataBreaches](https://www.instagram.com/explore/tags/databreaches/) ",
    "url": "https://www.reddit.com/r/datacleaning/comments/1ci7mjm/decoding_data_classification_a_simplified_yet/",
    "score": 2,
    "upvote_ratio": 0.75,
    "num_comments": 0,
    "created_utc": 1714627134.0,
    "author": "InfoVisioninc1",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1ci7mjm/decoding_data_classification_a_simplified_yet/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1bxhyfz",
    "title": "What does it imply when the total cost is negative, the unit selling price is positive and the order is 0? I am trying to clean data in Excel.",
    "selftext": "ORDER QUANTITY | UNIT SELLING PRICE| TOTAL COST\n\n0 | 151.47 | -86.9076\n\n0 | 690.89 | -1002.1401\n\n0 | 822.75 | -978.8337\n\nI am trying to clean a dataset and wanted to understand if it makes sense or if I should delete it from the table. There are about 28% of total entries with such data. It won't make sense to delete 28% either. Please drop your suggestions and understanding.",
    "url": "https://www.reddit.com/r/datacleaning/comments/1bxhyfz/what_does_it_imply_when_the_total_cost_is/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 0,
    "created_utc": 1712426669.0,
    "author": "ObjectiveSure999",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1bxhyfz/what_does_it_imply_when_the_total_cost_is/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1blii8d",
    "title": "Pricing Inquiry for Data Cleaning and Analysis Service with Databricks and PySpark Expertise",
    "selftext": " \n\nHello,\n\nI'm currently exploring options for professional data cleaning and analysis services, particularly those utilizing Databricks and PySpark expertise. I have a dataset that requires thorough cleaning to address inconsistencies and erroneous data, followed by in-depth analysis to extract valuable insights for my business.\n\nHere's a breakdown of the tasks I'm looking to outsource:\n\n1. **Initial Evaluation:** Assessing my dataset to identify data quality issues.\n2. **Data Cleaning:** Applying advanced data cleaning techniques to rectify inconsistencies and erroneous data.\n3. **Databricks Analysis:** Utilizing Databricks for large-scale data analysis, optimizing processing performance.\n4. **PySpark Development:** Writing PySpark scripts for efficient processing and analysis of distributed data.\n5. **Reporting and Insights:** Generating detailed reports and providing insights based on the analysis performed.\n6. **Continuous Optimization:** Recommending strategies for ongoing improvement of data quality and analysis processes.\n\nI understand that the cost of such services can vary depending on factors such as the complexity of the dataset, the volume of data, and the specific requirements of the analysis. However, I would appreciate any ballpark estimates or insights from forum members who have experience with similar projects.\n\nAdditionally, if you have recommendations for reputable service providers or consultants specializing in data cleaning and analysis with Databricks and PySpark, please feel free to share them.\n\nThank you in advance for your assistance!",
    "url": "https://www.reddit.com/r/datacleaning/comments/1blii8d/pricing_inquiry_for_data_cleaning_and_analysis/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1711164403.0,
    "author": "Worldly-Ad-7344",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1blii8d/pricing_inquiry_for_data_cleaning_and_analysis/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lkgdkva",
        "body": "Hi. Are you still looking for assistance with this?",
        "score": 1,
        "created_utc": 1724906613.0,
        "author": "Ok_Painting7856",
        "is_submitter": false,
        "parent_id": "t3_1blii8d",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1b97d2k",
    "title": "Cleaning header/footer text from OCR data",
    "selftext": "Hello! I have a collection of OCR text from about a million journal articles and would appreciate any input on how I can best clean it.\n\nFirst, a bit about the format of the data: each article is stored as an array of strings where each string is the OCR output for each page of the article. The goal is to have a single large string for each article, but before concatenating the strings in these arrays, some cleaning needs to be done at the start and end of each string. Because we're talking about raw OCR output, and many journals have things like journal titles, page numbers, article titles, author names, etc. at the top and/or bottom of each page, and those have to be removed first. \n\nThe real problem, however, is that there is just so much variation in how journals do this. For example, some alternate between journal title and article tile at the top of each page with page numbers at the bottom, some alternate between page numbers being at the top and the bottom of each page, and the list goes on. (So far, I've identified 10 different patterns just from examining 20 arrays.) This is further complicated by most articles having different first and sometimes last pages, tables and captions, etc.\n\nAt this point, I could keep going to identify patterns, write some regex to detect what pattern is present, then clean accordingly. But I also wonder if there's a more general approach, like searching for some kind of regularity, either across pages or (more commonly) every other page, but I'm not quite sure how I should approach this task.\n\nAny suggestions would be greatly appreciated!",
    "url": "https://www.reddit.com/r/datacleaning/comments/1b97d2k/cleaning_headerfooter_text_from_ocr_data/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1709850259.0,
    "author": "z18782",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1b97d2k/cleaning_headerfooter_text_from_ocr_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "ktu4i08",
        "body": "Here are some concrete examples of articles (with large chunks cut out because the middle stuff is less important):\n\n    # article title in caps followed by page number at the top of odd pages and page number followed by journal title in caps at the top of even pages, footnotes in bottom\n    article_1 = [\n       'AGRICULTURAL PRODUCTION IN CHINA Albert La Fleur and Edwin J. Foscue Economic Geographers, Clark University IT has been estimated that one may find over 4,000 people to the square mile in some of the most densely populated agricultural regions of China. ...... In view of the fact that China proper contains many mountainous areas, and I\"China: Land of Famine,\" W. H. Mallory, Amer. Geog. Soc., Spec. Pub., No. 6, 1926. p. 15. 2 Data dealing with Land Utilization obtained from an unpublished manuscript, loaned by Dr. 0. E. Baker.',\n       '298 EcONOMic GEOGRAPHY At: Chna (Ma coyrghe byAbr aFluEwnJ.Fs ,ad .E ae. IC- POPULATION EACH DOT REPRESENTS 25.000 PEOPLE 0 00 200 300 400 FIGURE I.-The population of China Proper and Manchuria according to the Post Office estimates for 1922 was approximately 437 million people. ....... The area of cul- tivated land per person in the Chinese Republic was roughly 0.40 acres, but',\n       \"AGRICULTURAL PRODUCTION IN CHINA 299 CHINA S :' > (N COMPARED -W' .TH UNITED STATES IN AREA AND LATITUDE * .. I:CC aT .. FIGURE 2.-China compared with the United States in area and latitude. this includes the sparsely populated provinces of Manchuria, Mongolia, and Sinkiang. ...... Only about one-fourth of the arable land is at present under cultivation. (Based on preliminary estimates made by 0. E. Baker.)\",\n       '300 ECONOMIC GEOGRAPHY / .. CULTIVATED LAND EACH DOT REPRESENTS 0.000 ACRES C, 00 200 300 400 FIGURE 4.-The area of cultivated land in China Proper and Manchuria was about 180 million acres in 1918. ...... The ability to compete with',\n       'AGRICULTURAL PRODUCTION IN CHINA 301 KIR \"I ~~7 ~ 2 ~~~SHANTUN )SZECHWAN %,~N El IANGSU M| 41 HUPEMHH k O ) `YSKWEICHOA HUNAN _ EKIANG YUNNAN 67 . 7 i2GKANGSI , WANGS DENTIFICATION MAP 7 ACRES= _PEFR.-PEOrLE ANGTUNG AVERGE FR CHA PROPER * S co A L E 2 5 .(- FIGURE 5.-Identification map and utilization of the land. The acres per farm, acres per capita, and people per farm are given for each province. (Preliminary estimates only.) ...... Approximately three-fourths of the cultivated land of China is occupied by the three major food crops-rice, wheat, and the sorghums-millets. (Based on preliminary esti- mates.)',\n       '302 ECONOMIC GEOGRAPHY 4:: _1. 4 l~---r-|r1 -11. I . 1 -\\'> \\'- /~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~/ ib L,,-_i I ,rV~~>sV \\'7\\',\\': \"r I =a~a >_t M*1 *-S,- \\':*,exIM > t s * , M I L E S ( g \\' > ttsERT \\' , ,-?S 0. E 4 ,-h~~~~~~~~~~r ItS ~ ~ ~ ~ :1PR c/b~~~~~~~~~~~~~~~1 MILES EOW.. J~~:. \\'. \\'\\' WE 0. . Baker. ...... China produces less wheat, but more sorghums and millets, than the United States.',\n       'AGRICULTURAL PRODUCTION IN CHINA 303 ~~~~~~~. I~~~~~~~~~~~~~~~~V I,~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~5 ow -? y-\\'$1 Ld 00* 1, .*. \"*~~~~~~~~~~~~~~J (?A * .. ~WHEAT 1918 A \\'., EACH DOT REPRESENTS 10,000 ACRES o 100 200 =_300= 400 MILES ooEo L. LEU FIGURE 9.-While rice is concentrated in the south, wheat is found chiefly in the less humid north- ern provinces. ...... The cotton crop is grown in the provinces of Chihli, Shantung, Kiangsu, Hupeh, Shansi, and Shensi with lesser amounts in several other provinces (Fig. 11). Women, in general, take care',\n       '304 ECONOMIC GEOGRAPHY law~~~~~~~~~~~~~~~~~~~~~~~O -~~~~~~~~ ~EC tDoOT REPRESENTS l ,0 , loo zoo 3eo <00 S ) ( ]~~~~~~0,00 A RE 6A LE PREPED. 0.E.0 . E FIGURE 10.-Sorghums and millets are grown chiefly in the northeastern provinces and in Man- churia. ...... The centers of greatest density are found in northern Chihli and in Manchuria (Fig. 12). In the',\n       ......]\n\n    # no headers, page numbers at the bottom of each page with journal title in caps after page number on even pages\n    article_2 = [\n       \"1992-1993 Special Interest Group Annual Directory The following is a list of Special Interest Groups currently active in the Association. ...... Contact: Michael J. Brody, 935 NW 35th St., Corvallis, OR 97330. JUNE-JULY 1992 41\",\n       'Economics Education Purpose: To disseminate research findings on the teaching and learning of economics, K-Adult and to strengthen the disciplinary ties between educa- tion research on economics education. ...... Middle-Level Education Purpose: To improve, promote, and disseminate educational research reflec- 42 EDUCATIONAL RESEARCHER',\n       \"ting early adolescence and middle-level education. ...... Dues: 54 members; S2 students. Contact. Norma Norris, Educational Testing Service, 18-T Rosedale Rd., Princeton, NJ 08541. JUNE-JULY 1992 43\",\n       'Research Utilization Purpose: To understand how research is utilized to improve education policy and practice. ...... Contact Alexander Friedlander, Department of Humanities/Communication, Drexel University, 32nd and Chestnut, Philadelphia, PA 19104. 44 EDUCATIONAL RESEARCHER',\n       ......]\n\n    # page numbers alternating at the top and bottom of each page\n    article_3 = [\n       '19th CENTURY MECHANICAL SYSTEM DESIGNS Robert Brucemann and Donald Prowler teach courses in art history and environmental con- trols, respectively, at the Graduate School of Fine Arts, University of Pennsylvania. ...... While some architects worked with their new colleagues, a sizeable number 11',\n       '12 instead renounced all responsibility in the matter and retreated into the \"art\" aspect of their work. ...... The most notable are the excellent chapters in John Hix, The Glass House, London, 1974; Jennifer Tann, The Development of the Factory, London, 1970; and Mark Girouard, The Victorian Country House, Oxford, 1971.',\n       '2 Hotel Continental, Paris. Section showing heating and ventilation installation by Geneste and Herscher, engineers, of Paris. ...... i#l~lll iii 13',\n       '14 oC \\'+ 4 -.. ? , .,. 4 Henry Ruttan\\'s scheme for a house which could be efficiently heated and ventilated, ...... From J C Loudon. An Encyclopedia of Cottage Farm and Villa Architecture London, 1833.',\n       ......]",
        "score": 1,
        "created_utc": 1709852471.0,
        "author": "z18782",
        "is_submitter": true,
        "parent_id": "t3_1b97d2k",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1b2zdkm",
    "title": "Looking to create a \"Clean Data\" definition",
    "selftext": "Hi,\n\nJust wondering what requirements or checklist items people would suggest for a definition of Clean Data ready to be used in machine learning?\nAkin to \"tidy data\", but for modelling. I.e.\n\n* There should be no string fields. All data should be either in a numeric form, or as a categorical data type\netc\n\nI know this will likely be opinionated, hence wanting to \"crowd source\" it üòÉ\n\nFeel free to disagree with any statements, as I imagine there will be differences ",
    "url": "https://www.reddit.com/r/datacleaning/comments/1b2zdkm/looking_to_create_a_clean_data_definition/",
    "score": 5,
    "upvote_ratio": 0.86,
    "num_comments": 0,
    "created_utc": 1709210396.0,
    "author": "youre_so_enbious",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1b2zdkm/looking_to_create_a_clean_data_definition/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "191q4vk",
    "title": "Avec les datas des r√©seaux sociaux du Web, une nouvelle sociologie.",
    "selftext": "",
    "url": "https://www.argotheme.com/organecyberpresse/spip.php?article4560",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 0,
    "created_utc": 1704733876.0,
    "author": "argotheme",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/191q4vk/avec_les_datas_des_r√©seaux_sociaux_du_web_une/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "18pgnex",
    "title": "Data Cleaning Freelancer",
    "selftext": " \n\nHey everyone,\n\nI'm a sophomore studying data science and I've been digging into ways to earn money online. I stumbled upon the idea of freelancing my data cleaning skills, and it seems like an exciting avenue. Though I'm still learning, I'm a quick learner and confident that I can get proficient in data cleaning soon.\n\nI'm keen to get hands-on experience and was wondering if anyone would be open to taking me under their wing as an apprentice or offering advice on where to begin.\n\nWhile I'm still early in my studies, I've worked on a few exploratory data analyses for my classes. These involved cleaning data and using RStudio to create graphs.\n\nI'm eager to turn this interest into a reality. Any guidance or tips on how to kickstart a career in freelancing data cleaning would be hugely appreciated!\n\nThanks in advance for any help or advice you can offer!",
    "url": "https://www.reddit.com/r/datacleaning/comments/18pgnex/data_cleaning_freelancer/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1703370221.0,
    "author": "SnooBooks8203",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/18pgnex/data_cleaning_freelancer/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "kk7mnv5",
        "body": "Hey, I have started the course for data science but I don‚Äôt know where can I practice questions for data cleaning.. any help from your side would help!!",
        "score": 1,
        "created_utc": 1706586830.0,
        "author": "Tamvert",
        "is_submitter": false,
        "parent_id": "t3_18pgnex",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "18b2a5x",
    "title": "AI tool to extract product characteristics",
    "selftext": "Hello everyone,  \n\n\nI am trying to clean up some data from our ERP systems regarding our items. I am working for a furniture company, we do have different characteristics that compose a product (size/timber/fabric and so on). So far, those characteristics has been input all in one description field. I'd like to extract those information and assign it to the new correct field (one field per characteristic). Maybe some AI tools might be able to help in that process? I am not a developer / technical IT. \n\n&#x200B;",
    "url": "https://www.reddit.com/r/datacleaning/comments/18b2a5x/ai_tool_to_extract_product_characteristics/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1701745906.0,
    "author": "stevenvnsg",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/18b2a5x/ai_tool_to_extract_product_characteristics/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "kogh2z7",
        "body": "I've built an AI tool that let's you do this by just describing what you want in english without any code (e.g. \"extract the size / timber / fabric from the description field\"): [https://www.getaugerdata.com/](https://www.getaugerdata.com/). It's self-serve but feel free to email [support@getaugerdata.com](mailto:support@getaugerdata.com) and I can sit down with you",
        "score": 1,
        "created_utc": 1706806723.0,
        "author": "Senior-Aardvark-5635",
        "is_submitter": false,
        "parent_id": "t3_18b2a5x",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "17p7rzv",
    "title": "Success Story: How data cleaning tools helped support my project",
    "selftext": " **Disclaimer**: This is a personal project I did, made possible with RPA (UiPath web scraping). The stats come from [SA Rugby](https://www.sarugby.co.za/match-centre/?teamId=e976cfc2-f80b-4c1a-8472-838cd54f0fbc&returnUrl=/sa-teams-players/springboks/) website & I developed automation flows to get the stats, player bio & profile pictures from the same website. I used PowerQuery to transform the output & to debug issues & finally Tableau for visualisation. I highly recommend getting comfortable with Power Query, you can do so much with it!\n\n&#x200B;\n\nHi everyone, I'd like to share a personal project I did about the Springboks RWC Campaign. I'd love to get your feedback as PowerBI people, to get your unique perspective. We only use Tableau at so I thought I'd overcome confirmation bias by getting your guys' opinions.\n\nThe project is basically match stats for all the games the Springboks played in all championships in 2023. You can see those who are consistently performing well. The stats come from [SA Rugby](https://www.sarugby.co.za/match-centre/?teamId=e976cfc2-f80b-4c1a-8472-838cd54f0fbc&returnUrl=/sa-teams-players/springboks/)\n\nEach match has highlight reels of the players' game contributions (71 total). The project also covers all the matches that the Boks under Rassie have played NZ (5 Wins, 5 Losses & 1 Draw).\n\nUltimately, the project shows how tough this World Cup was & the pressure the team faced, especially in the knockout phases.\n\nPS. I think this would be great for those new to rugby, since it covers the biggest matches in the sport with highlight reels to see the entertaining stuff.\n\nYou can check out the full work here: [https://public.tableau.com/views/Springboks2023RugbyWorldCupCampaign/TheSpringboks2023Campaign?:language=en-US&:display\\_count=n&:origin=viz\\_share\\_link](https://public.tableau.com/views/Springboks2023RugbyWorldCupCampaign/TheSpringboks2023Campaign?:language=en-US&:display_count=n&:origin=viz_share_link)\n\n&#x200B;\n\n[Final vs NZ](https://preview.redd.it/22veh4wvgryb1.png?width=1831&format=png&auto=webp&s=aa00dc4811ce6db127ff4e74ce5082428e074a23)\n\n&#x200B;\n\n[Semi Final vs England](https://preview.redd.it/ddooinpwgryb1.png?width=1632&format=png&auto=webp&s=8d21bc143edc60e86461537dbc5dfc9179c7b8a1)\n\n&#x200B;\n\n[Quarter Final vs France](https://preview.redd.it/g106a1dxgryb1.png?width=1847&format=png&auto=webp&s=dcd097df1e8ec1dfb5cd53d80c45389325fcad2a)",
    "url": "https://www.reddit.com/r/datacleaning/comments/17p7rzv/success_story_how_data_cleaning_tools_helped/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1699291140.0,
    "author": "BigIntroduction4586",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/17p7rzv/success_story_how_data_cleaning_tools_helped/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "17oxqyw",
    "title": "\"Cleaning Call Center Data: Seeking Guidance/Help\"",
    "selftext": "Hello everyone,\n\nI am currently working on a **call center trend dashboard project**, and I've encountered an issue with **multiple blank cells** in the data. I'm unsure about the **best approach to handle this**. Should I delete rows with multiple blank cells, or should I use statistics to fill these blank cells?\n\nI would greatly appreciate your guidance and suggestions on this matter. Your assistance would be invaluable. Thank you in advance!\n\n **Project Task :**\n\n Create a dashboard in Power BI for Claire that reflects all relevant Key Performance Indicators (KPIs) and metrics in the dataset \n\n \n\n**Possible KPIs include (to get you started, but not limited to):**\n\n* Overall customer satisfaction\n* Overall calls answered/abandoned\n* Calls by time\n* Average speed of answer\n* Agent‚Äôs performance quadrant -> average handle time (talk duration) vs calls answered\n\n**Some info about data:**\n\nTotal rows-5000\n\nTotal column :10\n\n[snapshot of data](https://preview.redd.it/ckwcyixoloyb1.png?width=1198&format=png&auto=webp&s=e75d5eee5814aa87fad270d3897ae800be098c06)\n\n \n\n\"Total rows having missing values: 946 Each of the 946 rows has 3 blank/missing cells.\n\nPlease guide me on the approach I should take to clean this data.\n\nNote: The blank column is just a temporary column used to check how many cells are blank in each row.\"\n\n&#x200B;\n\nTL;DR:Seeking advice on handling data with many missing values (946 rows, 3 blank cells each) for a call center trend dashboard project. Also, tasked with creating a Power BI dashboard for Claire, highlighting KPIs and metrics. Please assist. Thanks!  \n\n\n&#x200B;",
    "url": "https://www.reddit.com/r/datacleaning/comments/17oxqyw/cleaning_call_center_data_seeking_guidancehelp/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1699257158.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/17oxqyw/cleaning_call_center_data_seeking_guidancehelp/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "k81xwuc",
        "body": "Looks like the blank columns were all not answered, so you could ignore them and/or have a separate graph showing proportion of calls answered in that hour/day/week etc?",
        "score": 1,
        "created_utc": 1699267733.0,
        "author": "sifnt",
        "is_submitter": false,
        "parent_id": "t3_17oxqyw",
        "depth": 0
      },
      {
        "id": "k82drbe",
        "body": "\"Thank you for your response. Yes, I have already found out the reason for the empty cell after applying the filter.\"",
        "score": 1,
        "created_utc": 1699277192.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t1_k81xwuc",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "14xyilw",
    "title": "How to handle missing categorical values with more than 5% missing data?",
    "selftext": "I am upskilling in the field of data science. Recently started practicing on Kaggle datasets. Picked up a dataset which have more categorical columns than numerical and these columns have more that 5% (upto 60% null values in some columns) null values. I am confused about what technique to use on them. Cannot find resources where handling object columns specifically is focused upon. Any help please? can anyone suggest a book or website or just tell me how to proceed with this?",
    "url": "https://www.reddit.com/r/datacleaning/comments/14xyilw/how_to_handle_missing_categorical_values_with/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1689190988.0,
    "author": "winchester1806",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/14xyilw/how_to_handle_missing_categorical_values_with/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "js1ia1w",
        "body": "Typically you either remove the row or impute (guess) the missing value. Which is best depends on the dataset and your goals.\n\nYou can impute the missing value based on other values. For example if you have 'age' and 'retired' columns you can infer whether someone is retired based on their age and the mode of whether other people of that age in the dataset are retired or not retired. For example in the Easy Data Transform software you would use an 'Impute' transform with 'Using'='Mode' and 'Of'='age'. See also:\n\nhttps://www.youtube.com/watch?v=WXAGhtqI5xw",
        "score": 1,
        "created_utc": 1689407668.0,
        "author": "hermitcrab",
        "is_submitter": false,
        "parent_id": "t3_14xyilw",
        "depth": 0
      },
      {
        "id": "jrt48wq",
        "body": "the data is about real/fake job posting, i found the dataset on kaggle.",
        "score": 1,
        "created_utc": 1689259245.0,
        "author": "winchester1806",
        "is_submitter": true,
        "parent_id": "t1_jrs6e0u",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "14o26s3",
    "title": "How to get started with python for data analysis?",
    "selftext": "\n\nIf you're embarking on the odyssey of studying Python data analysis, commence by acquiring a mastery of the rudiments of Python programming. \n\nOnce you've attained a level of proficiency with Python, plunge into the depths of indispensable libraries such as NumPy for numerical computation and Pandas for data manipulation. Engage in practical exercises utilizing authentic datasets to accrue experiential knowledge, and refine your prowess in data visualization employing Matplotlib and Seaborn.\n\n \nDelve into the realm of statistical analysis using the comprehensive tools provided by SciPy, and contemplate augmenting your skill set with other pertinent libraries such as scikit-learn for machine learning. Engross yourself in online communities, undertake ambitious projects, and perpetually pursue learning and diligent practice to ascend to a zenith of expertise in Python data analysis‚Äîa gratifying pursuit that unveils the portals to unearthing invaluable insights from data.\n\n\nTo get you started, I will highly recommend you look at these articles.\n\n\n\n\nExploratory Data Analysis and visualization practical example:\n\nhttps://link.medium.com/FYuBpTyvCAb\n\nData cleaning with python (a practical example)\n\nhttps://link.medium.com/GBsdtEFvCAb\n\nHow to make data Visualization in python \n\nhttps://link.medium.com/6rWH2nKvCAb\n\nPython data cleaning made easy \n\nhttps://link.medium.com/6rWH2nKvCAb\n\n\nSales Statistical analysis with python \n\nhttps://link.medium.com/ZGx7NDRvCAb\n\nhttps://link.medium.com/OidaOBUvCAb\n\n\n\nPython Web App Development: Unleashing the Power of Simplicity and Flexibility\n\n\nhttps://medium.com/@mondoa/python-web-app-development-unleashing-the-power-of-simplicity-and-flexibility-d34e9d1bd658\n\n\nEnhancing Your Web Application with Python‚Äôs Data Analysis Tools\n\n\nhttps://medium.com/@mondoa/enhancing-your-web-application-with-pythons-data-analysis-tools-2ecd0af29027\n\nThe Ultimate Python 3 Guide: Everything You Need to Know\n\nhttps://medium.com/@mondoa/enhancing-a-comprehensive-python-3-tutorial-b8102f0cfcc4",
    "url": "https://www.reddit.com/r/datacleaning/comments/14o26s3/how_to_get_started_with_python_for_data_analysis/",
    "score": 6,
    "upvote_ratio": 0.88,
    "num_comments": 0,
    "created_utc": 1688236452.0,
    "author": "AdGreat4483",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/14o26s3/how_to_get_started_with_python_for_data_analysis/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "14jp9z0",
    "title": "Help Extracting Data from XML",
    "selftext": "I need help with figuring out the best tool to do so extraction of data. I work on a Wiki and I am able to download XMLs of large sets of pages. For this to be any use to us, I need to be able to put them in Excel to turn them into CSV files to be able to reupload them after I've fixed or added more data. Here's an example of what I can manually do right now to turn it into a format I need for the CSV file: \n\nFirst I download the XML File. This example only has 3 pages in it, but usually there are hundreds. It looks something like this:\n\n \n\nmediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.11/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.11/ http://www.mediawiki.org/xml/export-0.11.xsd\" version=\"0.11\" xml:lang=\"en\">  \n <siteinfo>  \n <sitename>FamilySearch Wiki</sitename>  \n <dbname>wiki\\_en</dbname>  \n <base>https://www.familysearch.org/en/wiki/Main\\_Page</base>  \n <generator>MediaWiki 1.35.8</generator>  \n <case>first-letter</case>  \n <namespaces>  \n <namespace key=\"-2\" case=\"first-letter\">Media</namespace>  \n <namespace key=\"-1\" case=\"first-letter\">Special</namespace>  \n <namespace key=\"0\" case=\"first-letter\" />  \n <namespace key=\"1\" case=\"first-letter\">Talk</namespace>  \n <namespace key=\"2\" case=\"first-letter\">User</namespace>  \n <namespace key=\"3\" case=\"first-letter\">User talk</namespace>  \n <namespace key=\"4\" case=\"first-letter\">FamilySearch Wiki</namespace>  \n <namespace key=\"5\" case=\"first-letter\">FamilySearch Wiki talk</namespace>  \n <namespace key=\"6\" case=\"first-letter\">File</namespace>  \n <namespace key=\"7\" case=\"first-letter\">File talk</namespace>  \n <namespace key=\"8\" case=\"first-letter\">MediaWiki</namespace>  \n <namespace key=\"9\" case=\"first-letter\">MediaWiki talk</namespace>  \n <namespace key=\"10\" case=\"first-letter\">Template</namespace>  \n <namespace key=\"11\" case=\"first-letter\">Template talk</namespace>  \n <namespace key=\"12\" case=\"first-letter\">Help</namespace>  \n <namespace key=\"13\" case=\"first-letter\">Help talk</namespace>  \n <namespace key=\"14\" case=\"first-letter\">Category</namespace>  \n <namespace key=\"15\" case=\"first-letter\">Category talk</namespace>  \n <namespace key=\"102\" case=\"first-letter\">Property</namespace>  \n <namespace key=\"103\" case=\"first-letter\">Property talk</namespace>  \n <namespace key=\"106\" case=\"first-letter\">Form</namespace>  \n <namespace key=\"107\" case=\"first-letter\">Form talk</namespace>  \n <namespace key=\"108\" case=\"first-letter\">Concept</namespace>  \n <namespace key=\"109\" case=\"first-letter\">Concept talk</namespace>  \n <namespace key=\"112\" case=\"first-letter\">smw/schema</namespace>  \n <namespace key=\"113\" case=\"first-letter\">smw/schema talk</namespace>  \n <namespace key=\"114\" case=\"first-letter\">Rule</namespace>  \n <namespace key=\"115\" case=\"first-letter\">Rule talk</namespace>  \n <namespace key=\"200\" case=\"first-letter\">Policy</namespace>  \n <namespace key=\"201\" case=\"first-letter\">Policy Talk</namespace>  \n <namespace key=\"202\" case=\"first-letter\">Shared Category</namespace>  \n <namespace key=\"203\" case=\"first-letter\">Shared Category Talk</namespace>  \n <namespace key=\"274\" case=\"first-letter\">Widget</namespace>  \n <namespace key=\"275\" case=\"first-letter\">Widget talk</namespace>  \n <namespace key=\"420\" case=\"first-letter\">GeoJson</namespace>  \n <namespace key=\"421\" case=\"first-letter\">GeoJson talk</namespace>  \n <namespace key=\"460\" case=\"case-sensitive\">Campaign</namespace>  \n <namespace key=\"461\" case=\"case-sensitive\">Campaign talk</namespace>  \n <namespace key=\"828\" case=\"first-letter\">Module</namespace>  \n <namespace key=\"829\" case=\"first-letter\">Module talk</namespace>  \n <namespace key=\"2300\" case=\"first-letter\">Gadget</namespace>  \n <namespace key=\"2301\" case=\"first-letter\">Gadget talk</namespace>  \n <namespace key=\"2302\" case=\"case-sensitive\">Gadget definition</namespace>  \n <namespace key=\"2303\" case=\"case-sensitive\">Gadget definition talk</namespace>  \n <namespace key=\"3100\" case=\"first-letter\">GuidedResearch</namespace>  \n <namespace key=\"3101\" case=\"first-letter\">GuidedResearch Talk</namespace>  \n <namespace key=\"3102\" case=\"first-letter\">AFOG</namespace>  \n <namespace key=\"3103\" case=\"first-letter\">AFOG Talk</namespace>  \n <namespace key=\"3104\" case=\"first-letter\">Indonesia</namespace>  \n <namespace key=\"3105\" case=\"first-letter\">Indonesia Talk</namespace>  \n <namespace key=\"3106\" case=\"first-letter\">Mongolian</namespace>  \n <namespace key=\"3107\" case=\"first-letter\">Mongolian Talk</namespace>  \n <namespace key=\"3108\" case=\"first-letter\">Norwegian</namespace>  \n <namespace key=\"3109\" case=\"first-letter\">Norwegian Talk</namespace>  \n <namespace key=\"3110\" case=\"first-letter\">GR</namespace>  \n <namespace key=\"3111\" case=\"first-letter\">GR Talk</namespace>  \n </namespaces>  \n </siteinfo>  \n <page>  \n <title>Cosal√°, Sinaloa, Mexico Genealogy</title>  \n <ns>0</ns>  \n <id>386351</id>  \n <revision>  \n <id>5345468</id>  \n <parentid>5345405</parentid>  \n <timestamp>2023-05-31T19:28:51Z</timestamp>  \n <contributor>  \n <username>Amberannelarsen</username>  \n <id>490153</id>  \n </contributor>  \n <minor/>  \n <comment>Text replacement - \"\\&amp;#243;\" to \"√≥\"</comment>  \n <origin>5345468</origin>  \n <model>wikitext</model>  \n <format>text/x-wiki</format>  \n <text bytes=\"1890\" sha1=\"dnzksnhh9qwij21kx164z04wnsci09g\" xml:space=\"preserve\">{{breadcrumb | link1=\\[\\[Mexico Genealogy|Mexico\\]\\]  \n| link2=\\[\\[Sinaloa, Mexico Genealogy|Sinaloa\\]\\]  \n| link3=  \n| link4=  \n| link5=\\[\\[Cosal√°, Sinaloa, Mexico Genealogy|Cosal√°\\]\\]  \n}}  \nGuide to '''Municipality of Cosal√° family history and genealogy''': birth records, marriage records, death records, census records, parish registers, and military records.   \n==History==  \n\\*El territorio donde actualmente se ubica Cosal√°, estuvo ocupado por pueblos prehisp√°nicos que se asentaron principalmente en la rivera de los r√≠os, como lo fueron  \nlos grupos ind√≠genas Tepehuanes, Acaxees y Xiximes.  \n\\*El municipio de Cosal√° fue fundado el 13 March 1562.  \n\\*El municipio de Cosal√° tiene una poblaci√≥n de aproximadamente 17.000 personas.\\&lt;ref\\&gt;Wikipedia contributors, ‚ÄúMunicipio de Cosal√°‚Äù in ''Wikipedia: the Free Encyclopedia'', https://es.wikipedia.org/wiki/Municipio\\_de\\_Cosal%C3%A1. accessed 25 February2021.\\&lt;/ref\\&gt;  \n==Localities within Cosal√°==  \n{| style=\"width:100%; vertical-align:top;\"  \n|- |  \n\\&lt;ul class=\"column-spacing-fullscreen\" style=\"padding-right:5px;\"\\&gt;  \n\\&lt;li\\&gt;Cosal√°\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;El Rodeo\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;La Llama\\&lt;/li\\&gt;  \n\\&lt;/ul\\&gt;  \n|}  \n==Civil Registration==  \n\\*'''1867-1929''' {{FHL|2819510|title-id|disp=Mexico, Sinaloa, Cosal√°, Civil Registration, 1867-1929}}(\\*) at FamilySearch Catalog ‚Äî images  \n==Parish Records==  \n\\*'''1777-1966''' {{FHL|263768|title-id|disp=Iglesia Cat√≥lica. Santa Ursula (Cosala, Sinaloa) Parish Records, 1777-1966}}(\\*) at FamilySearch Catalog ‚Äî images  \n\\*'''1874-1920''' {{FHL|260349|title-id|disp=Iglesia Cat√≥lica. Santa Ursula (Cosal√°, Sinaloa) Parish Records, 1874-1920}}(\\*) at FamilySearch Catalog ‚Äî images  \n==Census==  \n==Cemeteries==  \n\\*Cementerio de San Juan Cosala  \n:\\*\\[https://www.findagrave.com/cemetery-browse/Mexico/Sinaloa/Cosal%C3%A1-Municipality?id=county\\_13453 Find a Grave\\]  \n==References==  \n\\&lt;references/\\&gt;  \n\\&lt;br\\&gt;\\&lt;br\\&gt;  \n\\[\\[es:Cosal√°, Sinaloa, Mexico Genealogy\\]\\]  \n\\[\\[Category:Sinaloa, Mexico\\]\\]</text>  \n <sha1>dnzksnhh9qwij21kx164z04wnsci09g</sha1>  \n </revision>  \n </page>  \n <page>  \n <title>Mocorito, Sinaloa, Mexico Genealogy</title>  \n <ns>0</ns>  \n <id>386352</id>  \n <revision>  \n <id>5369276</id>  \n <parentid>5347024</parentid>  \n <timestamp>2023-06-14T20:58:28Z</timestamp>  \n <contributor>  \n <username>Amberannelarsen</username>  \n <id>490153</id>  \n </contributor>  \n <minor/>  \n <comment>Text replacement - \"\\&amp;#241;\" to \"√±\"</comment>  \n <origin>5369276</origin>  \n <model>wikitext</model>  \n <format>text/x-wiki</format>  \n <text bytes=\"2698\" sha1=\"0yetzi5a03vs76bfpv273ppgniyu94p\" xml:space=\"preserve\">{{breadcrumb | link1=\\[\\[Mexico Genealogy|Mexico\\]\\]  \n| link2=\\[\\[Sinaloa, Mexico Genealogy|Sinaloa\\]\\]  \n| link3=  \n| link4=  \n| link5=\\[\\[Mocorito, Sinaloa, Mexico Genealogy|Mocorito\\]\\]  \n}}  \nGuide to '''Municipality of Mocorito family history and genealogy''': birth records, marriage records, death records, census records, parish registers, and military records.   \n==History==  \n\\*En el a√±o de 1531 con la entrada del conquistador Nu√±o de Guzm√°n al noroeste mexicano y la fundaci√≥n de la villa de San Miguel de Navito, se inici√≥ la delimitaci√≥n geogr√°fica de la provincia de Culiac√°n.  \n\\*En 1732 cuando la expansi√≥n espa√±ola llegaba m√°s all√° del r√≠o Yaqui, se encuentra el territorio dividido en provincias.  \n\\*En 1830 se decreta la separaci√≥n definitiva de Sonora y Sinaloa. El nuevo estado de Sinaloa se dividi√≥ en once distritos, siendo Mocorito uno de ellos.  \n\\*Mocorito fue erigido como municipio el 8 April 1915.  \n\\*El municipio de Mocorito tiene una poblaci√≥n de aproximadamente 45.000 personas.\\&lt;ref\\&gt;Wikipedia contributors, ‚ÄúMunicipio de Mocorito‚Äù in ''Wikipedia: the Free Encyclopedia'', https://es.wikipedia.org/wiki/Municipio\\_de\\_Mocorito. accessed 26 February2021.\\&lt;/ref\\&gt;  \n==Localities within Mocorito==  \n{| style=\"width:100%; vertical-align:top;\"  \n|- |  \n\\&lt;ul class=\"column-spacing-fullscreen\" style=\"padding-right:5px;\"\\&gt;  \n\\&lt;li\\&gt;Pericos\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Mocorito\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Caimanero\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Melchor Ocampo\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Recoveco\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Higuera de los Vega\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Potrero de los S√°nchez (Estaci√≥n Techa)\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Cerro Agudo\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;El Valle de Leyva Solano (El Valle)\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Rancho Viejo\\&lt;/li\\&gt;  \n\\&lt;/ul\\&gt;  \n|}  \n==Civil Registration==  \n\\*'''1865-1929''' {{FHL|2819522|title-id|disp=Mexico, Sinaloa, Mocorito, Civil Registration, 1865-1929}}(\\*) at FamilySearch Catalog ‚Äî images  \n\\*'''1922''' {{FHL|2819540|title-id|disp=Mexico, Sinaloa, Mocorito y Guasave, Civil Registration, 1922}}(\\*) at FamilySearch Catalog ‚Äî images  \n==Parish Records==  \n\\*'''1677-1968''' {{FHL|262334|title-id|disp=Iglesia Cat√≥lica. Pur√≠sima Concepci√≥n (Mocorito, Sinaloa) Parish Records, 1677-1968}}(\\*) at FamilySearch Catalog ‚Äî images  \n\\*'''1856-1933''' {{FHL|589667|title-id|disp=Iglesia Cat√≥lica. Nuestra Se√±ora de las Angustias (Pericos, Sinaloa) Registros  \nparroquiales, 1856-1933}}(\\*) at FamilySearch Catalog ‚Äî images  \n==Census==  \n\\*'''1930''' {{FHL|454789|title-id|disp=Censo de poblaci√≥n del municipio de Mocorito, Sinaloa, 1930}}(\\*) at FamilySearch Catalog ‚Äî images  \n==Cemeteries==  \n\\*Panteon Reforma  \n:\\*Address: Mocorito  \n\\*Cementerio de Buena Vista  \n:\\*Address: Mocorito  \n\\*Cementerio de El Queso  \n:\\*Address: Boca de Arroyo, Mocorito  \n==References==  \n\\&lt;references/\\&gt;  \n\\&lt;br\\&gt;\\&lt;br\\&gt;  \n\\[\\[es:Mocorito, Sinaloa, Mexico Genealogy\\]\\]  \n\\[\\[Category:Sinaloa, Mexico\\]\\]</text>  \n <sha1>0yetzi5a03vs76bfpv273ppgniyu94p</sha1>  \n </revision>  \n </page>  \n <page>  \n <title>Sinaloa, Sinaloa, Mexico Genealogy</title>  \n <ns>0</ns>  \n <id>386353</id>  \n <revision>  \n <id>5348610</id>  \n <parentid>5348590</parentid>  \n <timestamp>2023-05-31T20:45:17Z</timestamp>  \n <contributor>  \n <username>Amberannelarsen</username>  \n <id>490153</id>  \n </contributor>  \n <minor/>  \n <comment>Text replacement - \"\\&amp;#243;\" to \"√≥\"</comment>  \n <origin>5348610</origin>  \n <model>wikitext</model>  \n <format>text/x-wiki</format>  \n <text bytes=\"2313\" sha1=\"gqx35x7qm1axjze8fhizfy6n6iasv3l\" xml:space=\"preserve\">{{breadcrumb | link1=\\[\\[Mexico Genealogy|Mexico\\]\\]  \n| link2=\\[\\[Sinaloa, Mexico Genealogy|Sinaloa\\]\\]  \n| link3=  \n| link4=  \n| link5=\\[\\[Sinaloa, Sinaloa, Mexico Genealogy|Sinaloa\\]\\]  \n}}  \nGuide to '''Municipality of Sinaloa family history and genealogy''': birth records, marriage records, death records, census records, parish registers, and military records.   \n==History==  \n\\*Sinaloa de Leyva se fund√≥ el 30 April 1583 con el nombre de Villa de San Phelipe y Santiago de Sinaloa.  \n\\*En 1732 La Villa es designada capital de la gobernaci√≥n de Sinaloa.  \n\\*Sinaloa fue erigido como municipio el 25 March 1915.  \n\\*El municipio de Sinaloa tiene una poblaci√≥n de aproximadamente 89.000 personas.\\&lt;ref\\&gt;Wikipedia contributors, ‚ÄúMunicipio de Sinaloa‚Äù in ''Wikipedia: the Free Encyclopedia'', https://es.wikipedia.org/wiki/Municipio\\_de\\_Sinaloa. accessed 26 February2021.\\&lt;/ref\\&gt;  \n==Localities within Sinaloa==  \n{| style=\"width:100%; vertical-align:top;\"  \n|- |  \n\\&lt;ul class=\"column-spacing-fullscreen\" style=\"padding-right:5px;\"\\&gt;  \n\\&lt;li\\&gt;Estaci√≥n Naranjo\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Sinaloa de Leyva\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Genaro Estrada\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Gabriel Leyva Vel√°zquez\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Ruiz Cortines N√∫mero Tres\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Alfonso G. Calder√≥n Velarde\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Cubiri de Portelas\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Ejido el Maquipo\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Llano Grande 1,540\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Santiago de Ocoroni\\&lt;/li\\&gt;  \n\\&lt;/ul\\&gt;  \n|}  \n==Civil Registration==  \n\\*'''1861-1929''' {{FHL|2819523|title-id|disp=Mexico, Sinaloa, Sinaloa, Civil Registration, 1861-1929}}(\\*) at FamilySearch Catalog ‚Äî images  \n==Parish Records==  \n\\*'''1852-1968''' {{FHL|263710|title-id|disp=Iglesia Cat√≥lica. San Felipe y Santiago (Sinaloa, Sinaloa) Parish Records, 1852-1968}}(\\*) at FamilySearch Catalog ‚Äî images  \n==Census==  \n\\*'''1930''' {{FHL|454801|title-id|disp=Censo de poblaci√≥n del municipio de Sinaloa, Sinaloa, 1930}}(\\*) at FamilySearch Catalog ‚Äî images  \n==Cemeteries==  \n\\*Pante√≥n Municipal de Estaci√≥n Naranjo Sinaloa Jesus Parra Gerardo  \n:\\*Address: Francisco Villa #0, Estaci√≥n Naranjo, Sinaloa  \n\\*Cementerio Municipal  \n:\\*Address: Sinaloa Guasave, Sinaloa de Leyva, Sinaloa  \n\\*Pante√≥n Municipal  \n:\\*Address: Isauro Vallejo #0, Tierra Blanca, Sinaloa  \n:\\*\\[https://www.findagrave.com/cemetery-browse/Mexico/Sinaloa/Sinaloa-Municipality?id=county\\_13465 Find a Grave\\]  \n==References==  \n\\&lt;references/\\&gt;  \n\\&lt;br\\&gt;\\&lt;br\\&gt;  \n\\[\\[es:Sinaloa, Sinaloa, Mexico Genealogy\\]\\]  \n\\[\\[Category:Sinaloa, Mexico\\]\\]</text>  \n <sha1>gqx35x7qm1axjze8fhizfy6n6iasv3l</sha1>  \n </revision>  \n </page>\n\n</mediawiki>\n\n&#x200B;\n\nThen I can manually go through and copy everything between xml:space=\"preserve\"> and </text> to get three separate pages:  \n\n\n{{breadcrumb | link1=\\[\\[Mexico Genealogy|Mexico\\]\\]  \n| link2=\\[\\[Sinaloa, Mexico Genealogy|Sinaloa\\]\\]  \n| link3=  \n| link4=  \n| link5=\\[\\[Cosal√°, Sinaloa, Mexico Genealogy|Cosal√°\\]\\]  \n}}  \nGuide to '''Municipality of Cosal√° family history and genealogy''': birth records, marriage records, death records, census records, parish registers, and military records.   \n==History==  \n\\*El territorio donde actualmente se ubica Cosal√°, estuvo ocupado por pueblos prehisp√°nicos que se asentaron principalmente en la rivera de los r√≠os, como lo fueron  \nlos grupos ind√≠genas Tepehuanes, Acaxees y Xiximes.  \n\\*El municipio de Cosal√° fue fundado el 13 March 1562.  \n\\*El municipio de Cosal√° tiene una poblaci√≥n de aproximadamente 17.000 personas.\\&lt;ref\\&gt;Wikipedia contributors, ‚ÄúMunicipio de Cosal√°‚Äù in ''Wikipedia: the Free Encyclopedia'', https://es.wikipedia.org/wiki/Municipio\\_de\\_Cosal%C3%A1. accessed 25 February2021.\\&lt;/ref\\&gt;  \n==Localities within Cosal√°==  \n{| style=\"width:100%; vertical-align:top;\"  \n|- |  \n\\&lt;ul class=\"column-spacing-fullscreen\" style=\"padding-right:5px;\"\\&gt;  \n\\&lt;li\\&gt;Cosal√°\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;El Rodeo\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;La Llama\\&lt;/li\\&gt;  \n\\&lt;/ul\\&gt;  \n|}  \n==Civil Registration==  \n\\*'''1867-1929''' {{FHL|2819510|title-id|disp=Mexico, Sinaloa, Cosal√°, Civil Registration, 1867-1929}}(\\*) at FamilySearch Catalog ‚Äî images  \n==Parish Records==  \n\\*'''1777-1966''' {{FHL|263768|title-id|disp=Iglesia Cat√≥lica. Santa Ursula (Cosala, Sinaloa) Parish Records, 1777-1966}}(\\*) at FamilySearch Catalog ‚Äî images  \n\\*'''1874-1920''' {{FHL|260349|title-id|disp=Iglesia Cat√≥lica. Santa Ursula (Cosal√°, Sinaloa) Parish Records, 1874-1920}}(\\*) at FamilySearch Catalog ‚Äî images  \n==Census==  \n==Cemeteries==  \n\\*Cementerio de San Juan Cosala  \n:\\*\\[https://www.findagrave.com/cemetery-browse/Mexico/Sinaloa/Cosal%C3%A1-Municipality?id=county\\_13453 Find a Grave\\]  \n==References==  \n\\&lt;references/\\&gt;  \n\\&lt;br\\&gt;\\&lt;br\\&gt;  \n\\[\\[es:Cosal√°, Sinaloa, Mexico Genealogy\\]\\]  \n\\[\\[Category:Sinaloa, Mexico\\]\\]  \n\n\n&#x200B;\n\n{{breadcrumb | link1=\\[\\[Mexico Genealogy|Mexico\\]\\]  \n| link2=\\[\\[Sinaloa, Mexico Genealogy|Sinaloa\\]\\]  \n| link3=  \n| link4=  \n| link5=\\[\\[Mocorito, Sinaloa, Mexico Genealogy|Mocorito\\]\\]  \n}}  \nGuide to '''Municipality of Mocorito family history and genealogy''': birth records, marriage records, death records, census records, parish registers, and military records.   \n==History==  \n\\*En el a√±o de 1531 con la entrada del conquistador Nu√±o de Guzm√°n al noroeste mexicano y la fundaci√≥n de la villa de San Miguel de Navito, se inici√≥ la delimitaci√≥n geogr√°fica de la provincia de Culiac√°n.  \n\\*En 1732 cuando la expansi√≥n espa√±ola llegaba m√°s all√° del r√≠o Yaqui, se encuentra el territorio dividido en provincias.  \n\\*En 1830 se decreta la separaci√≥n definitiva de Sonora y Sinaloa. El nuevo estado de Sinaloa se dividi√≥ en once distritos, siendo Mocorito uno de ellos.  \n\\*Mocorito fue erigido como municipio el 8 April 1915.  \n\\*El municipio de Mocorito tiene una poblaci√≥n de aproximadamente 45.000 personas.\\&lt;ref\\&gt;Wikipedia contributors, ‚ÄúMunicipio de Mocorito‚Äù in ''Wikipedia: the Free Encyclopedia'', https://es.wikipedia.org/wiki/Municipio\\_de\\_Mocorito. accessed 26 February2021.\\&lt;/ref\\&gt;  \n==Localities within Mocorito==  \n{| style=\"width:100%; vertical-align:top;\"  \n|- |  \n\\&lt;ul class=\"column-spacing-fullscreen\" style=\"padding-right:5px;\"\\&gt;  \n\\&lt;li\\&gt;Pericos\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Mocorito\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Caimanero\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Melchor Ocampo\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Recoveco\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Higuera de los Vega\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Potrero de los S√°nchez (Estaci√≥n Techa)\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Cerro Agudo\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;El Valle de Leyva Solano (El Valle)\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Rancho Viejo\\&lt;/li\\&gt;  \n\\&lt;/ul\\&gt;  \n|}  \n==Civil Registration==  \n\\*'''1865-1929''' {{FHL|2819522|title-id|disp=Mexico, Sinaloa, Mocorito, Civil Registration, 1865-1929}}(\\*) at FamilySearch Catalog ‚Äî images  \n\\*'''1922''' {{FHL|2819540|title-id|disp=Mexico, Sinaloa, Mocorito y Guasave, Civil Registration, 1922}}(\\*) at FamilySearch Catalog ‚Äî images  \n==Parish Records==  \n\\*'''1677-1968''' {{FHL|262334|title-id|disp=Iglesia Cat√≥lica. Pur√≠sima Concepci√≥n (Mocorito, Sinaloa) Parish Records, 1677-1968}}(\\*) at FamilySearch Catalog ‚Äî images  \n\\*'''1856-1933''' {{FHL|589667|title-id|disp=Iglesia Cat√≥lica. Nuestra Se√±ora de las Angustias (Pericos, Sinaloa) Registros  \nparroquiales, 1856-1933}}(\\*) at FamilySearch Catalog ‚Äî images  \n==Census==  \n\\*'''1930''' {{FHL|454789|title-id|disp=Censo de poblaci√≥n del municipio de Mocorito, Sinaloa, 1930}}(\\*) at FamilySearch Catalog ‚Äî images  \n==Cemeteries==  \n\\*Panteon Reforma  \n:\\*Address: Mocorito  \n\\*Cementerio de Buena Vista  \n:\\*Address: Mocorito  \n\\*Cementerio de El Queso  \n:\\*Address: Boca de Arroyo, Mocorito  \n==References==  \n\\&lt;references/\\&gt;  \n\\&lt;br\\&gt;\\&lt;br\\&gt;  \n\\[\\[es:Mocorito, Sinaloa, Mexico Genealogy\\]\\]  \n\\[\\[Category:Sinaloa, Mexico\\]\\]\n\n&#x200B;\n\n&#x200B;\n\n{{breadcrumb | link1=\\[\\[Mexico Genealogy|Mexico\\]\\]  \n| link2=\\[\\[Sinaloa, Mexico Genealogy|Sinaloa\\]\\]  \n| link3=  \n| link4=  \n| link5=\\[\\[Sinaloa, Sinaloa, Mexico Genealogy|Sinaloa\\]\\]  \n}}  \nGuide to '''Municipality of Sinaloa family history and genealogy''': birth records, marriage records, death records, census records, parish registers, and military records.   \n==History==  \n\\*Sinaloa de Leyva se fund√≥ el 30 April 1583 con el nombre de Villa de San Phelipe y Santiago de Sinaloa.  \n\\*En 1732 La Villa es designada capital de la gobernaci√≥n de Sinaloa.  \n\\*Sinaloa fue erigido como municipio el 25 March 1915.  \n\\*El municipio de Sinaloa tiene una poblaci√≥n de aproximadamente 89.000 personas.\\&lt;ref\\&gt;Wikipedia contributors, ‚ÄúMunicipio de Sinaloa‚Äù in ''Wikipedia: the Free Encyclopedia'', https://es.wikipedia.org/wiki/Municipio\\_de\\_Sinaloa. accessed 26 February2021.\\&lt;/ref\\&gt;  \n==Localities within Sinaloa==  \n{| style=\"width:100%; vertical-align:top;\"  \n|- |  \n\\&lt;ul class=\"column-spacing-fullscreen\" style=\"padding-right:5px;\"\\&gt;  \n\\&lt;li\\&gt;Estaci√≥n Naranjo\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Sinaloa de Leyva\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Genaro Estrada\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Gabriel Leyva Vel√°zquez\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Ruiz Cortines N√∫mero Tres\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Alfonso G. Calder√≥n Velarde\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Cubiri de Portelas\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Ejido el Maquipo\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Llano Grande 1,540\\&lt;/li\\&gt;  \n\\&lt;li\\&gt;Santiago de Ocoroni\\&lt;/li\\&gt;  \n\\&lt;/ul\\&gt;  \n|}  \n==Civil Registration==  \n\\*'''1861-1929''' {{FHL|2819523|title-id|disp=Mexico, Sinaloa, Sinaloa, Civil Registration, 1861-1929}}(\\*) at FamilySearch Catalog ‚Äî images  \n==Parish Records==  \n\\*'''1852-1968''' {{FHL|263710|title-id|disp=Iglesia Cat√≥lica. San Felipe y Santiago (Sinaloa, Sinaloa) Parish Records, 1852-1968}}(\\*) at FamilySearch Catalog ‚Äî images  \n==Census==  \n\\*'''1930''' {{FHL|454801|title-id|disp=Censo de poblaci√≥n del municipio de Sinaloa, Sinaloa, 1930}}(\\*) at FamilySearch Catalog ‚Äî images  \n==Cemeteries==  \n\\*Pante√≥n Municipal de Estaci√≥n Naranjo Sinaloa Jesus Parra Gerardo  \n:\\*Address: Francisco Villa #0, Estaci√≥n Naranjo, Sinaloa  \n\\*Cementerio Municipal  \n:\\*Address: Sinaloa Guasave, Sinaloa de Leyva, Sinaloa  \n\\*Pante√≥n Municipal  \n:\\*Address: Isauro Vallejo #0, Tierra Blanca, Sinaloa  \n:\\*\\[https://www.findagrave.com/cemetery-browse/Mexico/Sinaloa/Sinaloa-Municipality?id=county\\_13465 Find a Grave\\]  \n==References==  \n\\&lt;references/\\&gt;  \n\\&lt;br\\&gt;\\&lt;br\\&gt;  \n\\[\\[es:Sinaloa, Sinaloa, Mexico Genealogy\\]\\]  \n\\[\\[Category:Sinaloa, Mexico\\]\\]\n\n&#x200B;\n\n&#x200B;\n\nDoes anyone know an efficient way to get some sort of computer to do this? I tried having ChatGPT help me write functions for Google Sheets, but they weren't working very well for me. I also tried using regular expressions, but could still only get it to do one page at a time, and still had to manually do a lot of the work, which isn't feasible when there are 300+ pages to go through. I'm happy to try to learn something new in order to do this as it would help speed up some of our processes. I am sure something like this exists, but I don't know what. Thanks for your help!\n\n&#x200B;\n\n&#x200B;",
    "url": "https://www.reddit.com/r/datacleaning/comments/14jp9z0/help_extracting_data_from_xml/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1687804489.0,
    "author": "Genealogia-23",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/14jp9z0/help_extracting_data_from_xml/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "jpngnfs",
        "body": "Can look at - https://realpython.com/python-xml-parser/",
        "score": 1,
        "created_utc": 1687820293.0,
        "author": "edimaudo",
        "is_submitter": false,
        "parent_id": "t3_14jp9z0",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "147ncf7",
    "title": "Need help on cleaning this data!!",
    "selftext": "As in the picture, there are multiple records with same headers, i want to create data which has column headers and it's values below them. I am unable to find a way out. Please help!!!",
    "url": "https://i.redd.it/si4xl3q13l5b1.jpg",
    "score": 0,
    "upvote_ratio": 0.38,
    "num_comments": 3,
    "created_utc": 1686574215.0,
    "author": "likhithsai",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/147ncf7/need_help_on_cleaning_this_data/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "jnwnogp",
        "body": "What kind of tools and expertise do you have at your disposal?  The data itself doesn't look too bad, overloading of multidimensional data in individual cells notwithstanding.",
        "score": 2,
        "created_utc": 1686589056.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_147ncf7",
        "depth": 0
      },
      {
        "id": "jo2blzc",
        "body": "You might try\n\n```\nmlr --ixtab --opprint --ips-regex ': *' cat filename-goes-here\n```\n\n```\nmlr --ixtab --ocsv --ips-regex ': *' cat filename-goes-here\n```\n\nwhere `mlr` is from https://github.com/johnkerl/miller",
        "score": 1,
        "created_utc": 1686713135.0,
        "author": "johnkerl",
        "is_submitter": false,
        "parent_id": "t3_147ncf7",
        "depth": 0
      },
      {
        "id": "jo2cc2i",
        "body": "(Miller can also convert to JSON if you prefer)",
        "score": 1,
        "created_utc": 1686713617.0,
        "author": "johnkerl",
        "is_submitter": false,
        "parent_id": "t3_147ncf7",
        "depth": 0
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "147mv7r",
    "title": "Need help on cleaning this data!!",
    "selftext": "As in the picture, there are multiple records with same headers, i want to create data which has column headers and it's values below them. I am unable to find a way out. Please help!!!",
    "url": "https://i.redd.it/p6qb27csyk5b1.jpg",
    "score": 0,
    "upvote_ratio": 0.4,
    "num_comments": 1,
    "created_utc": 1686572790.0,
    "author": "likhithsai",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/147mv7r/need_help_on_cleaning_this_data/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "jnz9oaa",
        "body": "This format is really easy to turn into a human readable. I can help you with  this if you would like.",
        "score": 1,
        "created_utc": 1686650420.0,
        "author": "enzsio",
        "is_submitter": false,
        "parent_id": "t3_147mv7r",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "13txv0w",
    "title": "Textraction.ai released! Flexible entity extraction - no training needed",
    "selftext": "It can extract exact values (e.g. names, prices, dates), as well as provide ChatGPT-like semantic answers (e.g. text summary). Just describe the entities with a simple format:\n\n* description: a free text description of what you want to extract.\n* type: string / float / integer / string.\n* variable name: a descriptive variable name.\n* (optional) valid values: limit the output to a set of specific possible values.\n\nVery impressive, it worked great on my data which consists of product descriptions and specs.\n\nI like the interactive demo ([https://www.textraction.ai/](https://www.textraction.ai/)). The service is accessible also as an API for any commercial purpose via the RapidAPI platform: [https://rapidapi.com/textractionai/api/ai-textraction](https://rapidapi.com/textractionai/api/ai-textraction)",
    "url": "https://www.reddit.com/r/datacleaning/comments/13txv0w/textractionai_released_flexible_entity_extraction/",
    "score": 5,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1685272288.0,
    "author": "DoorDesigner7589",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/13txv0w/textractionai_released_flexible_entity_extraction/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "13onu71",
    "title": "Best Logic to calculate Idle time",
    "selftext": "Hello guys, in our college project we have the first task which is to cleanup the data and look for extra feature. \n\nThe data set is about bikes and stations in LA and it contains 1.7 Million Rows.\n\nWe have the following features: trans_id, start_time, start_station_id, end_time,end_station_id and bike_id.\n\nWe wanted to calculate the avg. Idle time of each station. Idle time = time between return and pick up of bike at station_id .\n\nWhat would be the best logic to calculate it.",
    "url": "https://www.reddit.com/r/datacleaning/comments/13onu71/best_logic_to_calculate_idle_time/",
    "score": 5,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1684753758.0,
    "author": "Sabaawi7",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/13onu71/best_logic_to_calculate_idle_time/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "jmpi3xk",
        "body": "How did you solve it?\n\nOne approach could be making a list of all the start and end events for a station since you do not care what type of event it is. \n\nSo you have station_id and event_time.\n\nYou sort by station_id and then event_time\n\nInitialize a column idle_time that stores the time between two events\n\nLoop on station_id\n\nFor each row (after the first) compute idle_time comparing event_time with previous row. First will be null.\n\nAverage of the idle_time column by station should be the result",
        "score": 2,
        "created_utc": 1685769166.0,
        "author": "juhuhui",
        "is_submitter": false,
        "parent_id": "t3_13onu71",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "13517xz",
    "title": "What is the fastest way to change this excel date format to python datetime format?",
    "selftext": "",
    "url": "https://i.redd.it/gcirs6ztsbxa1.png",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1682975036.0,
    "author": "Better_Practice5365",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/13517xz/what_is_the_fastest_way_to_change_this_excel_date/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "jihebp7",
        "body": "https://letmegooglethat.com/?q=excel+date+to+datetime+pandas",
        "score": 2,
        "created_utc": 1682976107.0,
        "author": "DorsetPerceiver",
        "is_submitter": false,
        "parent_id": "t3_13517xz",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "12lu9zl",
    "title": "Estimating predictability of raw CSV files",
    "selftext": "Seeking opinions on a tool for evaluating dataset predictability.\nFor small/medium datasets in csv format, the tool estimates predictability on the raw data. No need to clean it; just indicate what is the target attribute.\nThe tool uses a robust mixed attribute classifier that does not require the sorting of attributes. Of course, it does not eliminate the process of cleaning data for better results; but it can provide an initial indication of predictability. It can also be used on a smaller sample of cleaned and raw data to get an indication on how the cleaning process improves prediction.\n\nDetails available at:\n\n> https://github.com/c4pub/misc/blob/main/notebooks/csv_dataset_eval.ipynb",
    "url": "https://www.reddit.com/r/datacleaning/comments/12lu9zl/estimating_predictability_of_raw_csv_files/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1681472188.0,
    "author": "zx2zx",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/12lu9zl/estimating_predictability_of_raw_csv_files/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "11zwobt",
    "title": "Open database of hospital prices, uncleaned -- directly from insurance MRF data",
    "selftext": "",
    "url": "https://www.dolthub.com/repositories/dolthub/hospital-prices-allpayers/doc/main",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1679604104.0,
    "author": "alecs-dolt",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/11zwobt/open_database_of_hospital_prices_uncleaned/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1029s1m",
    "title": "What is the American number format?",
    "selftext": " Hello, i‚Äôm trying to dataclean some phone numbers, whereas i do understand the EU format, I have no clue about the US format\n\n001-377-014-0631x83215\n\n469-229-6851x300\n\n001-117-566-5683\n\nHere are couple examples of the data i have, I know the country code is +1 but what is the xNNN that follows some of these numbers, it could be the way they wrote it but there's a lot of similar ones so i dont think its human error",
    "url": "https://www.reddit.com/r/datacleaning/comments/1029s1m/what_is_the_american_number_format/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 3,
    "created_utc": 1672756676.0,
    "author": "No_Bench9222",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/1029s1m/what_is_the_american_number_format/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "j2rx50t",
        "body": "\"x\" is for \"extension\".\n\n469-229-6851x300:\n\n+1-469-229-6851 is the number.  After dialing, you'd have to follow prompts to get to extension 300.",
        "score": 1,
        "created_utc": 1672758649.0,
        "author": "hbdgas",
        "is_submitter": false,
        "parent_id": "t3_1029s1m",
        "depth": 0
      },
      {
        "id": "j2s64dl",
        "body": "Thank you so much that literally cleared up so much for me, Also quick question since i am data cleaning would it be better to keep it in that format +1-469-229-6851 or +1-469-229-6851x300",
        "score": 1,
        "created_utc": 1672762292.0,
        "author": "No_Bench9222",
        "is_submitter": true,
        "parent_id": "t1_j2rx50t",
        "depth": 1
      },
      {
        "id": "j2woq4z",
        "body": "Depends how it's going to be used.",
        "score": 1,
        "created_utc": 1672840590.0,
        "author": "hbdgas",
        "is_submitter": false,
        "parent_id": "t1_j2s64dl",
        "depth": 2
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "zb5xvk",
    "title": "Trifecta Wrangler",
    "selftext": "Does anyone have any experience using this?\n\nI have to do a presentation on this and show my classmates a step by step guide on how to clean a dataset. \n\nSo far I've found that the smart suggestions do most of the work for me. \n\nBefore I get into it even more, anyone have any thoughts/suggestions regarding it?",
    "url": "https://www.reddit.com/r/datacleaning/comments/zb5xvk/trifecta_wrangler/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1670036523.0,
    "author": "Constant-Squirrel555",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/zb5xvk/trifecta_wrangler/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "wt67u4",
    "title": "what attributes would help in identifying a fraud transaction in Ethereum?",
    "selftext": "I'm using this dataset https://www.kaggle.com/datasets/rupakroy/ethereum-fraud-detection.\n\nMy task is to clean it (drop some columns) and in this dataset there is a collection of many fraud and not fraud transactions denoted by flag field.\n\nMy question is which attributes will help me identify if it is a fraud transaction or in other words calculate the flag field, how do we know if the fraud is done over ether or erc20 tokens?\n\nI'm a student with limited knowledge please help me.ü•≤",
    "url": "https://www.reddit.com/r/datacleaning/comments/wt67u4/what_attributes_would_help_in_identifying_a_fraud/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1660999414.0,
    "author": "Correct_Exit_4248",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/wt67u4/what_attributes_would_help_in_identifying_a_fraud/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "w8a9yr",
    "title": "MLOps Community (recorded) session on new open source data prep tool",
    "selftext": "Quickly move your notebooks from research to production with no extra work!  \n[https://www.youtube.com/watch?v=6Iyt9Wip3C4](https://www.youtube.com/watch?v=6Iyt9Wip3C4)\n\nLink to tool: [https://github.com/mage-ai/mage-ai](https://github.com/mage-ai/mage-ai)",
    "url": "https://www.reddit.com/r/datacleaning/comments/w8a9yr/mlops_community_recorded_session_on_new_open/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 0,
    "created_utc": 1658810196.0,
    "author": "ollie_wollie_rocks",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/w8a9yr/mlops_community_recorded_session_on_new_open/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "vt0c4p",
    "title": "Data cleaning webinar: 07/13/2022 at 9:00AM PST",
    "selftext": "Join our CEO & Co-founder, Tommy, as he reveals our new open-source data preparation tool!\n\nRegister: [https://home.mlops.community/home/events/so-fresh-and-so-data-clean-2022-07-13](https://home.mlops.community/home/events/so-fresh-and-so-data-clean-2022-07-13)\n\nSee you live next Wednesday, 07/13/2022 at 9:00 AM PST\n\nhttps://preview.redd.it/36t7nc3538a91.png?width=1280&format=png&auto=webp&s=b032292bb85aa181f77be0d6af0a461f913dbfa8",
    "url": "https://www.reddit.com/r/datacleaning/comments/vt0c4p/data_cleaning_webinar_07132022_at_900am_pst/",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1657140948.0,
    "author": "ollie_wollie_rocks",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/vt0c4p/data_cleaning_webinar_07132022_at_900am_pst/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "vbi99o",
    "title": "Is data cleaning one of your pain points?",
    "selftext": "We just open-sourced the alpha version of our data cleaning tool: [https://github.com/mage-ai/mage-ai](https://github.com/mage-ai/mage-ai)\n\nAny beta testers who would be willing to test and provide feedback?\n\nPlease send any questions or feedback to me or reply here.  \n\nThanks for the consideration!\n\nDemo video: [https://youtu.be/cRib1zOaqWs](https://youtu.be/cRib1zOaqWs)",
    "url": "https://www.reddit.com/r/datacleaning/comments/vbi99o/is_data_cleaning_one_of_your_pain_points/",
    "score": 5,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1655143317.0,
    "author": "ollie_wollie_rocks",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/vbi99o/is_data_cleaning_one_of_your_pain_points/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "ic8zn7k",
        "body": "ngl I actually really enjoy it.  It's a bit of a meditative task for me.",
        "score": 2,
        "created_utc": 1655153939.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_vbi99o",
        "depth": 0
      },
      {
        "id": "iccgqcy",
        "body": "Thanks for the honest feedback! Would you be interested in trying? You could get through more data cleaning :)",
        "score": 2,
        "created_utc": 1655224120.0,
        "author": "ollie_wollie_rocks",
        "is_submitter": true,
        "parent_id": "t1_ic8zn7k",
        "depth": 1
      },
      {
        "id": "iccj7o5",
        "body": "Oh I've pulled the project and am definitely going to tear the hood off and poke around a bit.",
        "score": 1,
        "created_utc": 1655225116.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t1_iccgqcy",
        "depth": 2
      },
      {
        "id": "icd12df",
        "body": "Amazing! Thank you so much :)",
        "score": 1,
        "created_utc": 1655232400.0,
        "author": "ollie_wollie_rocks",
        "is_submitter": true,
        "parent_id": "t1_iccj7o5",
        "depth": 3
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "v11g20",
    "title": "End-To-End Data Preparation with my new open source project: https://github.com/kuwala-io/kuwala",
    "selftext": "",
    "url": "https://i.redd.it/4848a9qcim291.gif",
    "score": 4,
    "upvote_ratio": 0.75,
    "num_comments": 0,
    "created_utc": 1653920941.0,
    "author": "kuwala-io",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/v11g20/endtoend_data_preparation_with_my_new_open_source/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "uty7ra",
    "title": "What tool do you use for data cleaning at your company?",
    "selftext": "",
    "url": "https://www.reddit.com/r/datacleaning/comments/uty7ra/what_tool_do_you_use_for_data_cleaning_at_your/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 5,
    "created_utc": 1653058021.0,
    "author": "Fragrant-Ad8537",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/uty7ra/what_tool_do_you_use_for_data_cleaning_at_your/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "ibtlft9",
        "body": "Python. Dataprep Library.https://dataprep.ai/",
        "score": 2,
        "created_utc": 1654837834.0,
        "author": "Major-Tomatillo-7502",
        "is_submitter": false,
        "parent_id": "t3_uty7ra",
        "depth": 0
      },
      {
        "id": "ky4k3fa",
        "body": "Excel üò≠",
        "score": 1,
        "created_utc": 1712291822.0,
        "author": "rene041482",
        "is_submitter": false,
        "parent_id": "t3_uty7ra",
        "depth": 0
      },
      {
        "id": "i9exevw",
        "body": "Not currently doing data cleaning at my job, but my preferred tools are R and the tidyverse library.",
        "score": 1,
        "created_utc": 1653106286.0,
        "author": "foxfyre2",
        "is_submitter": false,
        "parent_id": "t3_uty7ra",
        "depth": 0
      },
      {
        "id": "i9f2eiz",
        "body": "A combination of Python and BitRook",
        "score": 1,
        "created_utc": 1653109645.0,
        "author": "weber_stephen",
        "is_submitter": false,
        "parent_id": "t3_uty7ra",
        "depth": 0
      },
      {
        "id": "isrm86t",
        "body": "We are using sweephy. It is the best one currently for us.",
        "score": 1,
        "created_utc": 1666068270.0,
        "author": "onurblt",
        "is_submitter": false,
        "parent_id": "t3_uty7ra",
        "depth": 0
      }
    ],
    "comments_extracted": 5
  },
  {
    "id": "ulaxh4",
    "title": "vnlog: richer commandline data processing with standard UNIX tool extensions",
    "selftext": "",
    "url": "https://github.com/dkogan/vnlog",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1652043185.0,
    "author": "dima55",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/ulaxh4/vnlog_richer_commandline_data_processing_with/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "ufgepu",
    "title": "Advice on how to clean/process a data set.",
    "selftext": "I've developed my analytical skills using Looker and some basic Excel work (Pivot tables, charts, calculated fields) but I want to learn more about the nitty gritty behind data and thought it would be good to dive in to a tough project that will challenge me. I'm looking for advice on how to clean and process this data set for analysis.\n\n[https://www.ons.gov.uk/businessindustryandtrade/business/activitysizeandlocation/datasets/businessdemographyreferencetable](https://www.ons.gov.uk/businessindustryandtrade/business/activitysizeandlocation/datasets/businessdemographyreferencetable)\n\nI'm used to working with Excel files that already have the data in tables so this format in the file available for download is very strange to me. I understand I'd need to eventually join the data I need at some point but right now I'm completely clueless on how to go about cleaning/preparing this data. I'm assuming I'd need to write some code, maybe VBA? I've come across the term before but I don't understand its uses. I wrote a bit of Python code a while back to scrape a website and print the data into an Excel file so I've got some knowledge on that front.\n\nI'm not necessarily looking for someone to give me all the answers in detail but if someone could point me in the right direction to a blog post or some useful keywords that go into more detail than \"How to clean data\" so that I can start googling to do my own research - that would be great.\n\nThanks for the help community!\n\nEDIT:\n\nThis youtube video helped me out a bit though I can't seem to find a pattern in the data set to apply the logic\n\n[https://www.youtube.com/watch?v=qHOu0\\_hAj0k&ab\\_channel=KarinaAdcock](https://www.youtube.com/watch?v=qHOu0_hAj0k&ab_channel=KarinaAdcock)",
    "url": "https://www.reddit.com/r/datacleaning/comments/ufgepu/advice_on_how_to_cleanprocess_a_data_set/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1651341563.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/ufgepu/advice_on_how_to_cleanprocess_a_data_set/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "uatx61",
    "title": "HELP: I can't decide how to dealing with missing stock data",
    "selftext": "I am trying to analyse stock data of the reddit White Girl Stock index. I collected historical data from Yahoo finance. The problem is the the list includes both old and young companies like Disney vs Etsy. Disney is much older than Etsy so in my data set I have null values for the years young.\n\n I thought I could just in put 0 but that messes up my mode calculations. I also I could start with the year the youngest company when public, but I loose way too much data. I would like to keep the data for each company from the year they went public.\n\nWhat would you do?\n\nOh note: eventually I would like to do some predictive analytics so the more data i have the better.",
    "url": "https://www.reddit.com/r/datacleaning/comments/uatx61/help_i_cant_decide_how_to_dealing_with_missing/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1650803950.0,
    "author": "cmdr--data",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/uatx61/help_i_cant_decide_how_to_dealing_with_missing/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "i60xamo",
        "body": "Just exclude the nan values.\n\nAlso, from your description of not missing any data in recent years, you're introducing heavy survivorship bias. Blockbuster for example should be missing after 2014.",
        "score": 2,
        "created_utc": 1650822023.0,
        "author": "swierdo",
        "is_submitter": false,
        "parent_id": "t3_uatx61",
        "depth": 0
      },
      {
        "id": "i63sq0r",
        "body": "Great point I didnt even think of that. You're awesome thank you",
        "score": 2,
        "created_utc": 1650873663.0,
        "author": "cmdr--data",
        "is_submitter": true,
        "parent_id": "t1_i60xamo",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "tgg73d",
    "title": "Transformania launches new CRM data cleaning platform",
    "selftext": "**The best new way to clean your CRM data!**\n\n* Want to know which email addresses in your CRM are going to bounce?\n* Need to format and clean the names in your CRM database?\n* Want to find hidden nicknames for better personalization of your CRM contacts?\n* Need to get overall better CRM quality?\n* Want to connect directly from HubSpot, or upload a CSV from Pipedrive, Salesforce, Zoho, Dynamics, etc?\n\nTransformania has launched its new platform that easily and quickly cleans your CRM data!\n\nUse the discount code ESPECIALLY for Redditors for a 50% discount off any credits you buy: reddit50off\n\nVisit: [https://www.transformania.com](https://www.transformania.com)",
    "url": "https://www.reddit.com/r/datacleaning/comments/tgg73d/transformania_launches_new_crm_data_cleaning/",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 1,
    "created_utc": 1647540115.0,
    "author": "transformania1",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/tgg73d/transformania_launches_new_crm_data_cleaning/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "str2me",
    "title": "Hello everyone - I am writing on behalf of an early stage startup venture looking to talk to data science, data architecture, data wrangling, data preparing and/or data engineering and analysis experts purely for research purposes. Would you have 30 mins to talk to us?",
    "selftext": "",
    "url": "https://www.reddit.com/r/datacleaning/comments/str2me/hello_everyone_i_am_writing_on_behalf_of_an_early/",
    "score": 0,
    "upvote_ratio": 0.14,
    "num_comments": 3,
    "created_utc": 1645000729.0,
    "author": "dinoomy",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/str2me/hello_everyone_i_am_writing_on_behalf_of_an_early/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "hxa7ifn",
        "body": "Thank you for the feedback - if you are able to register yourself with user interviews.com and then send me your profile, and LinkedIn details in a private message, then we can have that worked out :)",
        "score": 1,
        "created_utc": 1645084513.0,
        "author": "dinoomy",
        "is_submitter": true,
        "parent_id": "t3_str2me",
        "depth": 0
      },
      {
        "id": "hx8l7uo",
        "body": "If you want to have *good* conversations, you should offer to pay for that time at whatever hourly rate your interlocutor charges.",
        "score": 1,
        "created_utc": 1645053612.0,
        "author": "WallyMetropolis",
        "is_submitter": false,
        "parent_id": "t3_str2me",
        "depth": 0
      },
      {
        "id": "hxa2vl5",
        "body": "I have time. For money.",
        "score": 1,
        "created_utc": 1645081010.0,
        "author": "Resquid",
        "is_submitter": false,
        "parent_id": "t3_str2me",
        "depth": 0
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "sf0phm",
    "title": "Guidance on how to start",
    "selftext": "I have a data frame that will be coming next week, and I need to start working on it, the first step I'll do is to clean it. My question is what do you usually look for when cleaning a set? like duplicates, formatting problems and what?\n\nI need guidance on how to start and what to look for?\n\nAlso, when you remove identical rows/duplicates how do you make sure they're duplicate and not just other identical rows?",
    "url": "https://www.reddit.com/r/datacleaning/comments/sf0phm/guidance_on_how_to_start/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1643403908.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/sf0phm/guidance_on_how_to_start/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "hunybv4",
        "body": "What is the format? What tools are you familiar with/will you be using? Is repeated data expected as a part of the dataset?",
        "score": 1,
        "created_utc": 1643420719.0,
        "author": "SurlyNacho",
        "is_submitter": false,
        "parent_id": "t3_sf0phm",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "s860s8",
    "title": "Matching Data from Two Different Sheets in Same Workbook",
    "selftext": "I have a list of about 120 items in my dataset (of about 60,000+ rows) that I would like to delete. I have a list of these 120 items in another sheet in the same workbook. Can't see to figure out how to get my Vlookup formula to work. Any help?   \n\n\nHere is what the data looks like in the 1st sheet: \n\nhttps://preview.redd.it/sawd7bmesqc81.png?width=1056&format=png&auto=webp&s=8f54502d60c0b535872ef4185a0077e36d0d76b8\n\nAnd then here is the second sheet with the items I'd like to find in the 1st (above) sheet:   \n\n\nhttps://preview.redd.it/4gmdbtgosqc81.png?width=314&format=png&auto=webp&s=477f81b15bc61b58d36333915fe4731c7c5d3f3a\n\nBasically just want to match the items needing to be deleted from sheet two to the first sheet. Any help?",
    "url": "https://www.reddit.com/r/datacleaning/comments/s860s8/matching_data_from_two_different_sheets_in_same/",
    "score": 2,
    "upvote_ratio": 0.75,
    "num_comments": 1,
    "created_utc": 1642640820.0,
    "author": "InitialReasonable789",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/s860s8/matching_data_from_two_different_sheets_in_same/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "htirztm",
        "body": "For what I understand you want it should be something like this:\n\nImagine we gonna search for it the the cell B2 of the worksheet that gave the information of items to delete.\n\n=VLOOKUP(A2;Sheet1!B:B;1;FALSE)\n\nExplaining:\n\nA2 is the the value you are looking for\nSheet1!B:B is the range to return the value\n1 is the column number in the range\nFALSE to return the exact match\n\nI could be wrong in what you want but I think it's that or something similar.\n\nEdit: I do data cleaning as a part time (besides my actual work) through Fiverr. If I could help you in that manner just send me a message.",
        "score": 1,
        "created_utc": 1642719122.0,
        "author": "melproinvestments",
        "is_submitter": false,
        "parent_id": "t3_s860s8",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "ref9ul",
    "title": "Cleaning my 'Dates' Data on my excel dataset.",
    "selftext": "Hey Guys, I have a dataset with about 2,101 different dates. They're in a table with other things like price and locations but, a lot of the dates in the data set do not follow the date format I am using (MM/DD/YYYY),  some use DD/MM/YYYY or something else. How would I tackle this?",
    "url": "https://www.reddit.com/r/datacleaning/comments/ref9ul/cleaning_my_dates_data_on_my_excel_dataset/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 2,
    "created_utc": 1639277357.0,
    "author": "Empty_Profile_5567",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/ref9ul/cleaning_my_dates_data_on_my_excel_dataset/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "ho7rxay",
        "body": "I would suggest regex or dateutil with python if you can [as mentioned here](https://stackoverflow.com/questions/7048828/how-can-i-parse-multiple-unknown-date-formats-in-python#7048905). If you only have a few different formats you could do something manually with conditional formulas and the month, day, and year functions in Excel. If it needs to be repeatable maybe using VBA or python as mentioned. You'll have to decide how to handle situations where month and day are both less than or equal to 12 and it's not clear if it's MM/DD or DD/MM.",
        "score": 1,
        "created_utc": 1639288069.0,
        "author": "ultraStatikk",
        "is_submitter": false,
        "parent_id": "t3_ref9ul",
        "depth": 0
      },
      {
        "id": "hrx1un7",
        "body": "Is there some marker that tells you which one is in which format? For example how do you distinguish if 12/01/2022 is 12 Jan, 2022 or 01 Dec, 2022? Are these the only two or there are more?",
        "score": 1,
        "created_utc": 1641739113.0,
        "author": "easyasasunday",
        "is_submitter": false,
        "parent_id": "t3_ref9ul",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "q7ve2v",
    "title": "Organization of Images for e-Commerce Store",
    "selftext": "Hi Guys\n\nI have an excel file with over 30,000 products and their corresponding image URL links in the following basic format: SKU, Image1, Image2, Image3, Image4, Image5 and so on.\n\nThe quality of many images in this file is very poor and I want to be able to identify them, fix them up and essentially generate a new URL link for each of those images.\n\n&#x200B;\n\nThen, I will import that file back into the master system so that they will reflect on the front end website.\n\nWhat is the best software/method to tackle the above?\n\nThanks a ton.",
    "url": "https://www.reddit.com/r/datacleaning/comments/q7ve2v/organization_of_images_for_ecommerce_store/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1634199687.0,
    "author": "nadalsbicep",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/q7ve2v/organization_of_images_for_ecommerce_store/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "q5r9ys",
    "title": "Data cleaning issues",
    "selftext": "To all the people working with data, Apart from the general issues like\n\n\\- missing values, incorrect formats, trailing spaces, text case, etc\n\n  what are some issues you usually face while cleaning data in your organization",
    "url": "https://www.reddit.com/r/datacleaning/comments/q5r9ys/data_cleaning_issues/",
    "score": 4,
    "upvote_ratio": 0.84,
    "num_comments": 3,
    "created_utc": 1633940507.0,
    "author": "cchaituc",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/q5r9ys/data_cleaning_issues/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "hibikzd",
        "body": "Separating data, address standardization, enforcing rules and generating an automation script - I can get all this done with BitRook.com and more so far",
        "score": 2,
        "created_utc": 1635381000.0,
        "author": "weber_stephen",
        "is_submitter": false,
        "parent_id": "t3_q5r9ys",
        "depth": 0
      },
      {
        "id": "isrmdtr",
        "body": "We had data integrity problems but now they are fixed thanks to the tool that we are using currently",
        "score": 1,
        "created_utc": 1666068372.0,
        "author": "onurblt",
        "is_submitter": false,
        "parent_id": "t3_q5r9ys",
        "depth": 0
      },
      {
        "id": "i7q38eg",
        "body": "Does it support a lookup table? Example: Second National Bank and 2nd National Bank. Or Second National Bank  in Cincinnati vs Second National Bank in Omaha",
        "score": 1,
        "created_utc": 1651959193.0,
        "author": "Next-Good-9178",
        "is_submitter": false,
        "parent_id": "t1_hibikzd",
        "depth": 1
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "ppto8v",
    "title": "Zingg : Open source data reconciliation and deduplication using ML and Spark",
    "selftext": "",
    "url": "/r/dataengineering/comments/pp9o1t/zingg_open_source_data_reconciliation_and/",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1631854568.0,
    "author": "sonalg",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/ppto8v/zingg_open_source_data_reconciliation_and/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "oyh4tl",
    "title": "Data Cleansing Tools for ecommerce retailers",
    "selftext": "Hi Guys\n\nAnyone have any nice solutions which integrate with Shopify?\n\nBasically trying to remove mismatched data.",
    "url": "https://www.reddit.com/r/datacleaning/comments/oyh4tl/data_cleansing_tools_for_ecommerce_retailers/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 5,
    "created_utc": 1628168726.0,
    "author": "nadalsbicep",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/oyh4tl/data_cleansing_tools_for_ecommerce_retailers/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "hgla6a2",
        "body": "What kind of mismatched data do you have?",
        "score": 1,
        "created_utc": 1634202322.0,
        "author": "sonalg",
        "is_submitter": false,
        "parent_id": "t3_oyh4tl",
        "depth": 0
      },
      {
        "id": "hglc98k",
        "body": "Few scenarios:\n\n1.Products + Categories they are in.\n\nEg. I may have Trousers which are in the Toys category. This needs to be identified and fixed.\n\n2. I have descriptions of products which do not belong to a product. \n\nEg. I may have a Fidget Spinner for sale, but the description is of a Leather Glove.\n\n&#x200B;\n\nAny help, appreciated. Thanks for replying!",
        "score": 1,
        "created_utc": 1634204245.0,
        "author": "nadalsbicep",
        "is_submitter": true,
        "parent_id": "t1_hgla6a2",
        "depth": 1
      },
      {
        "id": "hglh96w",
        "body": "I had built something earlier with semantic difference between two columns for predicting the right category for procurement items given their names and descriptions. It was in python, I can share it if you think it can help you.",
        "score": 1,
        "created_utc": 1634208491.0,
        "author": "sonalg",
        "is_submitter": false,
        "parent_id": "t1_hglc98k",
        "depth": 2
      },
      {
        "id": "hglhzgf",
        "body": "That would be potentially very helpful. Please let me know where I can see your build.\n\n&#x200B;\n\nThanks again.",
        "score": 1,
        "created_utc": 1634209050.0,
        "author": "nadalsbicep",
        "is_submitter": true,
        "parent_id": "t1_hglh96w",
        "depth": 3
      },
      {
        "id": "hgln7qb",
        "body": "Sure, put a version at [https://github.com/sonalgoyal/categorizer](https://github.com/sonalgoyal/categorizer) \n\nSome caveats\n\n\\- it has been a while since I used this, so please use it at your discretion\n\n\\- it takes two files, one with the descriptions and the second with the names of categories and makes the best guess based on semantic similarity. \n\n\\- hope it will work for you, feel free to open an issue on git if you need help.",
        "score": 2,
        "created_utc": 1634212614.0,
        "author": "sonalg",
        "is_submitter": false,
        "parent_id": "t1_hglhzgf",
        "depth": 4
      }
    ],
    "comments_extracted": 5
  },
  {
    "id": "otoehi",
    "title": "Help with Cleaning Large Environmental Data Set in Jupiter Notebooks (Python3)",
    "selftext": "I have .csv files from a database that I'm trying to combine in order to perform a Shannon Diversity Index model. I have a Relationship Diagram and have been inputting everything into a Jupiter Notebook using Python3 and I have a list of filters I'm trying to apply but I'm brand new to programming and I'm having trouble quickly/efficiently filtering by multiple criteria (ie. I want data from the .csv within three different ranges, organized by timestamps). I need two of the .csv files (both of which share a key of EVENT\\_ID) so I'm currently taking one .csv and trying to apply the filters, then using the correct EVENT\\_IDs from that filtered set to pull the data needed from the other .csv. Is there an efficient way to do this other than creating multiple smaller .csv files for each parameter?",
    "url": "https://www.reddit.com/r/datacleaning/comments/otoehi/help_with_cleaning_large_environmental_data_set/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1627524805.0,
    "author": "Justhaventfound",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/otoehi/help_with_cleaning_large_environmental_data_set/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "h6zaz37",
        "body": "Not exactly sure what you are trying to do. But maybe you can use pandas to import all csv files as separate dataframes.\n\nThan you can join them and apply filters [in pandas](https://pandas.pydata.org/).",
        "score": 1,
        "created_utc": 1627580125.0,
        "author": "andartico",
        "is_submitter": false,
        "parent_id": "t3_otoehi",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "o4vmyn",
    "title": "Rolling up dates in Pyspark and dealing with negatives",
    "selftext": "Hi All, I am trying to clean a dataset by rolling up dates where the stop date of a row is within 1 day of the start date of the next row. However, I am running into a problem when the start/stop interval of the next record occurs inside the start-stop of the previous record. This creates a negative gap that I don't know how to handle. I detail my problem here with code examples: [https://stackoverflow.com/questions/68058168/dealing-with-negatives-in-roll-ups](https://stackoverflow.com/questions/68058168/dealing-with-negatives-in-roll-ups)\n\nCan anyone help?",
    "url": "https://www.reddit.com/r/datacleaning/comments/o4vmyn/rolling_up_dates_in_pyspark_and_dealing_with/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1624283615.0,
    "author": "schwandog",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/o4vmyn/rolling_up_dates_in_pyspark_and_dealing_with/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "n44m9p",
    "title": "Quantclean, a data cleaning tool for quants",
    "selftext": "Hey,\n\nI made a small program that I called Quantclean that basically help to reformat financial data to US Equity TradeBar format.\n\nYou can find all the information's about it on my repo here: [https://github.com/ssantoshp/quantclean](https://github.com/ssantoshp/quantclean)\n\nI just wanted to know what you think about that?\n\nWould it be useful, do you have any suggestions to make it better?",
    "url": "https://www.reddit.com/r/datacleaning/comments/n44m9p/quantclean_a_data_cleaning_tool_for_quants/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1620069177.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/n44m9p/quantclean_a_data_cleaning_tool_for_quants/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "n1105t",
    "title": "Today's Top-Rated Data Sets Sold on Ethereum",
    "selftext": "",
    "url": "https://rugpullindex.com",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1619690017.0,
    "author": "saltcookies1337",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/n1105t/todays_toprated_data_sets_sold_on_ethereum/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "my9ugh",
    "title": "Need help cleaning survey dataset",
    "selftext": "I'm using openrefine to clean a big messy survey dataset from a survey with over 2,000 entries. The comment boxes were open-ended.\n\nBasically trying to extract locations that people have written into a comment box. I've clustered them as best as I can, but around half of them are comments such as: \"X is at \\*this location\\* and \\*that location\\* and blah blah blah\" and all I want is the two locations, and to remove the extra stuff.\n\nIs there a way to do that on openrefine, and if not, on another program? Thanks!",
    "url": "https://www.reddit.com/r/datacleaning/comments/my9ugh/need_help_cleaning_survey_dataset/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1619362326.0,
    "author": "Melodramaticancholy",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/my9ugh/need_help_cleaning_survey_dataset/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "gvu1qd2",
        "body": "That sounds more like an NLP problem than \"data cleansing\"",
        "score": 2,
        "created_utc": 1619369771.0,
        "author": "Resquid",
        "is_submitter": false,
        "parent_id": "t3_my9ugh",
        "depth": 0
      },
      {
        "id": "gvvvn9v",
        "body": "Try running [owl-analytics.com](https://owl-analytics.com) software.  If you need help DM me",
        "score": 1,
        "created_utc": 1619402968.0,
        "author": "extkking",
        "is_submitter": false,
        "parent_id": "t3_my9ugh",
        "depth": 0
      },
      {
        "id": "h0jei6t",
        "body": "Were you able to solve this. If not can you give a few specific lines from your data sample here (anonymized as required).",
        "score": 1,
        "created_utc": 1622788633.0,
        "author": "easyasasunday",
        "is_submitter": false,
        "parent_id": "t3_my9ugh",
        "depth": 0
      },
      {
        "id": "gvv9i1a",
        "body": "what does that mean?",
        "score": 1,
        "created_utc": 1619390723.0,
        "author": "Melodramaticancholy",
        "is_submitter": true,
        "parent_id": "t1_gvu1qd2",
        "depth": 1
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "mkvrwy",
    "title": "Need Help with Excluding Participants from a Dataset!",
    "selftext": "Hi everyone,\n\nI  am currently working on a large data that consists of 175 participants.  There is approximately 15 participants that I need to exclude because they took extremely long to complete my survey, quick speed through my survey, and their responses were not consistent. My professor says that I  use to create an exclusion dummy variable, I am not quite sure how to create a dummy variable for participants that were too long or quickly speed through my survey. I have not done preliminary analyses to assess for any outliers yet. There are also 3 participants that only answered a  small portion of the survey but have a 100% completion rate.",
    "url": "https://www.reddit.com/r/datacleaning/comments/mkvrwy/need_help_with_excluding_participants_from_a/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1617659166.0,
    "author": "aninii",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/mkvrwy/need_help_with_excluding_participants_from_a/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "gtpfx6i",
        "body": "Never mind I was able to resolve this issue!",
        "score": 1,
        "created_utc": 1617814129.0,
        "author": "aninii",
        "is_submitter": true,
        "parent_id": "t3_mkvrwy",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "kwzpn3",
    "title": "Data cleaning excel data",
    "selftext": "I have a large dataset on excel which shows all countries in the world with there economic indicators statistics for 20 years, but the problem is I have a lot of missing values within this dataset and I‚Äôm not sure how to deal with all the missing values.",
    "url": "https://www.reddit.com/r/datacleaning/comments/kwzpn3/data_cleaning_excel_data/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1610603960.0,
    "author": "silavioavagado",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/kwzpn3/data_cleaning_excel_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "gj7mdyi",
        "body": "Depends on what you‚Äôre doing with the data, but in excel thow your data in a pivot table to get a sense  of whats in there. If its a big set, I would reccomend switching the set into r or python to get a summary table, but if you dont have that option, you can turn on ‚Äúdata analytics toolpack‚Äù in excels options, and use ‚Äúsummary‚Äù, to produce basic descriptive stats and stuff on null values.\n\nIf your just exploring, and want to get a feel for the data, you can turn on filter, and find all the null values that way and figure out what you need to do with them. Hope that helps.",
        "score": 2,
        "created_utc": 1610614786.0,
        "author": "rattacat",
        "is_submitter": false,
        "parent_id": "t3_kwzpn3",
        "depth": 0
      },
      {
        "id": "gj7ouvb",
        "body": "Thanks for answer. I decided to remove all missing values as it quite a big dataset with a lot of missing values it would take crazy amount of time to do an imputation on all missing values and decided to work with remaining data. Do you think this is a viable way as I still have a good amount of data to work with?",
        "score": 1,
        "created_utc": 1610617300.0,
        "author": "silavioavagado",
        "is_submitter": true,
        "parent_id": "t1_gj7mdyi",
        "depth": 1
      },
      {
        "id": "gj7pms2",
        "body": "It really depends on what you are doing with the data. Most reports only deal with a few covarbles at a time so it shouldn‚Äôt be too much of a problem, as long as it isn‚Äôt particularly relavant to the study. (Getting rid of dairy exports on a communication study for example) this is a case of knowing your data, and knowing how to use your data. \n\nKeep in mind sometimes null values are a story in itself. Why something is ommited from certain countries numbers might be something interesting - did a country not have a resource, or it was simply not reported? And what countries? \n\nChecking the datasets data dictionary notes might also be a good idea before taking a hacksaw to your NA‚Äôs.",
        "score": 3,
        "created_utc": 1610618108.0,
        "author": "rattacat",
        "is_submitter": false,
        "parent_id": "t1_gj7ouvb",
        "depth": 2
      },
      {
        "id": "gj7q3n0",
        "body": "Most of the missing values are from third world countries I assume most likely wasn‚Äôt reported but who knows lol I‚Äôm just creating a dashboard from 10 different countries and 15 indicators using tableau and  finding mean mode, regression for the data on SAS EG. Hopefully should be alright. Thanks again appreciate it!",
        "score": 1,
        "created_utc": 1610618600.0,
        "author": "silavioavagado",
        "is_submitter": true,
        "parent_id": "t1_gj7pms2",
        "depth": 3
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "jzm61a",
    "title": "Data Quality Analysts: Talk to us about data quality issues, get a $50 Amazon gift card!",
    "selftext": "Our startup builds quality control tools for data collection.  We‚Äôd like to talk to you about common problems you see in your data collection process, and how you currently detect and fix them.\n\nWe‚Äôre interested in speaking with people who:\n\n- Monitor the quality of large scale (or high value) data collection processes\n- Are responsible for finding and correcting data quality issues\n- Work with data other than personal information/customer data (eg. field reporting)\n- Are in Canada or the USA\n\nIf you fit our requirements, [please complete this short \\(2min\\) screening survey](https://forms.gle/2DAvFtrH3TmY7sZR9).  After we successfully complete the 20-30 minute interview, we‚Äôll email you a $50 gift card.",
    "url": "https://www.reddit.com/r/datacleaning/comments/jzm61a/data_quality_analysts_talk_to_us_about_data/",
    "score": 6,
    "upvote_ratio": 0.99,
    "num_comments": 0,
    "created_utc": 1606152800.0,
    "author": "revelaer",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/jzm61a/data_quality_analysts_talk_to_us_about_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "juu14t",
    "title": "How to Clean JSON Data at the Command Line",
    "selftext": "",
    "url": "https://towardsdatascience.com/how-to-clean-json-data-at-the-command-line-a1f31803f6d?sk=ee66715415335db3d806b49906f154ba",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 1,
    "created_utc": 1605476057.0,
    "author": "ezzeddinabdallah",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/juu14t/how_to_clean_json_data_at_the_command_line/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "gcfqnrb",
        "body": "It's too bad he's never heard of VisiData (visidata.org).  You don't need separate tools for CSV, JSON, fixed-width data, sqlite dbs, etc.  The same commands and slick interactive interface for all data formats.",
        "score": 2,
        "created_utc": 1605481757.0,
        "author": "spw1",
        "is_submitter": false,
        "parent_id": "t3_juu14t",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "jqidqv",
    "title": "How to Clean CSV Data at the Command Line | Part 2",
    "selftext": "",
    "url": "https://ezzeddinabdullah.medium.com/how-to-clean-csv-data-at-the-command-line-part-2-207215881c34",
    "score": 8,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1604865694.0,
    "author": "ezzeddinabdallah",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/jqidqv/how_to_clean_csv_data_at_the_command_line_part_2/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "jk5r5d",
    "title": "How xsv is ~1882x faster than csvkit (51ms vs. 1.6min) when cleaning your data at the command line",
    "selftext": "",
    "url": "https://towardsdatascience.com/how-to-clean-csv-data-at-the-command-line-4862cde6cf0a?sk=3c23f815744550e2e286ac73b9f0e37a",
    "score": 5,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1603956984.0,
    "author": "ezzeddinabdallah",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/jk5r5d/how_xsv_is_1882x_faster_than_csvkit_51ms_vs_16min/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "jdxqd6",
    "title": "How to Clean Text Data at the Command Line",
    "selftext": "",
    "url": "https://towardsdatascience.com/how-to-clean-text-files-at-the-command-line-ce2ff361a16c",
    "score": 4,
    "upvote_ratio": 0.75,
    "num_comments": 0,
    "created_utc": 1603095006.0,
    "author": "ezzeddinabdallah",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/jdxqd6/how_to_clean_text_data_at_the_command_line/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "jajbkq",
    "title": "Automated data validation/cleaning",
    "selftext": "Hi everyone!\n\nI‚Äôm new to this and have a problem whereby weekly/monthly I will have around 400 obs over 20/30 variables that should be roughly the same each week/month but with only slight differences.\n\nI‚Äôve so far found that R‚Äôs Validate package is great for getting passes/failures numerated for one validating factor on each variable \n\n(e.g. V1 > 0) (V2 must equal 1) etc.. \n\nI‚Äôve also found a way to compare dataset from week 1 to the next week‚Äôs information to check that they are equal - is anyone aware of a way to code it so that it must be equal to or greater than by no more than say 10%?\n\nAlso, I‚Äôm wondering if anyone knows a way to have the output show WHICH of the observations failed a validate step, as picking these out and dealing with them is most important.\n\nAnd if anyone has found a way to automate this better than having to import datasets and check each versus the last week - I‚Äôd be incredibly grateful for a heads up (AI, ML, DL etc)\n\nThank you!",
    "url": "https://www.reddit.com/r/datacleaning/comments/jajbkq/automated_data_validationcleaning/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1602613751.0,
    "author": "Jimbeany",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/jajbkq/automated_data_validationcleaning/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "iytup1",
    "title": "Mindful data wrangling",
    "selftext": "",
    "url": "https://medium.com/swlh/mindful-data-wrangling-1029df0a2dd1",
    "score": 8,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1600938781.0,
    "author": "sparkplugslug",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/iytup1/mindful_data_wrangling/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "g6emxd7",
        "body": "We all know that data science is 80% data wrangling / data cleaning. And most of you might have your own process on how to make data cleaning easier for yourself. What process do you guys follow?",
        "score": 1,
        "created_utc": 1600938906.0,
        "author": "sparkplugslug",
        "is_submitter": true,
        "parent_id": "t3_iytup1",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "ive934",
    "title": "Data cleaning feedback",
    "selftext": "Hi All,\n\nI have always been frustrated with data cleaning and the trivial errors I end up fixing each time. That's why, I am thinking of developing a library of functions that can come in handy when cleaning data for ML\n\nLooking to understand what kind of data cleaning steps you repeat often in your work. I am looking into building functions for cleaning textual data, numerical data, date/time data, bash scripts that clean files.\n\nDo any libraries already exist for this? I am used to writing functions from scratch for any specific cleaning I had to do eg correct spelling mistakes, filtering outliers, remove erroneous values.\n\nAny help is appreciated. Thanks.",
    "url": "https://www.reddit.com/r/datacleaning/comments/ive934/data_cleaning_feedback/",
    "score": 5,
    "upvote_ratio": 1.0,
    "num_comments": 5,
    "created_utc": 1600462047.0,
    "author": "crossvalidator",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/ive934/data_cleaning_feedback/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "g5sdu72",
        "body": "It's definitely a good idea to try to minimize the amount of time you spend doing 'rote' activities.  The trick I found with data cleaning is that it's always a little different and you don't always know before you see it, so instead of a library I made an interactive tool, VisiData (visidata.org), which e.g. will convert a column to date from any string with a single keystroke (`@`), or let you select rows with a certain regex, or split columns, etc etc, but most importantly, you can see your data at every step along the way.",
        "score": 2,
        "created_utc": 1600494368.0,
        "author": "spw1",
        "is_submitter": false,
        "parent_id": "t3_ive934",
        "depth": 0
      },
      {
        "id": "g5syr63",
        "body": "This is a great idea. I have also been quite frustrated with the same problem and wanted to explore different solutions. I have taken a different approach to this by writing an [article](https://medium.com/@sparkplugslug19/mindful-data-wrangling-1029df0a2dd1?source=friends_link&sk=aec162ff15dbd999f3f537bdff9c7165). Would love to hear your thoughts on this. TIA",
        "score": 2,
        "created_utc": 1600514954.0,
        "author": "sparkplugslug",
        "is_submitter": false,
        "parent_id": "t3_ive934",
        "depth": 0
      },
      {
        "id": "g5thu83",
        "body": "VisiData looks useful; I will have to try it out. I wonder if it can generate the code for you when you transform data? Case I have in mind is one where new data is coming in every day that needs to be cleaned.",
        "score": 1,
        "created_utc": 1600525924.0,
        "author": "crossvalidator",
        "is_submitter": true,
        "parent_id": "t1_g5sdu72",
        "depth": 1
      },
      {
        "id": "g5thdd6",
        "body": ">These are good steps. Would be better if some examples were shown to illustrate the points. Thanks for sharing",
        "score": 2,
        "created_utc": 1600525768.0,
        "author": "crossvalidator",
        "is_submitter": true,
        "parent_id": "t1_g5syr63",
        "depth": 1
      },
      {
        "id": "g5vi4t0",
        "body": "You can record the commands and replay them on each day's data.  Not exactly like generating the code but might suffice in a quick-n-dirty pipeline.",
        "score": 1,
        "created_utc": 1600554394.0,
        "author": "spw1",
        "is_submitter": false,
        "parent_id": "t1_g5thu83",
        "depth": 2
      }
    ],
    "comments_extracted": 5
  },
  {
    "id": "ivm887",
    "title": "Data cleaning and preprocessing without a single line of code!! #SamoyAI#Api for data cleaning and preprocessing#RapidAPI. Please follow link for full video : https://youtu.be/ue_j4GH4i_Y",
    "selftext": "",
    "url": "https://v.redd.it/uamegcbmd1o51",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1600491761.0,
    "author": "Ps21priyanka",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/ivm887/data_cleaning_and_preprocessing_without_a_single/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "g5sawmi",
        "body": "Full video link:\n\nhttps://youtu.be/ue_j4GH4i_Y",
        "score": 1,
        "created_utc": 1600491930.0,
        "author": "Ps21priyanka",
        "is_submitter": true,
        "parent_id": "t3_ivm887",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "il74xf",
    "title": "Data Cleaning In R Programming Language",
    "selftext": "",
    "url": "https://youtu.be/Tvjrvugpavs",
    "score": 2,
    "upvote_ratio": 0.63,
    "num_comments": 1,
    "created_utc": 1599054612.0,
    "author": "Reginald_Martin",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/il74xf/data_cleaning_in_r_programming_language/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "g3pvhs1",
        "body": "How do you get rid of unwanted data while using the R Programming language? \n\nMaster the art of data cleaning and data pre-processing in this tutorial series on Machine Learning with R by [OdinSchool](https://www.youtube.com/channel/UCtOOo75kcRq_IFNVpHOUeLA). \n\nYou will learn more about methods of handling unwanted data, supervised vs unsupervised, data processing techniques, and so on.",
        "score": 1,
        "created_utc": 1599054779.0,
        "author": "Reginald_Martin",
        "is_submitter": true,
        "parent_id": "t3_il74xf",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "iduvs4",
    "title": "Don't you think data cleaning is a clich√© for any data scientist or ML engineer, so let's see how to clean data with the help of a new library samoy (built on python).. So guys please go and download this lib and try out its function. It's really cool",
    "selftext": "",
    "url": "https://v.redd.it/7qotxs6laci51",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1598008772.0,
    "author": "Ps21priyanka",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/iduvs4/dont_you_think_data_cleaning_is_a_clich√©_for_any/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "hr7gfv",
    "title": "Full cleaning tutorials",
    "selftext": "So last week I found a YouTube video where a guy went through a full set data cleaned and wrangled it and asked the questions he was trying to answer. Let you try to clean and wrangle the data and then did it. It was a great video for learning. I was wondering if there is any other videos that you know of where some take a large set up data and cleans and wrangle and lets you try and wrangle it/clean ahead of time. \n\n\nPs I have found many tutorials of little training videos I am looking for large data sets and full working through all the steps as you tackle a real world problem!",
    "url": "https://www.reddit.com/r/datacleaning/comments/hr7gfv/full_cleaning_tutorials/",
    "score": 9,
    "upvote_ratio": 1.0,
    "num_comments": 7,
    "created_utc": 1594752849.0,
    "author": "Mykguy2",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/hr7gfv/full_cleaning_tutorials/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "fy4ho7h",
        "body": "Can you share that video please?",
        "score": 1,
        "created_utc": 1594800158.0,
        "author": "IIRaVeII",
        "is_submitter": false,
        "parent_id": "t3_hr7gfv",
        "depth": 0
      },
      {
        "id": "fy5aybh",
        "body": "RemindME! 24 hours",
        "score": 1,
        "created_utc": 1594823395.0,
        "author": "Sanaki13",
        "is_submitter": false,
        "parent_id": "t3_hr7gfv",
        "depth": 0
      },
      {
        "id": "fyuawv4",
        "body": "I like to watch David Robinson's Tidy Tuesday videos in YouTube, it's written in R. Every week explores new datasets he's never seen before . He walks you through his thought process and how to clean/visualize the data.",
        "score": 1,
        "created_utc": 1595392260.0,
        "author": "Redosu",
        "is_submitter": false,
        "parent_id": "t3_hr7gfv",
        "depth": 0
      },
      {
        "id": "fyv04e6",
        "body": "Thank you very much",
        "score": 1,
        "created_utc": 1595415616.0,
        "author": "Mykguy2",
        "is_submitter": true,
        "parent_id": "t3_hr7gfv",
        "depth": 0
      },
      {
        "id": "fy5b90e",
        "body": "https://youtu.be/eMOA1pPVUc4!\nThis is the link to the video I was talking about. This is most of what you need to learn data science but I want to do it with like 10-20 data sets so when I interview I feel more confident.",
        "score": 2,
        "created_utc": 1594823554.0,
        "author": "Mykguy2",
        "is_submitter": true,
        "parent_id": "t1_fy4ho7h",
        "depth": 1
      },
      {
        "id": "fy5b6kp",
        "body": "https://youtu.be/eMOA1pPVUc4!\nThis is the link to the video I was talking about. This is most of what you need to learn data science but I want to do it with like 10-20 data sets so when I interview I feel more confident.",
        "score": 3,
        "created_utc": 1594823518.0,
        "author": "Mykguy2",
        "is_submitter": true,
        "parent_id": "t1_fy5aybh",
        "depth": 1
      },
      {
        "id": "fy5y25t",
        "body": "There is a 3 hour delay fetching comments.\n\nI will be messaging you in 1 day on [**2020-07-16 14:29:55 UTC**](http://www.wolframalpha.com/input/?i=2020-07-16%2014:29:55%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/datacleaning/comments/hr7gfv/full_cleaning_tutorials/fy5aybh/?context=3)\n\n[**CLICK THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fdatacleaning%2Fcomments%2Fhr7gfv%2Ffull_cleaning_tutorials%2Ffy5aybh%2F%5D%0A%0ARemindMe%21%202020-07-16%2014%3A29%3A55%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%20hr7gfv)\n\n*****\n\n|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
        "score": 1,
        "created_utc": 1594834580.0,
        "author": "RemindMeBot",
        "is_submitter": false,
        "parent_id": "t1_fy5aybh",
        "depth": 1
      }
    ],
    "comments_extracted": 7
  },
  {
    "id": "hgfy5n",
    "title": "Removing the records that are not english",
    "selftext": "I have a data having 1 million records in it. I view my data and clean it using Pandas, but normally I only see the first 20\\~30 rows or last 20\\~30 rows to analyze my data.\n\nI want something that can take me through the whole data. Say, I have a reviews column that is in english, at some 50,000th record, the review data has random symbols or may be another language. I'd definitely want that record to be deleted. So the question is that if I can't view the whole data, how will I know that there is something wrong in my data right hidden beneath?",
    "url": "https://www.reddit.com/r/datacleaning/comments/hgfy5n/removing_the_records_that_are_not_english/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1593205226.0,
    "author": "TechGennie",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/hgfy5n/removing_the_records_that_are_not_english/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "fw52enp",
        "body": "Transform it into a corpus, run frequencies, and identify words that could be used to sort out observations entered in other languages.",
        "score": 2,
        "created_utc": 1593235445.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_hgfy5n",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "haaci5",
    "title": "Can someone please help me differentiate between data wrangling and data cleaning?",
    "selftext": "Hi all! I‚Äôm currently researching data cleaning and trying to find good information on how it‚Äôs done, as there is not much literature/ guidelines from what I know. However, it seems people often say that data wrangling and data cleaning are the same thing, but I was warned against this and told not to bunch them together. \n\nI know that they are different but it‚Äôs hard to find something that really lays out why. Can someone please explain the difference between them and outline why they are not the same? \n\nThanks so much!",
    "url": "https://www.reddit.com/r/datacleaning/comments/haaci5/can_someone_please_help_me_differentiate_between/",
    "score": 6,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1592334174.0,
    "author": "Mandypandie",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/haaci5/can_someone_please_help_me_differentiate_between/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "fv3jc86",
        "body": "Cleaning is picking up errors, odd things and possibly dealing with things like missing values, inconsistent variable formats, categories etc. \n\nWrangling is taking source data and putting it into a useful form, merging tables, aggregating, filtering etc. \n\nA classic pipeline would be \nextract->clean->wrangle->model",
        "score": 5,
        "created_utc": 1592375014.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_haaci5",
        "depth": 0
      },
      {
        "id": "fvq3lqt",
        "body": "The main difference is that **data wrangling** is the process of converting and mapping data from one format to another format to prepare the data for analyzing, but **data cleaning** is the process of eliminating the inaccurate data.",
        "score": 3,
        "created_utc": 1592900579.0,
        "author": "javeriagauhar",
        "is_submitter": false,
        "parent_id": "t3_haaci5",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "gv8873",
    "title": "How do data scientists clean datasets for training CNNs?",
    "selftext": "Given that there could be millions of examples in these datasets, It's hard to believe it would be a manual process. Is there some kind of automated process to find these misrepresentations?",
    "url": "https://www.reddit.com/r/datacleaning/comments/gv8873/how_do_data_scientists_clean_datasets_for/",
    "score": 5,
    "upvote_ratio": 0.86,
    "num_comments": 0,
    "created_utc": 1591107799.0,
    "author": "zdmwi",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/gv8873/how_do_data_scientists_clean_datasets_for/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "fsmj3j",
    "title": "comparing timestamps in two consecutive rows which have different values for column A and the same value for column B in Big Query",
    "selftext": "Hey guys, I would really appreciate your help on this. I have a Google BigQuery result which shows me the time (in the column `local_time`) that riders (in the column `rider_id`) log out of an app (the column `event`), so there are two distinct values for the column event, \"authentication\\_complete\" and \"logout\".\n\n    event_date\trider_id\tevent\t        local_time\n    20200329\t100695\tauthentication_complete\t20:07:09\n    20200329\t100884\tauthentication_complete\t12:00:51\n    20200329\t100967\tlogout\t                10:53:17\n    20200329\t100967\tauthentication_complete\t10:55:24\n    20200329\t100967\tlogout\t                11:03:28\n    20200329\t100967\tauthentication_complete\t11:03:47\n    20200329\t101252\tauthentication_complete\t7:55:21\n    20200329\t101940\tauthentication_complete\t8:58:44\n    20200329\t101940\tauthentication_complete\t17:19:57\n    20200329\t102015\tauthentication_complete\t14:20:27\n    20200329\t102015\tauthentication_complete\t22:39:42\n    20200329\t102015\tlogout           \t22:47:50\n    20200329\t102015\tauthentication_complete\t22:48:3\n\nwhat I want to achieve is for each rider who ever logged out, in one column I want to get the time they logged out, and in another column I want to get the time for the event \"authentication\\_complete\" that comes right after that logout event for that rider. In this way, I can see the time period that each rider was out of the app. the query result I want to get will look like below.\n\n    event_date rider_id\ttime_of_logout authentication_complete_right_after_logout\n    20200329   100967\t10:53:17\t10:55:24\n    20200329   100967\t11:03:28\t11:03:47\n    20200329   102015\t22:47:50\t22:48:34\n\nThis was a very unclean data set, and so far I was able to clean this much, but at this step, I am feeling very stuck. I was looking into functions like `lag()` but since the data is 180,000 rows, there can be multiple events named \"logout\" for a rider\\_id and there are multiple consecutive events named \"authentication\\_complete\" for the same rider\\_id, it is extra confusing. I would really appreciate any help. Thanks!\n\n[https://stackoverflow.com/questions/60960431/comparing-timestamps-in-two-consecutive-rows-which-have-different-values-for-col](https://stackoverflow.com/questions/60960431/comparing-timestamps-in-two-consecutive-rows-which-have-different-values-for-col)",
    "url": "https://www.reddit.com/r/datacleaning/comments/fsmj3j/comparing_timestamps_in_two_consecutive_rows/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1585692614.0,
    "author": "sbossman",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/fsmj3j/comparing_timestamps_in_two_consecutive_rows/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "ff3qum",
    "title": "Data Cleaning for missing values",
    "selftext": "Hi, \nI have a dataset with time variable year, month, day, form individual column, and I have some green houses gases column follow by these columns. There are some missing values for each of the green houses column. What is the best way to fill these missing values without affect the accuracy of the whole dataset? Please comment below. Thank you",
    "url": "https://www.reddit.com/r/datacleaning/comments/ff3qum/data_cleaning_for_missing_values/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 4,
    "created_utc": 1583624395.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/ff3qum/data_cleaning_for_missing_values/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "fjw0aed",
        "body": "Missing values are truly NULLs. Can you get upstream from that data source to obtain the missing values?",
        "score": 1,
        "created_utc": 1583632956.0,
        "author": "fazeka",
        "is_submitter": false,
        "parent_id": "t3_ff3qum",
        "depth": 0
      },
      {
        "id": "fjxlej7",
        "body": "What is the functional meaning of the table and what are you trying to achieve?\n\nI‚Äôm definitely not an expert, but from what I know you have multiple options here:\n1. You remove the lines with missing data. I wouldn‚Äôt really recommend this drastic approach, but if you‚Äôre sure you don‚Äôt need that data and the missing values will throw off whatever analysis or ... you need the data for. \n2. If possible, you could write a function to input the correct values, based on the other info you have and if you can lookup the correct values. If it‚Äôs not too much data you could maybe do this manually as well. \n3. You input null values if that‚Äôs not already the case (if it‚Äôs all blank spaces in case of a string type or zero‚Äôs in case of integer type). Most programming languages are better equipped at handeling null values rather than blanks. \n4. You input something like e.g. ‚Äò-123‚Äô in case of an integer or ‚Äò$$$‚Äô in case of a string, so your programming language will consider the original missing values as seperate types. \n\nIn case of approach 3 and 4 you can do an analysis on the missing values. \nIn case of approach 1 you don‚Äôt see in your final result the possible impact of the missing values. \nIn case of approach 2 you risk incorrect data. \n\nHope this helps you!",
        "score": 1,
        "created_utc": 1583677078.0,
        "author": "flamingosarecool365",
        "is_submitter": false,
        "parent_id": "t3_ff3qum",
        "depth": 0
      },
      {
        "id": "fjw0j0d",
        "body": "It was from Kaggle..",
        "score": 1,
        "created_utc": 1583633039.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t1_fjw0aed",
        "depth": 1
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "f8t0i7",
    "title": "What's the best way to clean a large dataset on my local (RAM constrained) machine?",
    "selftext": "Hi folks,\n\nI'm wondering how to approach the problem of cleaning/transforming a dataset on my local machine, when the dataset is too large to fit into memory.\n\nMy first thought is to stream it line by line using a Python generator and perform my cleaning steps that way. Is there any existing library or framework that is built around this concept? Or is there a better way to approach this?\n\nThanks.",
    "url": "https://www.reddit.com/r/datacleaning/comments/f8t0i7/whats_the_best_way_to_clean_a_large_dataset_on_my/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1582558852.0,
    "author": "General_Example",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/f8t0i7/whats_the_best_way_to_clean_a_large_dataset_on_my/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "fipkj61",
        "body": "Depending on what exactly you need to do (and if you are on a *if machine like Unix, Linux or MacOS) shell tools like \n\n* sed  \n* awk  \n* grep  \n* cat  \n\nMight be you best friends. Even if the learning curve can be quite steep. \n\nThere are some good introductions and if I am not mistaken even an O'Reilly book on that topic. \n\nYou might get a first idea with an intro like [this one](https://www.datafix.com.au/cookbook/index.html).\n\nFew weeks ago I had a 84GB .csv file to analyze on a MacBook Pro with 16GB of RAM. Got the data down to 19GB and was then able to do the rest in a jupyter notebook with pandas.\n\n[Edit:]\nFound the O'Reilly book: [Data Science at the Command Line](https://www.datascienceatthecommandline.com/)\n\n> To get you started‚Äîwhether you‚Äôre on Windows, macOS, or Linux‚Äîauthor Jeroen Janssens has developed a Docker image packed with over 80 command-line tools.\n\nI believe you can get the book for free as well.",
        "score": 3,
        "created_utc": 1582609828.0,
        "author": "andartico",
        "is_submitter": false,
        "parent_id": "t3_f8t0i7",
        "depth": 0
      },
      {
        "id": "finpfmi",
        "body": "Is it time series based? Also what are language are using? Many ways to things, python‚Äôs dask library is good for running things locally and has built in pandas support.",
        "score": 1,
        "created_utc": 1582567706.0,
        "author": "christophvel",
        "is_submitter": false,
        "parent_id": "t3_f8t0i7",
        "depth": 0
      },
      {
        "id": "fio36qu",
        "body": "Check out dask: https://distributed.dask.org/en/latest/manage-computation.html",
        "score": 1,
        "created_utc": 1582575405.0,
        "author": "christophvel",
        "is_submitter": false,
        "parent_id": "t3_f8t0i7",
        "depth": 0
      },
      {
        "id": "finsg1a",
        "body": "I guess Python is the most convenient. If the dataset is several GB then I can't load it all into memory, so some sort of streaming solution sounds appealing. Is there any framework for doing stream-based data cleaning?",
        "score": 1,
        "created_utc": 1582569250.0,
        "author": "General_Example",
        "is_submitter": true,
        "parent_id": "t1_finpfmi",
        "depth": 1
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "e9ymms",
    "title": "Data Cleaning Guide: Saving 80% of Your Time to Do Data Analysis",
    "selftext": "",
    "url": "https://www.finereport.com/en/data-analysis/data-cleaning-guide-saving-80-of-your-time-to-do-data-analysis.html?utm_source=reddit&utm_medium=media&utm_term=1213_3&utm_content=1213_3",
    "score": 8,
    "upvote_ratio": 0.84,
    "num_comments": 0,
    "created_utc": 1576208754.0,
    "author": "JaneLu0113",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/e9ymms/data_cleaning_guide_saving_80_of_your_time_to_do/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "d9k4vj",
    "title": "Visually explore and analyze Big Data from any Jupyter Notebook",
    "selftext": "Hi everyone, today we are launching Bumblebee [https://hi-bumblebee.com/](https://hi-bumblebee.com/), a platform for big data exploration and profiling that works over pyspark. Can be used for free on your laptop or the cloud also you can find link for Google Colab on the site.\n\nYou can get stats, filter columns by data type, histogram and frequency charts easily.\n\nWe would like to hear your feedback. Just click in the bubble chat a let us know what you think.",
    "url": "https://www.reddit.com/r/datacleaning/comments/d9k4vj/visually_explore_and_analyze_big_data_from_any/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1569506925.0,
    "author": "argenisleon",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/d9k4vj/visually_explore_and_analyze_big_data_from_any/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "d4c01g",
    "title": "Remove rows that are too much alike not to be duplicates",
    "selftext": "I have a [dataset of real estate advertisements](https://docs.google.com/spreadsheets/d/1XUjqeXVgjZJ8jVNAn9MeIb9zHLhVj4mwRs-hk1P050o/edit?usp=sharing). Several of the lines are about the same real estate property so it's full of duplicates that aren't exactly the same. What would be the best methods to remove rows that are too much alike not to be duplicates?\n\nIt looks like this :\n\n&#x200B;\n\n        \tID\tURL\tCRAWL_SOURCE\tPROPERTY_TYPE\tNEW_BUILD\tDESCRIPTION\tIMAGES\tSURFACE\tLAND_SURFACE\tBALCONY_SURFACE\t...\tDEALER_NAME\tDEALER_TYPE\tCITY_ID\tCITY\tZIP_CODE\tDEPT_CODE\tPUBLICATION_START_DATE\tPUBLICATION_END_DATE\tLAST_CRAWL_DATE\tLAST_PRICE_DECREASE_DATE\n        0\t22c05930-0eb5-11e7-b53d-bbead8ba43fe\thttp://www.avendrealouer.fr/location/levallois...\tA_VENDRE_A_LOUER\tAPARTMENT\tFalse\tAu rez de chauss√É¬©e d'un bel immeuble r√É¬©cent,...\t[\"https://cf-medias.avendrealouer.fr/image/_87...\t72.0\tNaN\tNaN\t...\tLamirand Et Associes\tAGENCY\t54178039\tLevallois-Perret\t92300.0\t92\t2017-03-22T04:07:56.095\tNaN\t2017-04-21T18:52:35.733\tNaN\n        1\t8d092fa0-bb99-11e8-a7c9-852783b5a69d\thttps://www.bienici.com/annonce/ag440414-16547...\tBIEN_ICI\tAPARTMENT\tFalse\tJe vous propose un appartement dans la rue Col...\t[\"http://photos.ubiflow.net/440414/165474561/p...\t48.0\tNaN\tNaN\t...\tProprietes Privees\tMANDATARY\t54178039\tLevallois-Perret\t92300.0\t92\t2018-09-18T11:04:44.461\tNaN\t2019-06-06T10:08:10.89\t2018-09-25\n\nSo far I tried to compare the description :\n\n&#x200B;\n\n        df['is_duplicated'] = df.duplicated(['DESCRIPTION'])\n\nAnd to compare the array of photos :\n\n&#x200B;\n\n        def image_similarity(imageAurls,imageBurls):\n            imageAurls = ast.literal_eval(imageAurls)\n            imageBurls = ast.literal_eval(imageBurls)\n            for urlA in imageAurls:\n                responseA = requests.get(urlA)\n                imgA = Image.open(BytesIO(responseA.content))\n                print(imgA)\n                for urlB in imageBurls:\n                    responseB = requests.get(urlB)\n                    imgB = Image.open(BytesIO(responseB.content))    \n                    hash0 = imagehash.average_hash(imgA) \n                    hash1 = imagehash.average_hash(imgB) \n                    cutoff = 5\n        \n                    if hash0 - hash1 < cutoff:\n                        print(urlA)\n                        print(urlB)\n                        return('similar')\n                return('not similar')\n    \n        df['NextImage'] = df['IMAGES'][df['IMAGES'].index - 1]\n        df['IsSimilar'] = df.apply(lambda x: image_similarity(x['IMAGES'], x['NextImage']), axis=1)",
    "url": "https://www.reddit.com/r/datacleaning/comments/d4c01g/remove_rows_that_are_too_much_alike_not_to_be/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1568499842.0,
    "author": "MikeREDDITR",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/d4c01g/remove_rows_that_are_too_much_alike_not_to_be/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "f0di5vq",
        "body": "Sounds like you are looking for Fuzzy Matching.",
        "score": 1,
        "created_utc": 1568554432.0,
        "author": "Omega037",
        "is_submitter": false,
        "parent_id": "t3_d4c01g",
        "depth": 0
      },
      {
        "id": "f0ezwpd",
        "body": "Have a look at record linkage and deduplication methods. You can train a classification model that takes a candidate pair (two properties) and outputs wether these are the same or different properties. \n\nNaturally the features you build will depends on what constitutes a real duplicate in your data. But common features include: levenstein distance and other exit distances, sounded, value and number of digits, geographic distance, initials, etc. \n\nDepending on the size of your data you may also need some form of pre pair blocking or clustering to find similar groups of properties and avoid the need to compare every possible pair.",
        "score": 1,
        "created_utc": 1568575970.0,
        "author": "postb",
        "is_submitter": false,
        "parent_id": "t3_d4c01g",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "c5cshd",
    "title": "Data extraction from scanned documents",
    "selftext": "I've been tasked with coming up with an automated way of processing a large number of scanned documents and extracting key data items from these docs. \n\nThe majority of these are scanned PDFs of varying quality and wildly varying layouts. The data elements im looking to extract are somewhat standardized. Some examples to illustrate : I need to extract client name and that might be recorded in the document as \"Client : client X\", \"client name: client x\", \"CName: client X\". Similarly, to extract invoice date I would look for \"invoice date : mmddyyy\", \"treatment date : dd-MM-yy\", \"incall date - ddmmyyyy\" etc..etc..\n\nI've implemented a solution in R that :\n\n1. Converts a scanned pdf to PNG\n2. Uses Tesseract to run OCR\n3. Uses Regex to extract key data items from the extracted text (6 to 15 items per document, depending on the document type) \n\nEach document type will have a slightly different way the data needs to be extracted. I have created functions to extract individual items e.g. getClientName(), getInvoiceDate() and then combine these into a list, so that for each document I get the extracted items. \n\nThe above works, for most of the simple docs. I can't help feel that regex is a bit unwieldy and might not generalize to all cases - this is supposed to be a process that will used across my organization on a daily basis. My aim is to expose this extraction service as an API so that users in my organization can send pdf, images or text and my API returns key data in JSON.\n\nThis is a very specific use case, but I'm hoping there are others out there that have dealt with similar scenarios. Are there any tools or approaches that might work here? Any other things to be mindful of?",
    "url": "https://www.reddit.com/r/datacleaning/comments/c5cshd/data_extraction_from_scanned_documents/",
    "score": 8,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1561486850.0,
    "author": "elbogotazo",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/c5cshd/data_extraction_from_scanned_documents/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "es19h6y",
        "body": "I haven't used it, but have you tried pdftools (https://docs.ropensci.org/pdftools/)? See this blogpost: https://www.brodrigues.co/blog/2018-06-10-scraping_pdfs/\nAlso checkout Tabulizer (https://datascienceplus.com/extracting-tables-from-pdfs-in-r-using-the-tabulizer-package/)",
        "score": 1,
        "created_utc": 1561491974.0,
        "author": "yaymayhun",
        "is_submitter": false,
        "parent_id": "t3_c5cshd",
        "depth": 0
      },
      {
        "id": "es1awz9",
        "body": "Yes, I'm using pdftools to convert the file to png. Tabulizer is also available in R but the problem there is that it's not great at capturing tables in scanned docs.\n\nIt looks I'll have to do this with regex for extraction and then build rules on to process the extraction outputs. Doable & a nice challenge but just wanted to check if there was something out there that might make this a bit easier.",
        "score": 2,
        "created_utc": 1561492832.0,
        "author": "elbogotazo",
        "is_submitter": true,
        "parent_id": "t1_es19h6y",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "bwx0kw",
    "title": "Need help parsing NPM dependency versions",
    "selftext": "I'm doing a project using some data about npm package dependencies from [libraries.io](https://libraries.io/). My problem right now is that people use a lot of different strings to set their version and I'm not sure I'll be able to write an algorithm to parse them in a reasonable amount of time. So I was hoping someone had come across the problem before and written (or knows of) something that I could use. \n\nHere is a link to the [npm rules for package dependency version strings](https://docs.npmjs.com/files/package.json#dependencies) and here's a [list of some sample data](https://imgur.com/a/w9yKcsL).\n\nEDIT: Tried to clear up language and added links.\n\nEDIT 2: Here is the pseudo code I wrote out:\n\n*Base algorithm:*\n\n1. If it's a URL, drop it.\n2. If it has '||' explode it then:\n  1. Run the helper parser on each part.\n  2. Return the highest number.\n3. Else run hepler on whole string and return result.\n\n*Helper parser:*\n\n1. Trim trailing whitespace\n2. Explode on whitespace\n3. If it's just 1 number:\n  1. If it starts with a ~ or = or ^ return the major version.\n  2. If it starts with > return highest version.\n  3. If it starts with <\n    1. and contains an = or the either of the next two version is greater than 0 return major version listed.\n    2. Else return major minus 1.\n4. If more than one number check is there is a - in the middle slot.\n  1. If there is  find a number between the two.\n  2. If not find a number that satifies both rules.",
    "url": "https://www.reddit.com/r/datacleaning/comments/bwx0kw/need_help_parsing_npm_dependency_versions/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1559698968.0,
    "author": "AnotherSkullcap",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/bwx0kw/need_help_parsing_npm_dependency_versions/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "bjz8cl",
    "title": "What data formats/pipelining do you use to store and wrangle data which contains both text and float vectors ?",
    "selftext": "",
    "url": "https://www.reddit.com/r/LanguageTechnology/comments/bjz5ld/what_data_formatspipelining_do_you_use_to_store/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1556826397.0,
    "author": "BatmantoshReturns",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/bjz8cl/what_data_formatspipelining_do_you_use_to_store/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "axay7q",
    "title": "Data Cleaning CV question.",
    "selftext": "Hello.\nI'm really trying to nail an Analyst/D.S. position. Proficient with Python and SQL.  \nHowever I do not have any real world experience. I have 3 Independent Python projects that I am prideful about and I am quite comfortable with working with CSV files and manipulating DataFrames.\nRecently had an interview for Business Analyst position. The DBM and Hiring Manager were pretty impressed with my Mathematical background but when asked about experience I jumped into trying to explain my projects realizing I should of probably added a GitHub link in my CV.  \nWhat I got from the questions they were asking is that they're big on VBA and SQL.  \nMy intuition tells me that they want to hire me but are unsure about my capabilities and would rather give the position to someone with experience. \nMy question is: \nWhat would be the most effective way of showcasing I am more than capable of cleaning/prepping data? What kinds of skills with cleaning/prepping data are attractive to have?  \nThank you for reading. \nedit: Words",
    "url": "https://www.reddit.com/r/datacleaning/comments/axay7q/data_cleaning_cv_question/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 6,
    "created_utc": 1551726482.0,
    "author": "DudeData",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/axay7q/data_cleaning_cv_question/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "ehtcfuy",
        "body": "Nowadays, paper qualification and technical skills are hygiene factors. Too often, many who come for such interviews, lack data interpretation to insights and presentation skills.  Generally, those who possess most of the 4 required skills (programming, maths, stats and domain expertise)  are those with experience. You need to take on some projects in your freetime and do a presentation on the 4. ",
        "score": 3,
        "created_utc": 1551754197.0,
        "author": "chantzeleong",
        "is_submitter": false,
        "parent_id": "t3_axay7q",
        "depth": 0
      },
      {
        "id": "ekrj1ka",
        "body": "I'm trying to learn more about how data scientists think about data and data cleaning/preparation.  Perhaps you might consider the following question.  \nHow important are purpose for data, impact of data use, context, situation, user (source of data), and user social factors to you when you are data \"cleaning?\"",
        "score": 1,
        "created_utc": 1555116273.0,
        "author": "ethicalbau",
        "is_submitter": false,
        "parent_id": "t3_axay7q",
        "depth": 0
      },
      {
        "id": "ehuqw42",
        "body": "Thank you.",
        "score": 1,
        "created_utc": 1551807481.0,
        "author": "DudeData",
        "is_submitter": true,
        "parent_id": "t1_ehtcfuy",
        "depth": 1
      },
      {
        "id": "ekrlsuv",
        "body": "Through my journey in this DS path I have learned that datas that are stored on large scale are messy to say the least. \nCleaning is essentially an organizational process to pull what you need out of the companys DB and structure it in a way that can be further analyzed.\nIndeed you would check for missing values and address them according to it's significance relating to the context of the companys question/explanation.\nI am not entirely sure what you mean by social factors but yes, you need to have solid relations with your team including the stakeholders.",
        "score": 1,
        "created_utc": 1555118416.0,
        "author": "DudeData",
        "is_submitter": true,
        "parent_id": "t1_ekrj1ka",
        "depth": 1
      },
      {
        "id": "ekrsre6",
        "body": "Thank you for engaging. \nActually I meant the social factors associated with the data's origin.  \nFor example: health data - patients < \n                    loan data - borrowers\n                    crime data - citizens \nI'm wondering about the considerations that DS take on understanding the socio-cultural factors in which the data are based and those that could help answer HOW to clean data better more relevantly?  Does that help? These are the kinds of questions, I would ask if I was a fly on the wall observing data cleaning: \n*Did you assess the type and scope of data in your data sets (for example whether they contain\npersonal data)?\n*Did you consider ways to develop the AI system or train the model without or with minimal use of\npotentially sensitive or personal data?",
        "score": 1,
        "created_utc": 1555124162.0,
        "author": "ethicalbau",
        "is_submitter": false,
        "parent_id": "t1_ekrlsuv",
        "depth": 2
      },
      {
        "id": "eku4vu1",
        "body": "> *Did you assess the type and scope of data in your data sets (for example whether they contain personal data)? *Did you consider ways to develop the AI system or train the model without or with minimal use of potentially sensitive or personal data?\n\n\nNo I have not. My projects consist free-source datas but I am certain I would take any precautions needed as to not jeopardize the datas integrity.  \n> HOW to clean data better more relevantly?  \n\n\nBetter? As opposed to what? \n> health data - patients < loan data - borrowers crime data - citizens \n  \nI have no idea what you are referring to.",
        "score": 1,
        "created_utc": 1555200928.0,
        "author": "DudeData",
        "is_submitter": true,
        "parent_id": "t1_ekrsre6",
        "depth": 3
      }
    ],
    "comments_extracted": 6
  },
  {
    "id": "aw7c42",
    "title": "Removing near-duplicates from an excel data set",
    "selftext": "I'm trying to clean up a set of data in excel that has names of places repeated incorrectly. For example, I frequently see WP Davidson listed three different ways:\n\n* WP Davidson (Mobile\n* WP Davidson (Mobile AL)\n* WP Davidson (Mobile, AL)\n\nI currently have a data set of roughly 8700 unique places, but I think it should be closer to 4000-5000 after removing these duplicates. Is there an easy way to do this?\n\n&#x200B;",
    "url": "https://www.reddit.com/r/datacleaning/comments/aw7c42/removing_nearduplicates_from_an_excel_data_set/",
    "score": 5,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1551460746.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/aw7c42/removing_nearduplicates_from_an_excel_data_set/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "ehkfziz",
        "body": "Never done it myself, but it sounds like you are looking for fuzzy matching logic. There are several addins and ETL tools that can do this. This is an example, never used the product myself. https://www.ablebits.com/docs/excel-find-fuzzy-duplicates/\n",
        "score": 1,
        "created_utc": 1551461921.0,
        "author": "steel13",
        "is_submitter": false,
        "parent_id": "t3_aw7c42",
        "depth": 0
      },
      {
        "id": "ehmhfab",
        "body": "This has a number of clustering algorithms for deduplication.\n\n[http://openrefine.org/](http://openrefine.org/)",
        "score": 1,
        "created_utc": 1551530818.0,
        "author": "kamonohashisan",
        "is_submitter": false,
        "parent_id": "t3_aw7c42",
        "depth": 0
      },
      {
        "id": "ehkx3i5",
        "body": "This worked, thanks!",
        "score": 1,
        "created_utc": 1551473529.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t1_ehkfziz",
        "depth": 1
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "a5he7w",
    "title": "NeurIPS 2018 Recap by Forge.AI",
    "selftext": "",
    "url": "https://hackernoon.com/neurips-2018-recap-eed61eea8b39",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 0,
    "created_utc": 1544614748.0,
    "author": "jenniferlum",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/a5he7w/neurips_2018_recap_by_forgeai/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "a4vx0u",
    "title": "Data cleansing vendors",
    "selftext": "I'm curious what experience with data cleansing vendors are out there. I've worked with fun and Bradstreet, are there others? Thoughts?",
    "url": "https://www.reddit.com/r/datacleaning/comments/a4vx0u/data_cleansing_vendors/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 5,
    "created_utc": 1544450648.0,
    "author": "ocho747",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/a4vx0u/data_cleansing_vendors/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "eckrp10",
        "body": "I don't know about vendors but if it is text cleaning, kindly follow this [article](https://medium.com/@yuganthadiyal/textcleaner-a-data-pre-processing-library-1a234eec02dd) and mentioned python library.\n\nRegards.",
        "score": 1,
        "created_utc": 1545815246.0,
        "author": "yuganthm",
        "is_submitter": false,
        "parent_id": "t3_a4vx0u",
        "depth": 0
      },
      {
        "id": "eh1ji4k",
        "body": "My company [Woyera](https://www.woyera.com) provides an API that makes it super easy to clean and QA data ",
        "score": 1,
        "created_utc": 1550856514.0,
        "author": "atomic_explosion",
        "is_submitter": false,
        "parent_id": "t3_a4vx0u",
        "depth": 0
      },
      {
        "id": "ej589g1",
        "body": "Makes sense. Can you have it call the API to cleanse and have a new Target for the cleansed data? \n\nSo say I want to cleanse crm data and put it in a reporting db. Can I have your API read from the CRM, cleanse, then write to a MySQL db or something like redshift?",
        "score": 1,
        "created_utc": 1553285629.0,
        "author": "ocho747",
        "is_submitter": true,
        "parent_id": "t3_a4vx0u",
        "depth": 0
      },
      {
        "id": "ej0ep5s",
        "body": "Does that work as a Middleware layer for calling the API? \n\nWhat I'm trying to ask is how it works in simple terms.",
        "score": 1,
        "created_utc": 1553136814.0,
        "author": "ocho747",
        "is_submitter": true,
        "parent_id": "t1_eh1ji4k",
        "depth": 1
      },
      {
        "id": "ej32lil",
        "body": "No it is the API. You pass your data to the API and your cleaning runs based on things you can select in the web app. Then once it is done, you get your data back as a result",
        "score": 1,
        "created_utc": 1553216566.0,
        "author": "atomic_explosion",
        "is_submitter": false,
        "parent_id": "t1_ej0ep5s",
        "depth": 2
      }
    ],
    "comments_extracted": 5
  },
  {
    "id": "a29cvf",
    "title": "Noob data cleaning question",
    "selftext": "Hi everyone,\n\nI am working on cleaning dataset that requires me to calculate a total time between a person's bedtime and wake time. Some participants are good about reporting a single hour (e.g., 10pm) whereas others report a range (e.g., 9-11pm). Obviously this makes it difficult to accurately calculate a total hours sleep variable.\n\n&#x200B;\n\nWhat is best practice for dealing with the latter? Should I just recode those as missing (i.e., 999) or is there a system I should follow? Thanks in advance!",
    "url": "https://www.reddit.com/r/datacleaning/comments/a29cvf/noob_data_cleaning_question/",
    "score": 3,
    "upvote_ratio": 0.8,
    "num_comments": 3,
    "created_utc": 1543716702.0,
    "author": "sikeguy88",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/a29cvf/noob_data_cleaning_question/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "eb5arpd",
        "body": "I would never recommend replacing the data with 999 or dismissing it, it really depends on how much of the total data set is a range you might just consider averaging out the numbers for example 9AM-11AM will be 10AM and see if there is a large error with the end result and with that error result fine tune the model ",
        "score": 2,
        "created_utc": 1544027084.0,
        "author": "walhaider",
        "is_submitter": false,
        "parent_id": "t3_a29cvf",
        "depth": 0
      },
      {
        "id": "eawh0ml",
        "body": "I'm not sure about \"best practice\" but I think the answer is \"it depends\". If your population of variable times is small, you might be able to get away with averaging. For example, if your ranges are a small sample of the population, you may be able to get away with averaging, for example the range of 9-11PM could be averaged to 10PM and that might be enough to fit the rest of your set. If the majority of the population has a range, I would represent the range as the result and say 8-10 hours depending on wake and sleep ranges (assuming they could say something like sleep 10-11PM,  wake 7-8AM, assuming minimal would be 11PM-7AM and max would be 10PM-8AM). It also depends on the audience and how critical the result is to other decisions. If its necessary to be as precise as possible, I wouldn't average anything and report the results as accurately as possible. If you feel its appropriate to generalize/average, then do so to make the results cleaner, just make sure you make note of it when reporting the results. Good luck.",
        "score": 1,
        "created_utc": 1543725261.0,
        "author": "ultraStatikk",
        "is_submitter": false,
        "parent_id": "t3_a29cvf",
        "depth": 0
      },
      {
        "id": "eawpmfd",
        "body": "Be Sure to document the method you use. Since you have to find the sum I am uncertain of  how to approach this but I would have calculated the min, max and mean (assuming uniform distribution) statistic for each ranged value reported and calculate the sum for each. The idea is to get a range from the information they give. Hopefully the range is small. I assume your data is from different individuals each at one point in time i.e. not time series data. \n\n&#x200B;",
        "score": 1,
        "created_utc": 1543735272.0,
        "author": "shaq1f",
        "is_submitter": false,
        "parent_id": "t3_a29cvf",
        "depth": 0
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "9ucpo1",
    "title": "Outsource Web Scraping - The Right Option for Your Business",
    "selftext": "",
    "url": "https://www.offshoreindiadataentry.com/blog/2018/11/02/outsource-web-scraping-right-option-for-business/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1541416927.0,
    "author": "chrissteveuk",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/9ucpo1/outsource_web_scraping_the_right_option_for_your/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "9lpqlr",
    "title": "Show reddit: we launched an unlimited data cleaning service",
    "selftext": "",
    "url": "https://www.reddit.com/r/datascience/comments/9lm6xi/show_reddit_we_launched_an_unlimited_data/",
    "score": 4,
    "upvote_ratio": 0.75,
    "num_comments": 4,
    "created_utc": 1538770072.0,
    "author": "Coup1",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/9lpqlr/show_reddit_we_launched_an_unlimited_data/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "e8h9yl8",
        "body": "Hi,\n\naren't you legally required to include an address of your company or a similar thing on the website? No Information about the legal status of your corporation as well.\n\nIf it's based in Germany I am pretty sure that is required.\n\nIf you want to get enterprise customers (with that rate it seems like it), you need credibility first.",
        "score": 1,
        "created_utc": 1540545515.0,
        "author": "pythonr",
        "is_submitter": false,
        "parent_id": "t3_9lpqlr",
        "depth": 0
      },
      {
        "id": "e8scyua",
        "body": "pushshift timeline subreddit=datacleaning after=0 frequency=week",
        "score": 1,
        "created_utc": 1540993500.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_9lpqlr",
        "depth": 0
      },
      {
        "id": "e968ivp",
        "body": "thanks for pointing that out, although we never had issues with our customers. We added a section. ",
        "score": 1,
        "created_utc": 1541527990.0,
        "author": "Coup1",
        "is_submitter": true,
        "parent_id": "t1_e8h9yl8",
        "depth": 1
      },
      {
        "id": "e8scyyf",
        "body": "[Here is the data you requested.](https://slackbot.pushshift.io/files/15161d82-8368-4782-942d-ae8729e8c464.png)",
        "score": 1,
        "created_utc": 1540993503.0,
        "author": "pushshift_bot",
        "is_submitter": false,
        "parent_id": "t1_e8scyua",
        "depth": 1
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "9eacor",
    "title": "Join r/MachinesLearn!",
    "selftext": "With the permission from moderators, let me invite you to join the new AI subreddit: [r/MachinesLearn](https://www.reddit.com/r/MachinesLearn).\n\nThe community is oriented on practitioners in the AI field, so tutorials, reviews, and news on practically useful machine learning algorithms, tools, frameworks, libraries and datasets are welcome.\n\nJoin us!\n\n(Thanks to mods for allowing this post.)",
    "url": "https://www.reddit.com/r/datacleaning/comments/9eacor/join_rmachineslearn/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 0,
    "created_utc": 1536466526.0,
    "author": "lohoban",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/9eacor/join_rmachineslearn/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "8xsxju",
    "title": "Poll: Reoccurring data formatting problems",
    "selftext": "Was thinking it'd be interesting to aggregate common data transformation and formatting problems that we run into, based on our jobs. (Disclosure: I'm thinking through building a data cleaning tool). \n\nI'll start.\n\n**Role:** Head of Marketing/Growth\n\n**Company Size:** 15\n\n**Type:** Enterprise tech startup\n\n\n**Common problems:**\n\nI spend a lot of time generating leads for outbound sales campaigns. A lot of my problems revolve around:\n\n \n* Converting user-input phone numbers to the same format. \n\n\n* Catching entries that are not emails (e.g. joe.com or joe@gmail)\n\n\n* Finding duplicates of contacts from the same company\n\n\nWhat issues do you run into?",
    "url": "https://www.reddit.com/r/datacleaning/comments/8xsxju/poll_reoccurring_data_formatting_problems/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1531254179.0,
    "author": "hellopolymers",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/8xsxju/poll_reoccurring_data_formatting_problems/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "8s8skj",
    "title": "Data Preparation Gripes/Tips",
    "selftext": "x-post from /r/datascience\n\nJust curious what everyone else's biggest gripes with data preparation are, and if you have any tips/tricks that help you get through it faster.\n\nThanks.",
    "url": "https://www.reddit.com/r/datacleaning/comments/8s8skj/data_preparation_gripestips/",
    "score": 3,
    "upvote_ratio": 0.81,
    "num_comments": 2,
    "created_utc": 1529414309.0,
    "author": "all_about_effort",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/8s8skj/data_preparation_gripestips/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "e5lebf5",
        "body": "One recurring problem I solve cleaning typed survey inputs. For some fields, using a drop down menu is just too inefficient, so you'll end up with 10 different spellings, plus all these systematic misspellings that you'll need to map back to a single entity. Instead of coding the manipulations, you could simply use this library: https://github.com/ChrisMuir/refinr",
        "score": 1,
        "created_utc": 1536377044.0,
        "author": "justUseAnSvm",
        "is_submitter": false,
        "parent_id": "t3_8s8skj",
        "depth": 0
      },
      {
        "id": "e5lec17",
        "body": "Knowing about this a year ago would have saved me many hours of work!",
        "score": 1,
        "created_utc": 1536377065.0,
        "author": "justUseAnSvm",
        "is_submitter": false,
        "parent_id": "t1_e5lebf5",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "8s2ism",
    "title": "Forge.AI - Veracity: Models, Methods, and Morals",
    "selftext": "",
    "url": "https://medium.com/forge-ai/veracity-models-methods-and-morals-600ccef4690",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1529351915.0,
    "author": "jenniferlum",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/8s2ism/forgeai_veracity_models_methods_and_morals/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "8le00s",
    "title": "Forge.AI - Takeaways from TensorFlow Dev Summit 2018",
    "selftext": "",
    "url": "https://medium.com/forge-ai/takeaways-from-the-tensorflow-dev-summit-2018-4440de73ee9d",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1527026195.0,
    "author": "jenniferlum",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/8le00s/forgeai_takeaways_from_tensorflow_dev_summit_2018/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "8jjzco",
    "title": "Help with cleaning txt file!",
    "selftext": "I have a dataset that has multiple headers on different rows. Also the values are not directly beneath those headers. I have difficulties in trying to separate all the headers into different columns. Within this text file it also contains repeating chunks of different data but they have the same headers as the first. I have no clue on how to start cleaning this data.",
    "url": "https://www.reddit.com/r/datacleaning/comments/8jjzco/help_with_cleaning_txt_file/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1526372880.0,
    "author": "Cushionman",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/8jjzco/help_with_cleaning_txt_file/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "dz0po1l",
        "body": "Might be able to give more concrete information if we had an example of the first few lines.\n\nIs the non-header data numeric? Then you could write a script that chunks the file into data labels (i.e. rows that contain headers and repeated headers) and the data itself (i.e. rows that contain numeric cells). Then you could collapse the headers vertically so you have 1 header string for each column.\n\nWith some example data, I could give more specific details.",
        "score": 1,
        "created_utc": 1526398124.0,
        "author": "mitchellpkt",
        "is_submitter": false,
        "parent_id": "t3_8jjzco",
        "depth": 0
      },
      {
        "id": "dz1h149",
        "body": "How big is the file?  Is it ANSI or UTF-8? Are the line endings UNIX or Windows? Is there something specific you‚Äôre trying to extract or make readable?",
        "score": 1,
        "created_utc": 1526422589.0,
        "author": "SurlyNacho",
        "is_submitter": false,
        "parent_id": "t3_8jjzco",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "8gms5e",
    "title": "Pythonic Data Cleaning With NumPy and Pandas ‚Äì Real Python",
    "selftext": "",
    "url": "https://realpython.com/python-data-cleaning-numpy-pandas/",
    "score": 5,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1525311954.0,
    "author": "Roon",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/8gms5e/pythonic_data_cleaning_with_numpy_and_pandas_real/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "8f6rgg",
    "title": "7 Steps to Mastering Data Preparation with Python",
    "selftext": "",
    "url": "https://www.kdnuggets.com/2017/06/7-steps-mastering-data-preparation-python.html",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1524781691.0,
    "author": "Roon",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/8f6rgg/7_steps_to_mastering_data_preparation_with_python/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "8emhbm",
    "title": "Best Graphic User Interface tools for data cleaning?",
    "selftext": "I am curious if there are good tools with user interface to review, clean and prepare data for machine learning.\n\nBased on my work experience in Excel extensively I would prefer to avoid as much command line as possible when developing my ML workflow.\n\nI am not scared of code but would prefer to do all my data cleaning with a tool and then begin working with clean data command line.\n\nWhat popular commercial or open source tools exist?\n\nI could clean data well using Excel I am a complete Excel expert but I am going to need a stronger framework when working with image data or any large data sets.\n\nThe more popular the tool the better as I often rely on blog posts and troubleshooting guides to complete my projects.\n\nThanks for your consideration.",
    "url": "https://www.reddit.com/r/datacleaning/comments/8emhbm/best_graphic_user_interface_tools_for_data/",
    "score": 5,
    "upvote_ratio": 1.0,
    "num_comments": 9,
    "created_utc": 1524592788.0,
    "author": "Amazon-SageMaker",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/8emhbm/best_graphic_user_interface_tools_for_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "dxxphdi",
        "body": "Tableau, though expensive, is one of the best answers here. To be honest, though, I prefer both Python and R, because it's just so much more powerful, and reproducible. I recently had a dataset that needed to be cleaned, and now I have a very similar one. With R, which I used this time, I can basically just rerun the analysis for the most part (sometimes changing out variable names, which a simple search and replace does for me). ",
        "score": 2,
        "created_utc": 1524653885.0,
        "author": "aizheng",
        "is_submitter": false,
        "parent_id": "t3_8emhbm",
        "depth": 0
      },
      {
        "id": "dxxqvij",
        "body": "Take a look at EasyMorph, CSVed, reCSVeditor, DataCleaner, and KNIME.",
        "score": 2,
        "created_utc": 1524656308.0,
        "author": "SurlyNacho",
        "is_submitter": false,
        "parent_id": "t3_8emhbm",
        "depth": 0
      },
      {
        "id": "e5leeay",
        "body": "OpenRefine. It has a ton of functionality for cleaning up text, manipulating columns, etc. Further, it has a nice feature where you can export all of your commands to a json file, making it somewhat reproducible!",
        "score": 1,
        "created_utc": 1536377143.0,
        "author": "justUseAnSvm",
        "is_submitter": false,
        "parent_id": "t3_8emhbm",
        "depth": 0
      },
      {
        "id": "dxy406y",
        "body": "Ideally the platform could take custom operations as well so that Python script that is used repeatedly is a custom function within the program.\n\nPreviews the transformation it is about to make and I confirm it etc.\n\nI just have spent most of my career intensive Excel and in past got way more development done with tools like Eclipse than doing everything command line.\n\nThank you for info.",
        "score": 1,
        "created_utc": 1524670503.0,
        "author": "Amazon-SageMaker",
        "is_submitter": true,
        "parent_id": "t1_dxxphdi",
        "depth": 1
      },
      {
        "id": "dxy42x7",
        "body": "Thank you.\n\nDo you use any of these?\n\nAre any particularly good at pre-processing images?\n\nI imagine image data cleaning is a lot more extensive than empty columns and duplicates etc in business data sets.",
        "score": 1,
        "created_utc": 1524670570.0,
        "author": "Amazon-SageMaker",
        "is_submitter": true,
        "parent_id": "t1_dxxqvij",
        "depth": 1
      },
      {
        "id": "dxyd94g",
        "body": "I'm not sure I quite understand you. Eclipse is an IDE, so anyone I know who uses it, uses it to program. R has a very good IDE (RStudio). For Python, I tend to use notebooks (jupyter) (especially for data cleaning), which also directly give you the output of your commands. Both of them let you make an operation without assignment, and show you what it looks like, and then you can assign (I do this quite often when I'm not sure). Rstudio also lets you see and work around in the table itself. \nOtherwise, if you work well with Excel (and I am very, very hesitant to promote this), why not stick with it, and learn how to use python to write custom functions e.g.? Or write visual basic macros, if you already know visual basic... Again, reproducibility is the key thing you're missing out on then. ",
        "score": 2,
        "created_utc": 1524678399.0,
        "author": "aizheng",
        "is_submitter": false,
        "parent_id": "t1_dxy406y",
        "depth": 2
      },
      {
        "id": "dy0zq77",
        "body": "I use all of them on a fairly frequent basis. I‚Äôm not sure how they would fare for image/binary file data, but KNIME is the only one that handles it.  I‚Äôd have to look, but Orange may also handle image data either directly or via a plug-in.",
        "score": 2,
        "created_utc": 1524781426.0,
        "author": "SurlyNacho",
        "is_submitter": false,
        "parent_id": "t1_dxy42x7",
        "depth": 2
      },
      {
        "id": "dxzqdqr",
        "body": "I will likely use Excel when applicable but will need a more powerful tool for most tasks.\n\nEclipse all I mean is I started developing Python following guides which took me command line and once I started using Eclipse GUI with more experience I was moving a lot faster than navigating command line.\n\nThanks again for your advice.",
        "score": 1,
        "created_utc": 1524732658.0,
        "author": "Amazon-SageMaker",
        "is_submitter": true,
        "parent_id": "t1_dxyd94g",
        "depth": 3
      },
      {
        "id": "dy88hqh",
        "body": "Hey I am starting to get a good idea planned of my \"full stack\" for machine learning problems and am curious if you think I am missing any pieces.\n\nKNIME would be the key tool for reviewing starting data and performing cleaning operations.\n\nThen I would move it to AWS SageMaker and avoid any data manipulation there coming back out to KNIME to do additional cleaning if needed.\n\nThanks for your feedback it is much appreciated.\n\n**1 Gather Data - Download data directly from web or provided links**\n\n**2 Review Data - Load data into KNIME analytics platform, Review using visual data exploration tools**\n\n**3 Pre-process data - Use KNIME data transformation tools to pre-process and clean data for machine learning purposes.**\n\n* Once complete export the ‚Äúclean‚Äù data and load into AWS S3 bucket as a starting point.\n\n**4 Create machine learning models - Use AWS SageMaker high level API to train models**\n\n* Generate trained model endpoints that can be queried to predict based on trained models\n\n* Any additional data manipulation required during the model training process will be done on KNIME and new ‚Äúclean‚Äù sets will be uploaded\n\n* All hyper-parameter variance while training will be done on AWS SageMaker\n\n**5 Expose model endpoints to web - AWS Lambda / AWS Web API Gateway to create a function that references the SageMaker endpoint and creates a useful output in JSON**\n\n**6 Web application references exposed machine learning model endpoints**\n\n* Queries AWS SageMaker API endpoints and formats results\n\n* Web applications are for transformation / visual display and uploading only.\n\n* Robust ‚Äúupload image‚Äù and various input user interface.\n\n* All actual application logic to calculate the results it displays happens via AWS applications that are exposing endpoints for the application to reference.\n\n* I already hired an experienced web developer to make a \"starter application\" in which I can make minor changes to version in new projects",
        "score": 1,
        "created_utc": 1525116903.0,
        "author": "Amazon-SageMaker",
        "is_submitter": true,
        "parent_id": "t1_dy0zq77",
        "depth": 3
      }
    ],
    "comments_extracted": 9
  },
  {
    "id": "8bk2n9",
    "title": "How We're Using Natural Language Generation to Scale at Forge.AI",
    "selftext": "",
    "url": "https://medium.com/@Forge_AI/how-were-using-natural-language-generation-to-scale-at-forge-ai-f7f99120504e",
    "score": 4,
    "upvote_ratio": 0.84,
    "num_comments": 0,
    "created_utc": 1523476582.0,
    "author": "jenniferlum",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/8bk2n9/how_were_using_natural_language_generation_to/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "8a3wr6",
    "title": "Clustering Based Unsupervised Learning",
    "selftext": "",
    "url": "https://medium.com/@sadatnazrul/clustering-based-unsupervised-learning-8d705298ae51",
    "score": 4,
    "upvote_ratio": 0.84,
    "num_comments": 0,
    "created_utc": 1522965058.0,
    "author": "snazrul",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/8a3wr6/clustering_based_unsupervised_learning/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "8a2lkh",
    "title": "Software Development Design Principles",
    "selftext": "",
    "url": "https://medium.com/@sadatnazrul/software-development-design-principles-79d15ef765f3",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1522955636.0,
    "author": "snazrul",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/8a2lkh/software_development_design_principles/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "89wx6p",
    "title": "How to make your Software Development experience‚Ä¶ painless‚Ä¶.",
    "selftext": "",
    "url": "https://medium.com/@sadatnazrul/how-to-make-your-software-development-experience-painless-2591ebcc69b6",
    "score": 4,
    "upvote_ratio": 0.75,
    "num_comments": 0,
    "created_utc": 1522903796.0,
    "author": "snazrul",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/89wx6p/how_to_make_your_software_development_experience/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "89mt28",
    "title": "Data Science Interview Guide",
    "selftext": "",
    "url": "https://medium.com/@sadatnazrul/data-science-interview-guide-4ee9f5dc778",
    "score": 15,
    "upvote_ratio": 0.94,
    "num_comments": 0,
    "created_utc": 1522820475.0,
    "author": "snazrul",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/89mt28/data_science_interview_guide/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "898s4b",
    "title": "A Way to Standardize This Data?",
    "selftext": "Not sure if theres a reasonable way to do this but wanted to see if anyone more knowledgeable had an idea.\n\nI have 2 reports that I want to join based on fund name. I have a report that has 30k funds scraped from morningstar and a report from a company with participants and fund names. Fund name is the only similar field between the 2 reports. I have tickers on the morningstar report but unfortunately am missing them on the company report.\n\nI want the reports joined so that I can match the rate of return per morningstar to the participant.\n\nThe issue is the fund names are named slightly different on both reports. An example is:\nFidelity Freedom 2020 K verse Fid Freed K Class 2020\n\nSo I was just wondering is there a way to somehow standardize the data so that they will match without manually going through all 30 thousand records or is it most likely not going to work?\n\n",
    "url": "https://www.reddit.com/r/datacleaning/comments/898s4b/a_way_to_standardize_this_data/",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1522725809.0,
    "author": "audit157",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/898s4b/a_way_to_standardize_this_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "dwplo4c",
        "body": "This is a problem called entity resolution, and it's explored pretty well here:\n\nhttps://www.districtdatalabs.com/basics-of-entity-resolution/",
        "score": 3,
        "created_utc": 1522726772.0,
        "author": "gulittis_journal",
        "is_submitter": false,
        "parent_id": "t3_898s4b",
        "depth": 0
      },
      {
        "id": "dy0hqdz",
        "body": "Do the reports not include the trading symbol? e.g. Fidelity Freedom 2020 K = FFFDX?",
        "score": 1,
        "created_utc": 1524765078.0,
        "author": "mitchellpkt",
        "is_submitter": false,
        "parent_id": "t3_898s4b",
        "depth": 0
      },
      {
        "id": "dwrsg3c",
        "body": "Thanks for posting that. Interesting read. ",
        "score": 1,
        "created_utc": 1522809536.0,
        "author": "GBR24",
        "is_submitter": false,
        "parent_id": "t1_dwplo4c",
        "depth": 1
      },
      {
        "id": "dx1smja",
        "body": "Thanks! It is pretty interesting.  Im unfortunately not even close to that level yet",
        "score": 1,
        "created_utc": 1523240998.0,
        "author": "audit157",
        "is_submitter": true,
        "parent_id": "t1_dwplo4c",
        "depth": 1
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "84dvr1",
    "title": "Knowledge Graphs for Enhanced Machine Reasoning at Forge.AI",
    "selftext": "",
    "url": "https://medium.com/@Forge_AI/knowledge-graphs-for-enhanced-machine-reasoning-at-forge-ai-ef1ffa03af3d",
    "score": 3,
    "upvote_ratio": 0.81,
    "num_comments": 0,
    "created_utc": 1521038482.0,
    "author": "jenniferlum",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/84dvr1/knowledge_graphs_for_enhanced_machine_reasoning/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "845yuy",
    "title": "What do you use for data cleaning (Hadoop, SQL, noSQL, etc) ?",
    "selftext": "I was thinking of using some sort of SQL because I much prefer it over Excel, but I'm not too familiar with options outside of those. ",
    "url": "https://www.reddit.com/r/datacleaning/comments/845yuy/what_do_you_use_for_data_cleaning_hadoop_sql/",
    "score": 3,
    "upvote_ratio": 0.81,
    "num_comments": 2,
    "created_utc": 1520961997.0,
    "author": "DisastrousProgrammer",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/845yuy/what_do_you_use_for_data_cleaning_hadoop_sql/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "dvnqd6k",
        "body": "Spark & SparkSQL or just pandas.",
        "score": 1,
        "created_utc": 1520986749.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_845yuy",
        "depth": 0
      },
      {
        "id": "dvngx3w",
        "body": "wow thank you so much for the detailed answer!!!\n\n",
        "score": 2,
        "created_utc": 1520977414.0,
        "author": "DisastrousProgrammer",
        "is_submitter": true,
        "parent_id": "t1_dvngf3b",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "81hcug",
    "title": "Hierarchical Classification at Forge.AI",
    "selftext": "",
    "url": "https://www.forge.ai/blog/hierarchical-classification-at-forge.ai",
    "score": 6,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1520020207.0,
    "author": "jenniferlum",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/81hcug/hierarchical_classification_at_forgeai/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "7za6bw",
    "title": "Forge.AI: Fueling Machine Intelligence Through Structuring Unstructured Data",
    "selftext": "",
    "url": "https://medium.com/forge-ai/forge-ai-technical-overview-94a3c7ba750c",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1519255897.0,
    "author": "tmarkovich",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/7za6bw/forgeai_fueling_machine_intelligence_through/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "7vv5iy",
    "title": "Wide Benefits of Data Cleansing for Business Endeavor",
    "selftext": "",
    "url": "https://www.dataentryexport.com/blog/data-cleansing/wide-benefits-data-cleansing-business-endeavor/",
    "score": 5,
    "upvote_ratio": 0.86,
    "num_comments": 0,
    "created_utc": 1517998589.0,
    "author": "chrissteveuk",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/7vv5iy/wide_benefits_of_data_cleansing_for_business/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "7r7486",
    "title": "Iterating over Pandas dataframe using zip and df.apply()",
    "selftext": "I'm trying to iterate over a df to calculate values for a new column, but it's taking too long. Here is the code (it's been simplified for brevity):\n\n    def calculate(row):\n        values = []\n        weights = []\n        continued = False\n\n        df_a = df[((df.winner_id == row['winner_id']) | (df.loser_id == row['winner_id']))].loc[row['index'] + 1:]\n        if len(df_a) < 30:\n            df.drop(row['index'], inplace = True)    \n            continued = True\n        #if we dropped the row, we don't want to calculate it's value\n        if continued == False:\n            for match in zip(df_a['winner_id'],df_a['tourney_date'],df_a['winner_rank'],df_a['loser_rank'],\n                             df_a['winner_serve_pts_pct']):\n                    weight = time_discount(yrs_between(match[1],row['tourney_date']))\n                    #calculate individual values and weights\n                    values.append(match[4] * weight * opp_weight(match[3]))\n                    weights.append(weight)\n        #return calculated value\n        return sum(values)/sum(weights)\n\n\n    df['new'] = df.apply(calculate, axis = 1)\n\n\nMy dataframe is not too large (60,000 by 35), but it's taking about 40 minutes for my code to run (and I need to do this for 10 different variables). I originally used iterrows(), but people suggested that I use zip() and apply - but it's still taking very long. Any help will be greatly appreciated. Thank you",
    "url": "https://www.reddit.com/r/datacleaning/comments/7r7486/iterating_over_pandas_dataframe_using_zip_and/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 1,
    "created_utc": 1516248760.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/7r7486/iterating_over_pandas_dataframe_using_zip_and/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "dsuyqi8",
        "body": "if i understand correctly (and it greatly helps if you not only share a black box function, but create a example dataframe and explain what you want it to do), \n\nyou have a dataframe where each row is a match of some sport, with a winner and a loser. \n\nThen for each player if they have only played less than 30 games, you remove them. So these you can remove **before** the by row function.  Also, it seems that you dont care about games, you care about players!. So it might make more sense to create a function that works by player id. \n",
        "score": 1,
        "created_utc": 1516263902.0,
        "author": "manueslapera",
        "is_submitter": false,
        "parent_id": "t3_7r7486",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "7pynal",
    "title": "Irregularities in TFX 2018 Qualifier Results by FloElite",
    "selftext": "",
    "url": "https://alexenos.github.io/tfx-qual-2018/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1515782398.0,
    "author": "alexenos",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/7pynal/irregularities_in_tfx_2018_qualifier_results_by/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "dskzun0",
        "body": "Before posting here, I contacted FloElite and TFX. I haven't heard anything from FloElite. TFX responded that they were troubled by these results and appreciated my work. Due to many athletes turning down the initial round of invitations, TFX indicated that many athletes higher up in the rankings received invitations. I even contacted Amanda Garces and she had received an invitation to compete in the Women's Pro division.\n\nTFX will not be using FloElite for the competition leaderboard in a couple of weeks and assured me these kind of inconsistencies would not occur again.",
        "score": 1,
        "created_utc": 1515782429.0,
        "author": "alexenos",
        "is_submitter": true,
        "parent_id": "t3_7pynal",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "7mhmdw",
    "title": "Way to Recognize Handwriting in Scanned Forms/Tables? (x-post /r/MachineLearning)",
    "selftext": "I'm looking to automate data entry from scanned forms with fields and tables containing handwritten data. I imagine that if I could find a way to automatically separate each field into a separate image, then I could find an existing handwriting recognition library. But I know this is a common problem, and maybe someone has already built a full implementation. Any ideas?",
    "url": "https://www.reddit.com/r/datacleaning/comments/7mhmdw/way_to_recognize_handwriting_in_scanned/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1514407148.0,
    "author": "mopperv",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/7mhmdw/way_to_recognize_handwriting_in_scanned/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "7hp1xj",
    "title": "7 Rules for Spreadsheets and Data Preparation for Analysis and Machine Learning",
    "selftext": "",
    "url": "https://jabustyerman.com/2017/12/04/7-rules-for-spreadsheets-and-data-preparation-for-analysis-and-machine-learning/",
    "score": 3,
    "upvote_ratio": 0.71,
    "num_comments": 1,
    "created_utc": 1512472761.0,
    "author": "jabustyerman",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/7hp1xj/7_rules_for_spreadsheets_and_data_preparation_for/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "dqsptge",
        "body": "Well for starters, wouldn‚Äôt it make sense putting your headers in the first row as excel will more easily recognize them as headers?",
        "score": 2,
        "created_utc": 1512477542.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_7hp1xj",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "77nduq",
    "title": "Inconsistent and Incomplete Product Information",
    "selftext": "What is the best way to clean/complete data like this? I don't have a \"master list\" to check against.\n\nBRAND | TYPE | MODEL\n---|---|----\nFORD | PICKUP | F150\nFORD | PICKUP | F15O\n | PICKUP | F150\nFORD | TRUCK | F150\nFORD | PICKUP | F150\nFORD | PICKUP | \nFORD | PICKUP | F150\nFORD | PICKUP | F150\n\nMy current method is to assume that the Brand&Type&Model combos that appear the most are correct. I use this as my list to compare the rest against with the Fuzzy LookUp add-in in Excel.\n\nThen I manually review the matches, pasting in the ones that I believe to be correct. \n\nThere has to be a better way?\n\nOur system currently says there are about 150,000 unique Brand/Type/Model combinations when in reality there isn't more than 25,000.\n\n",
    "url": "https://www.reddit.com/r/datacleaning/comments/77nduq/inconsistent_and_incomplete_product_information/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1508518388.0,
    "author": "birdnose",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/77nduq/inconsistent_and_incomplete_product_information/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "dpcnlxm",
        "body": "Use OpenRefine. The video tutorial does exactly what you want.",
        "score": 1,
        "created_utc": 1509831743.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_77nduq",
        "depth": 0
      },
      {
        "id": "drh8g0p",
        "body": "Do you have to use excel? Its been a very long time since i was stuck in office... but id probably use the master table and create a pivot, then use that as a vlookup table and write a logical formula to fill in the pieces. If you can use R or python this is way simpler",
        "score": 1,
        "created_utc": 1513703028.0,
        "author": "cxr1b0",
        "is_submitter": false,
        "parent_id": "t3_77nduq",
        "depth": 0
      },
      {
        "id": "drjj74z",
        "body": "It doesn't have to be in excel. How would I use Python to do this?",
        "score": 1,
        "created_utc": 1513808486.0,
        "author": "birdnose",
        "is_submitter": true,
        "parent_id": "t1_drh8g0p",
        "depth": 1
      },
      {
        "id": "drjl6tt",
        "body": "Sent you a chat.. wrote an R function that will fix this in nano seconds... but had a few other questions",
        "score": 1,
        "created_utc": 1513810677.0,
        "author": "cxr1b0",
        "is_submitter": false,
        "parent_id": "t1_drjj74z",
        "depth": 2
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "774j8i",
    "title": "What if I don't clean my data 100% properly?",
    "selftext": "Seriously... no matter how hard we clean... some bad examples are going to get through!!\n\nHow can I take that into account when looking at my results?\n\n\nIs it better to have HUGE sets with some errors or small sets with none?",
    "url": "https://www.reddit.com/r/datacleaning/comments/774j8i/what_if_i_dont_clean_my_data_100_properly/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 1,
    "created_utc": 1508303827.0,
    "author": "PostNationalism",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/774j8i/what_if_i_dont_clean_my_data_100_properly/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "doj52ut",
        "body": "Data will never be 100% clean. Clean what you can and lock the database. You'd probably discover minor ones down the road. If it doesn't affect the outcome much, then they're minor.\n\nEDIT: I always ask them what is their error threshold.",
        "score": 2,
        "created_utc": 1508316484.0,
        "author": "fabskong",
        "is_submitter": false,
        "parent_id": "t3_774j8i",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "75m19x",
    "title": "Identifying text that is all caps",
    "selftext": "I've got some data on available apartments and a description of the apartment. Some of the descriptions are in all caps or they have a subset in the description that is in all caps. \n\nI'm interested in seeing if there is any relationship between presence of all caps and whether or not the apartment is over priced, but I'm not sure how to go about identifying whether a description contains capitalized phrases. I suppose I could try calculating the percentage of characters that are capitalized, but I'm wondering if anyone has any other ideas about how to extract this type of information.",
    "url": "https://www.reddit.com/r/datacleaning/comments/75m19x/identifying_text_that_is_all_caps/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 7,
    "created_utc": 1507688020.0,
    "author": "nkk36",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/75m19x/identifying_text_that_is_all_caps/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "do8560r",
        "body": "Write a simple program in a language of your choice?",
        "score": 1,
        "created_utc": 1507742783.0,
        "author": "yardightsure",
        "is_submitter": false,
        "parent_id": "t3_75m19x",
        "depth": 0
      },
      {
        "id": "do87ofu",
        "body": "How much are you willing to pay?",
        "score": 1,
        "created_utc": 1507745388.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_75m19x",
        "depth": 0
      },
      {
        "id": "do8h24j",
        "body": "Look at levenshtein distance. Make dummy variable of your description to lower case then find highest differences between the dummy and original. Do some exploratory on different weights for optimal results",
        "score": 1,
        "created_utc": 1507755171.0,
        "author": "timtrice",
        "is_submitter": false,
        "parent_id": "t3_75m19x",
        "depth": 0
      },
      {
        "id": "do8hpw6",
        "body": "How about using something like regex to find uppercase words i.e. maybe 2 or 3 consecutive uppercase letters and return the price\n\n",
        "score": 1,
        "created_utc": 1507755880.0,
        "author": "emet",
        "is_submitter": false,
        "parent_id": "t3_75m19x",
        "depth": 0
      },
      {
        "id": "do8jrqi",
        "body": "Thanks! I hadn't thought of making this a distance calculation, but now that you've given me an idea and a starting point I'll take look into this. ",
        "score": 1,
        "created_utc": 1507758146.0,
        "author": "nkk36",
        "is_submitter": true,
        "parent_id": "t1_do8h24j",
        "depth": 1
      },
      {
        "id": "do8juny",
        "body": "Regular expressions are definitely a good idea. I thought about it, but I'm no expert in using them so I wasn't sure how to write a regex to find words that are written in all caps, but I took a deeper look at it this afternoon and found some code I could use. Thanks!",
        "score": 1,
        "created_utc": 1507758236.0,
        "author": "nkk36",
        "is_submitter": true,
        "parent_id": "t1_do8hpw6",
        "depth": 1
      },
      {
        "id": "dpcnouv",
        "body": "[A-Z]+",
        "score": 2,
        "created_utc": 1509831844.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t1_do8juny",
        "depth": 2
      }
    ],
    "comments_extracted": 7
  },
  {
    "id": "703khs",
    "title": "Data cleansing and exploration made simple with Python and Apache Spark",
    "selftext": "",
    "url": "https://hioptimus.com",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1505408434.0,
    "author": "argenisleon",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/703khs/data_cleansing_and_exploration_made_simple_with/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "6y8d3w",
    "title": "The Ultimate Guide to Basic Data Cleaning",
    "selftext": "",
    "url": "http://www.kdnuggets.com/2017/08/socialcops-ultimate-guide-basic-data-cleaning.html",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1504622120.0,
    "author": "lalypopa123",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/6y8d3w/the_ultimate_guide_to_basic_data_cleaning/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "6x7x27",
    "title": "Live Demo: SQL-like language for cleaning JSONs and CSVs",
    "selftext": "",
    "url": "https://demo.raw-labs.com/",
    "score": 2,
    "upvote_ratio": 0.76,
    "num_comments": 0,
    "created_utc": 1504196847.0,
    "author": "msbranco",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/6x7x27/live_demo_sqllike_language_for_cleaning_jsons_and/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "6pe3kl",
    "title": "5 Simple and Efficient Steps for Data Cleansing",
    "selftext": "",
    "url": "http://www.floridadataentry.com/blog/5-simple-efficient-steps-data-cleansing/",
    "score": 0,
    "upvote_ratio": 0.4,
    "num_comments": 0,
    "created_utc": 1500959815.0,
    "author": "juliaruther",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/6pe3kl/5_simple_and_efficient_steps_for_data_cleansing/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "6oql8r",
    "title": "Help! how to make data more representative",
    "selftext": "Hi everyone. \nThis is the situation: I work in a tourism wholesaler and I get a lot of request via XML.\nThe thing is that some clients make a lot of RQs for one destination but don't make a lot of reservations. And some the other way around. How can I display the importance of the destination based on the RQs without inclining the scale towards those clients that convert less?\nEg: Client1: 10M request for NYC; only 10 Reservations in NYC\nClient2: 10k request for NYC; 10 reservations in NYC\n\nI know that for both NYC is important because they make 10 rez but one client needs 1000 times more rqs.\n\nHow can I get legit insights? because client one will have higher ponderation and will mess my data.\n\nI hope somebody understands what I said and may help me :)\nThank you oall ",
    "url": "https://www.reddit.com/r/datacleaning/comments/6oql8r/help_how_to_make_data_more_representative/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1500668678.0,
    "author": "abiaus",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/6oql8r/help_how_to_make_data_more_representative/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "6olngj",
    "title": "Why Data Cleansing is an Absolute-Must for your Enterprise?",
    "selftext": "",
    "url": "http://www.floridadataentry.com/blog/data-cleansing-absolute-must-for-enterprise/",
    "score": 2,
    "upvote_ratio": 0.76,
    "num_comments": 1,
    "created_utc": 1500610856.0,
    "author": "juliaruther",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/6olngj/why_data_cleansing_is_an_absolutemust_for_your/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "dkipr5x",
        "body": "This is like saying \"why paying your employees is an absolute-must for your enterprise\".  Nobody needs to be convinced.\n\nI can't think of a single company that would just take data as it comes, never transform it, never filter it, etc.  If your company deals in data, I guarantee you are already data cleansing.",
        "score": 1,
        "created_utc": 1500642131.0,
        "author": "ImPostingOnReddit",
        "is_submitter": false,
        "parent_id": "t3_6olngj",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "6nm9wt",
    "title": "What approaches are recommended to get this pdf data into a consumable tabular form?",
    "selftext": "",
    "url": "http://www.bedfordny.gov/wp-content/uploads/2013/12/2017-Bedford-tentative.pdf",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1500212520.0,
    "author": "longprogression",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/6nm9wt/what_approaches_are_recommended_to_get_this_pdf/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "6mzqt1",
    "title": "Need help downloading (using google/yahoo APIs) end of day trading data from many exchanges for ml project.",
    "selftext": "I've been searching for free end of day trading data for historic analysis. The two main free sources I've found are google and yahoo finance. I am planning using using octave's \"urlread(link)\" to load the data. I have two problems:\n\n1) how to use the google api to download the data.\n\n2) how to generalize the download to the full list of companies.\n\nFrom an old reddit comment: data = urlread(\"http://www.google.com/finance/getprices?i=60&p=10d&f=d,o,h,l,c,v&df=cpct&q=IBM\")\n\nAny help would be appreciated. ",
    "url": "https://www.reddit.com/r/datacleaning/comments/6mzqt1/need_help_downloading_using_googleyahoo_apis_end/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1499925232.0,
    "author": "LukeSkyWalkerGetsIt",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/6mzqt1/need_help_downloading_using_googleyahoo_apis_end/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "6lm98e",
    "title": "Network Packets --> Nice trainable/testable data",
    "selftext": "Hello!\n\n\n\nI am trying to build a system on a home Wi-fi router that can detect network anomalies to halt a distributed-denial of service (Ddos) attack. \n\n\n\n**Here is the structure of my project so far:**\n\n\n* Sending all network packets to a python program where I can accept/drop packets (We accomplish this with iptables and NFQUEUE if you're curious).\n\n\n* My program parses all packets in a way to see all packet fields (headers, protocol, TTL‚Ä¶etc) and then accepts all packets\n\n\n* Eventually, I want some sort of classifier to make decisions on what packets to accept/drop\n\n\n**What is a sound way to convert network packets into something a classifier can train/test on?**\n\n\n* Packets depending on their protocol (TCP/UDP/ICMP) have a varying number of fields/features. (Each packet basically has different dimensionality!)\n\n\n\n* Should I just put a zero/-1 in the features that don‚Äôt exist?\n\n\n* I am familiar with Scikit-learn, TesorFlow, and R. \n\n\nThanks! \n\n\n",
    "url": "https://www.reddit.com/r/datacleaning/comments/6lm98e/network_packets_nice_trainabletestable_data/",
    "score": 3,
    "upvote_ratio": 0.8,
    "num_comments": 0,
    "created_utc": 1499352902.0,
    "author": "yannimou",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/6lm98e/network_packets_nice_trainabletestable_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "6kc273",
    "title": "Resources to learn how to clean data",
    "selftext": "I was interviewing for a data scientist position and was asked about my experience in data cleaning and how to clean data. I did not have a very good answer. I've played around with messy data sets, but I couldn't explain how to clean data at a high-level summary. What typical things do you examine, common data quality problems, techniques for cleaning data, etc...?\n\nIs there a resource (website, textbook) that I could read to learn about data cleaning methodologies and best practices? I'd like to improve my data cleaning skills so that I am more ready for questions like this. I recently purchase [this textbook](https://www.amazon.com/Best-Practices-Data-Cleaning-Everything/dp/1412988012) in hopes that it would help. I'm just looking for other recommendations if anyone has some ideas.",
    "url": "https://www.reddit.com/r/datacleaning/comments/6kc273/resources_to_learn_how_to_clean_data/",
    "score": 3,
    "upvote_ratio": 0.81,
    "num_comments": 3,
    "created_utc": 1498777237.0,
    "author": "nkk36",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/6kc273/resources_to_learn_how_to_clean_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "djl7sjq",
        "body": "In an ideal world, your dataset would be perfectly structured, with no excess, no missing values, no inconsistencies or redundancy. But that's rarely the case. Data cleaning is a pretty big job, there is a lot that can be wrong with your data. \n\nToday I was working with log data in a plain text file. I wanted to iterate through every line, breaking each line down into a date (which I transformed into a date object from a string object so that I can do time-series), a message, and a title associated with every message, which would then be dataframe'd. I'd use a pattern to determine the title, but the titles weren't consistent throughout the entire document, e.g. half way through it changes from ~XXXX, to ~Xx, thus I had to work with it to make it consistent‚ÄîI had to clean it, but to do so, I actually had to become aware of it first, which takes getting familiar with the data. So you really have to go through your data to begin with, and look at summaries and patterns. My text file has hundreds of thousands of records, it would have been easy to miss this inconsistency if I hadn't been looking, which would have put a serious dent in my analysis. \n\nAdditionally, I had a lot of missing values. Data you work with might have missing values represented as -999, NaN, \"NA\", or whatever other funky thing. If you include something like -999 in the calculations of your analysis, you're going to be inaccurate, so you usually want to clean these sorts of values in a mindful way: do you find the mode surrounding them?‚Äîthe mean?‚Äîdo you drop them? And on numerical values, you're sometimes going to need to normalize them, otherwise a model might be thrown off. \n\nThis guy's video is a pretty good example of data cleaning (sorry if GIS bores you): https://youtu.be/qvHXRuGPHl0?t=24m51s You can find the author on Twitter with the handle [@dreyco676](https://twitter.com/dreyco676)\n\nPretty early into it you can see that he looks at summary information on his data, including memory usage. He tests assumptions about the index and whether there are duplicates or not (just to be safe‚Äîyou should be skeptical), transforms some variables to categorical type to make processing less computationally expensive, deals with missing data, drops unneeded columns, etc. He makes sure the data is relevant/useful/accurate/usable. I don't know about what resources you can use to learn more, but these are some things that you should be mindful of.",
        "score": 4,
        "created_utc": 1498793816.0,
        "author": "_ckhoward",
        "is_submitter": false,
        "parent_id": "t3_6kc273",
        "depth": 0
      },
      {
        "id": "djkxun4",
        "body": "I find data cleaning and feature engineering to actually be the two most fun parts of data analysis.\n\nTwo resources that I found helpful early on were:\n\n[Data Preparation for Data Mining](https://www.amazon.com/Preparation-Mining-Kaufmann-Management-Systems/dp/1558605290/ref=sr_1_1), by Dorian Pyle. Although this is a very old book, the topics and techniques are generally timeless.\n\n[Introduction to Data Cleaning with R](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiK1-ulneTUAhWl54MKHZBSA_MQFggoMAA&url=https%3A%2F%2Fcran.r-project.org%2Fdoc%2Fcontrib%2Fde_Jonge%2Bvan_der_Loo-Introduction_to_data_cleaning_with_R.pdf&usg=AFQjCNF7MyfLYZ3gVobJEvfW9VJjaSAhPg), by Edwin de Jonge. This is a PDF that will be downloaded. Even if you are not using R, the topics he covers are pretty comprehensive.\n\nGood luck!",
        "score": 1,
        "created_utc": 1498779754.0,
        "author": "vmsmith",
        "is_submitter": false,
        "parent_id": "t3_6kc273",
        "depth": 0
      },
      {
        "id": "djl7slx",
        "body": "SECTION | CONTENT\n:--|:--\nTitle | Geospatial Analysis with Python\nDescription | Data comes in all shapes and sizes and often government data is geospatial in nature. Often times data science programs & tutorials ignore how to work with this rich data to make room for more advanced topics. Our MinneMUDAC competition heavily utilized geospatial data but was processed to provide students a more familiar format. But as good scientists, we should use primary sources of information as often as possible.  Come to this talk to get a basic understanding of how to read, write, query ...\nLength | 1:03:30\n\n \n\n \n \n \n****\n \n^(I am a bot, this is an auto-generated reply | )^[Info](https://www.reddit.com/u/video_descriptionbot) ^| ^[Feedback](https://www.reddit.com/message/compose/?to=video_descriptionbot&subject=Feedback) ^| ^(Reply STOP to opt out permanently)",
        "score": 1,
        "created_utc": 1498793818.0,
        "author": "video_descriptionbot",
        "is_submitter": false,
        "parent_id": "t1_djl7sjq",
        "depth": 1
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "6jm7zp",
    "title": "What is the best approach to clean a large dataset?",
    "selftext": "Hello! \n\nI have two csv files with more 1+ million rows each. Both files have records in common and I need to combine information for those records from both files. Would you recommend R or Python for such a task?\n\nMoreover, it would be highly appreciated if you provide me with any training/tutorial resources, examples on data cleaning in both languages.\n\nThanks",
    "url": "https://www.reddit.com/r/datacleaning/comments/6jm7zp/what_is_the_best_approach_to_clean_a_large_dataset/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1498495006.0,
    "author": "elshami",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/6jm7zp/what_is_the_best_approach_to_clean_a_large_dataset/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "djfhav5",
        "body": "I don't know about Record Linkage with R or Python, but you can take a look in the Duke that is developed in Java e work fine for it.  https://github.com/larsga/Duke",
        "score": 1,
        "created_utc": 1498503652.0,
        "author": "muschneider",
        "is_submitter": false,
        "parent_id": "t3_6jm7zp",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "6i21t9",
    "title": "[Noob]How to round up values",
    "selftext": "How to round up values\n\nHello!\nReally noob question here:\n\nI'm working with some rain volume data here, and I have the following question:\nThe lower number of rain volume in my data set is 0, and the larger number is 67. How can I group this values, so that if the number is between 0 and 10, it changes to 10, and if it is between 10 and 20, it changes to 20, and so on?\n\nAlso: Is open refine the best software to do this, or is Excel more recommended?\nThanks in advance!",
    "url": "https://www.reddit.com/r/datacleaning/comments/6i21t9/noobhow_to_round_up_values/",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1497820015.0,
    "author": "Daniel--Santos",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/6i21t9/noobhow_to_round_up_values/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "dj3iwxa",
        "body": "If (data <= 10): {data==10} and repeat for each level? I'd personally read the data into R and then clean that way. However I'd personally add another column called \"data level\" and set that to 1 corresponding to 0<x<=10 and so forth. ",
        "score": 2,
        "created_utc": 1497856942.0,
        "author": "Dephscent",
        "is_submitter": false,
        "parent_id": "t3_6i21t9",
        "depth": 0
      },
      {
        "id": "dk13ov9",
        "body": "You could do something like ceiling(x/10)*10",
        "score": 1,
        "created_utc": 1499702306.0,
        "author": "Stepfunction",
        "is_submitter": false,
        "parent_id": "t3_6i21t9",
        "depth": 0
      },
      {
        "id": "dj4m32n",
        "body": "Thanks! It looks easier than I imagined.",
        "score": 1,
        "created_utc": 1497913122.0,
        "author": "Daniel--Santos",
        "is_submitter": true,
        "parent_id": "t1_dj3iwxa",
        "depth": 1
      },
      {
        "id": "dj4m1ls",
        "body": "Thanks for the advice! I presumed that there was a name for what I was trying to do. ",
        "score": 1,
        "created_utc": 1497913072.0,
        "author": "Daniel--Santos",
        "is_submitter": true,
        "parent_id": "t1_dj47i0k",
        "depth": 1
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "6gqo6l",
    "title": "How can we erase our privacy with protection?",
    "selftext": "",
    "url": "https://www.youtube.com/watch?v=rwy4JvjAVt0",
    "score": 0,
    "upvote_ratio": 0.4,
    "num_comments": 0,
    "created_utc": 1497250976.0,
    "author": "Momsen17",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/6gqo6l/how_can_we_erase_our_privacy_with_protection/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "6eon6i",
    "title": "Urjanet Data Guru Series Part 2: A Guide to Data Mapping and Tagging",
    "selftext": "",
    "url": "http://urjanet.com/resources/utility-data-guru-series-part-2-data-mapping-tagging/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 0,
    "created_utc": 1496341182.0,
    "author": "urjanet",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/6eon6i/urjanet_data_guru_series_part_2_a_guide_to_data/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "6dgz23",
    "title": "Dirty Data ‚Äì Preventing the Pollution of Your IoT Data Lake",
    "selftext": "",
    "url": "http://www.iot-inc.com/dirty-data-preventing-iot-data-lake-pollution-podcast/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1495805639.0,
    "author": "BrightWolfIIoT",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/6dgz23/dirty_data_preventing_the_pollution_of_your_iot/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "6a3vfm",
    "title": "How to Engineer and Cleanse your data prior to Machine Learning | Analytics | Data Science",
    "selftext": "",
    "url": "http://www.acheronanalytics.com/acheron-blog/how-to-engineer-your-data-for-data-science",
    "score": 9,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1494313207.0,
    "author": "nonkeymn",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/6a3vfm/how_to_engineer_and_cleanse_your_data_prior_to/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "dhbnnui",
        "body": "I've heard some very strong opinions on how to handle missing data. I really like how they handled it here. It seems more thought-out than some methods I've seen in practice and they actually tested the results against one of the more common methods.",
        "score": 2,
        "created_utc": 1494327644.0,
        "author": "thisdata",
        "is_submitter": false,
        "parent_id": "t3_6a3vfm",
        "depth": 0
      },
      {
        "id": "dhbis0x",
        "body": "Would love some more tip sites below!",
        "score": 1,
        "created_utc": 1494313225.0,
        "author": "nonkeymn",
        "is_submitter": true,
        "parent_id": "t3_6a3vfm",
        "depth": 0
      },
      {
        "id": "dhby157",
        "body": "I would love to know  what other ways your are reffering too! That is always something that requires work and just really...knowing your data.",
        "score": 1,
        "created_utc": 1494343334.0,
        "author": "nonkeymn",
        "is_submitter": true,
        "parent_id": "t1_dhbnnui",
        "depth": 1
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "654g7l",
    "title": "How to match free form UK addresses?",
    "selftext": "I have different data set which have the same addresses written in slightly different form \"oxford street 206 W1D\" and in other cases \"W1D 2, OXFORD STREET, 206 London\" etc.  Unfortunately they are the only information I can use to match the values across. All the logic I wrote so far took me to low match rates. Is there \"tool\" that can help with that?",
    "url": "https://www.reddit.com/r/datacleaning/comments/654g7l/how_to_match_free_form_uk_addresses/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1492075909.0,
    "author": "df016",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/654g7l/how_to_match_free_form_uk_addresses/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "dg8ezjb",
        "body": "First idea that comes to mind is segmenting each address into punctuation-stripped case-homogeneous words, then do pairwise comparisons of all list of words, then assign a score based on single matches plus some bonus when ordering coincides.\n\nIt's definitely not a simple problem though, depending on how heterogeneous the formats are. There are companies that do this for a fee.\n\nOther approaches are discussed [here](https://datascience.stackexchange.com/questions/10810/how-to-do-postal-addresses-fuzzy-matching).",
        "score": 2,
        "created_utc": 1492126333.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_654g7l",
        "depth": 0
      },
      {
        "id": "dgadngf",
        "body": "Regex is what comes to mind first.  \n\nYou can make several passes with different patterns that pull out different parts and ignore the rest.  It might take a few tries to get all the right patterns, but if the addresses have some level of consistency (albeit, maybe 20 ways of writing an address), it should be doable.\n\nAnother way to do it would be some kind of string tokenizing approach, which I believe /u/joevector was getting at.",
        "score": 1,
        "created_utc": 1492237448.0,
        "author": "Omega037",
        "is_submitter": false,
        "parent_id": "t3_654g7l",
        "depth": 0
      },
      {
        "id": "di5h8k8",
        "body": "There is a very interesting project here that could solve a lot of problems, including expensive tool costs: \n\nhttps://github.com/openvenues/libpostal\n\nBTW it is not only valid for UK.",
        "score": 1,
        "created_utc": 1495989130.0,
        "author": "df016",
        "is_submitter": true,
        "parent_id": "t3_654g7l",
        "depth": 0
      },
      {
        "id": "dgdeilb",
        "body": "To answer both, I think that addresses should be parsed against the Royal Mail Delivery Point Address structure. There is documentation for developers about it. That should take to a structured for of an address.",
        "score": 1,
        "created_utc": 1492431011.0,
        "author": "df016",
        "is_submitter": true,
        "parent_id": "t1_dgadngf",
        "depth": 1
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "64t8nv",
    "title": "Anyone here interested in IoT data cleaning?",
    "selftext": "",
    "url": "http://brightwolf.com/2017/04/07/when-good-iot-data-goes-bad/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 0,
    "created_utc": 1491940123.0,
    "author": "BrightWolfIIoT",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/64t8nv/anyone_here_interested_in_iot_data_cleaning/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "628gto",
    "title": "Looking for a data set / corpus of labeled job posting data. Any hints?",
    "selftext": "Does anyone has a tip for me?",
    "url": "https://www.reddit.com/r/datacleaning/comments/628gto/looking_for_a_data_set_corpus_of_labeled_job/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1490812064.0,
    "author": "tikiParty",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/628gto/looking_for_a_data_set_corpus_of_labeled_job/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "dfm5plw",
        "body": "Easiest to access is the nyc_jobs data package on Quilt:\nhttps://quiltdata.com/package/akarve/nyc_jobs\n\nThere's also aggregated data here: https://www.indeed.com/jobtrends/Big-data.html\n\nThis looks promising but sources seem down:\nhttps://github.com/OpenData-CS-VT/ccars-jobpostings/wiki",
        "score": 3,
        "created_utc": 1490897797.0,
        "author": "brightpixels",
        "is_submitter": false,
        "parent_id": "t3_628gto",
        "depth": 0
      },
      {
        "id": "dfm6phy",
        "body": "I love you :) This is gold!",
        "score": 1,
        "created_utc": 1490898834.0,
        "author": "tikiParty",
        "is_submitter": true,
        "parent_id": "t1_dfm5plw",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "5z64xq",
    "title": "How can I access specific data sets between certain time frames with specific occurrence frames (ie. days, weeks, months)?",
    "selftext": "Pretty much title.\n\nI'm looking to pull data for certain time frames of with specific occurances in mind (don't know if I'm using the right wording here).\n\nFor example: If I want to find the data on traffic accidents in a county per day rather than per month. I seem to be able to find this sort of data per month, but have a problem finding it per day.\n\n",
    "url": "https://www.reddit.com/r/datacleaning/comments/5z64xq/how_can_i_access_specific_data_sets_between/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1489424129.0,
    "author": "BrianDynBardd",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/5z64xq/how_can_i_access_specific_data_sets_between/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "dg9hnqa",
        "body": "Sounds to me like this would depend on the data set and what's available.  Can you be more specific?\n\nIf the raw data has this specificity, then it's just a matter of additional analysis to get what you need.  If it wasn't collected in the first place with that fidelity, then it sounds like you need another data source or need to persuade someone to change how they're currently collecting or reporting the data.",
        "score": 1,
        "created_utc": 1492192574.0,
        "author": "BrightWolfIIoT",
        "is_submitter": false,
        "parent_id": "t3_5z64xq",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "5xu24q",
    "title": "Data Quality - Standardise Enrich Cleanse",
    "selftext": "",
    "url": "https://www.datalytyx.com/data-quality-standardise-enrich-cleanse/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1488816762.0,
    "author": "gibran_kazi",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/5xu24q/data_quality_standardise_enrich_cleanse/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "5t8otx",
    "title": "How to Clean Your Data Quickly in 5 Steps",
    "selftext": "",
    "url": "http://www.datasciencecentral.com/profiles/blogs/how-to-clean-your-data-quickly-in-5-steps",
    "score": 3,
    "upvote_ratio": 0.71,
    "num_comments": 0,
    "created_utc": 1486748524.0,
    "author": "psangrene",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/5t8otx/how_to_clean_your_data_quickly_in_5_steps/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "5rqp31",
    "title": "Thoughts on CrowdFlower.com?",
    "selftext": "",
    "url": "https://www.crowdflower.com",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1486081897.0,
    "author": "Rafael_Bacardi",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/5rqp31/thoughts_on_crowdflowercom/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "5r8jnd",
    "title": "Outsource people for data labeling?",
    "selftext": "What are good sites to find people to do some very basic picture labeling? \n\nThis is for a personal side project and wouldn't require too many hours. \n\nI known about cloudfactory.com, but they only offer more hours and people that I need.",
    "url": "https://www.reddit.com/r/datacleaning/comments/5r8jnd/outsource_people_for_data_labeling/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1485873621.0,
    "author": "notevencrazy99",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/5r8jnd/outsource_people_for_data_labeling/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "dd5ebx4",
        "body": "Amazon's [mechanical turk](https://www.mturk.com/mturk/welcome)?",
        "score": 2,
        "created_utc": 1485881701.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_5r8jnd",
        "depth": 0
      },
      {
        "id": "dd9d1g7",
        "body": "CrowdFlower.com\n",
        "score": 1,
        "created_utc": 1486081579.0,
        "author": "Rafael_Bacardi",
        "is_submitter": false,
        "parent_id": "t3_5r8jnd",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "5qlbkq",
    "title": "Papers on dealing with erroneous or missing data from the likes of Bloomberg, Thomson Reuters, . .",
    "selftext": "I am in search of papers or articles on how to detect, validate, and correct missing, noisy, or erroneous data being streamed in real time by the likes of Bloomberg, Thomson Reuters, S&P Capital?  The goal is to clean things up before the data is fed to RNN.  This applies to data for investment securities (stocks, bonds, options, . . .)",
    "url": "https://www.reddit.com/r/datacleaning/comments/5qlbkq/papers_on_dealing_with_erroneous_or_missing_data/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1485569813.0,
    "author": "alghar",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/5qlbkq/papers_on_dealing_with_erroneous_or_missing_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "52jgaa",
    "title": "Interactive outlier analysis using PCA",
    "selftext": "",
    "url": "https://twitter.com/michal_sustr/status/775426838730510336",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1473755953.0,
    "author": "michal_sustr",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/52jgaa/interactive_outlier_analysis_using_pca/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "50pxpm",
    "title": "Local Presence, Culture and Data Quality | International Data Verification",
    "selftext": "",
    "url": "http://www.acquiro.com/local-presence-culture-data-quality/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1472768807.0,
    "author": "DataGeekDenver",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/50pxpm/local_presence_culture_and_data_quality/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "dbj4kuu",
        "body": "Nice stock photo",
        "score": 1,
        "created_utc": 1482461407.0,
        "author": "fasnoosh",
        "is_submitter": false,
        "parent_id": "t3_50pxpm",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "50pe4e",
    "title": "For those who use it",
    "selftext": "",
    "url": "http://wolfram.com/language/11/units-and-dates/?product=mathematica",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1472762096.0,
    "author": "Pascal_Rascal",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/50pe4e/for_those_who_use_it/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "4zqfd3",
    "title": "Cleaning data in SQL database from R?",
    "selftext": "Hi guys,\n\nIm very new to R. I found dplyr to be quite useful in manipulating data and was quite happy to find that it can access sql database from dplyr.\n\nAs you know, data is sometimes messy. Is there any packages that can clean an sql database from R without importing tables? I tried to do it with tidyr but i dont think it works.\n\nOr maybe data cleaning in sql database just requires sql?\n\nThanks",
    "url": "https://www.reddit.com/r/datacleaning/comments/4zqfd3/cleaning_data_in_sql_database_from_r/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1472240129.0,
    "author": "lan69",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/4zqfd3/cleaning_data_in_sql_database_from_r/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "d6yambx",
        "body": "Yes just write sql to clean the table in the database it is in or import the data into R and clean it in R.",
        "score": 1,
        "created_utc": 1472260316.0,
        "author": "edimaudo",
        "is_submitter": false,
        "parent_id": "t3_4zqfd3",
        "depth": 0
      },
      {
        "id": "d7bv6p8",
        "body": "you have to do a better job describing what you want to do if you expect to get help. Give an example of what your \"dirty\" data is like and how you would like to clean it. I have no idea if you are replacing nulls with 0 or trying to transform the data or just filtering outliers. \n",
        "score": 1,
        "created_utc": 1473191219.0,
        "author": "CMastication",
        "is_submitter": false,
        "parent_id": "t3_4zqfd3",
        "depth": 0
      },
      {
        "id": "d7ccqy6",
        "body": "Basically i want the functions of tidyr but i notice it cant access sql database like dplyr. You have to import the data first and then tidy it. Should have been more clear",
        "score": 1,
        "created_utc": 1473216819.0,
        "author": "lan69",
        "is_submitter": true,
        "parent_id": "t1_d7bv6p8",
        "depth": 1
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "4wmwhs",
    "title": "I'd like to build a data cleaning toolkit from scratch, where do I begin?",
    "selftext": "Hey guys,  \n\nI'm relatively new to data mining and analytics and like the sidebar says, data cleaning does take a while. I'd like to build a toolkit from scratch but I'm unsure where to begin.",
    "url": "https://www.reddit.com/r/datacleaning/comments/4wmwhs/id_like_to_build_a_data_cleaning_toolkit_from/",
    "score": 3,
    "upvote_ratio": 0.72,
    "num_comments": 6,
    "created_utc": 1470606541.0,
    "author": "KrustyKrab111",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/4wmwhs/id_like_to_build_a_data_cleaning_toolkit_from/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "d68bvz2",
        "body": "Some tools already exist:\n\n- OpenRefine: web-desktop app, originally developed by Google, then recently open-sourced\n\n- Trifacta Wrangler: commercial software, originally developed by Stanford researchers\n\n- Talend: don't know much their products\n\nAre you aware of some other tools, like SaaS based ?\n",
        "score": 2,
        "created_utc": 1470610516.0,
        "author": "alexandreyc",
        "is_submitter": false,
        "parent_id": "t3_4wmwhs",
        "depth": 0
      },
      {
        "id": "d6qdv1q",
        "body": "Probably, the most interesting things to start with are:\n\n* study/identification of the outliers that could help identifying bad data ingestion\n\n* study of the distributions: are the features of your data multimodal, monomodal, etc.?\n\n* correlations between features: are two features the same and/or telling you almost the same thing?\n\n* space coverage: is the data covering in a uniform way the space that you defined with the features?\n\nSome references:\n\n* This one is a nice talk that I attended last year on the subject: http://www.slideshare.net/huguk/trifacta-hug-march2015\n\n* This is a book that could be useful: http://shop.oreilly.com/product/0636920024422.do",
        "score": 1,
        "created_utc": 1471765838.0,
        "author": "leaningtoweravenger",
        "is_submitter": false,
        "parent_id": "t3_4wmwhs",
        "depth": 0
      },
      {
        "id": "da5urvf",
        "body": "There is datacleaner at https://github.com/rhiever/datacleaner\nIt is a Python tool that automatically cleans data sets and readies them for analysis.",
        "score": 1,
        "created_utc": 1479488851.0,
        "author": "impulsecorp",
        "is_submitter": false,
        "parent_id": "t3_4wmwhs",
        "depth": 0
      },
      {
        "id": "d68nyt2",
        "body": "> OpenRefine: web-desktop app, originally developed by Google, then recently open-sourced\n\n\nOriginally developed as Freebase Gridwords by Metaweb :-)\n\n\n\n> While at Metaweb I also created Freebase Gridworks (2009), which got rebranded as Google Refine (2010), and subsequently OpenRefine (2012, see its history). It is being taught and used widely among librarians and data journalists (courses/tutorials by IBM's Big Data University, School of data, Duke University, Rice University, etc.). There is much love for it, though resources for its open-source development are lacking.\n\n[Source: David Huynh](http://davidhuynh.net/), the same person that has been pushing commits on [google-refine](https://code.google.com/archive/p/google-refine/source/default/commits?page=27)",
        "score": 1,
        "created_utc": 1470631786.0,
        "author": "fawkesdotbe",
        "is_submitter": false,
        "parent_id": "t1_d68bvz2",
        "depth": 1
      },
      {
        "id": "d6qf8qd",
        "body": "Forgot to add:\n\n* identification of missing data points, even better is characterized in some way, e.g., between date X and Y there was a spike in missing data, etc.",
        "score": 1,
        "created_utc": 1471770698.0,
        "author": "leaningtoweravenger",
        "is_submitter": false,
        "parent_id": "t1_d6qdv1q",
        "depth": 1
      },
      {
        "id": "d68u9aq",
        "body": "Right.",
        "score": 1,
        "created_utc": 1470650645.0,
        "author": "alexandreyc",
        "is_submitter": false,
        "parent_id": "t1_d68nyt2",
        "depth": 2
      }
    ],
    "comments_extracted": 6
  },
  {
    "id": "4tszr4",
    "title": "What Exactly is Data Quality?",
    "selftext": "Need feedback. \nMy company just posted this blog and would love feedback. We couldn't find anything else that talked simply about data quality, so we wrote one ourselves. What do you think? How could we expand or does it help or just lemme know your thoughts. Would really help!\n\n[What Exactly is Data Quality?](http://www.acquiro.com/data-quality/)",
    "url": "https://www.reddit.com/r/datacleaning/comments/4tszr4/what_exactly_is_data_quality/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1469044100.0,
    "author": "DataGeekDenver",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/4tszr4/what_exactly_is_data_quality/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "4tq7e8",
    "title": "Splitting Data with R",
    "selftext": "Does anyone know the command to split my data set so that I can portray it on a plot with a break. For instance I have crop data for certain days of the year (75:333) and I want to leave out days (100:150). How do I code this in R?",
    "url": "https://www.reddit.com/r/datacleaning/comments/4tq7e8/splitting_data_with_r/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1469003663.0,
    "author": "TheBaldManCry",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/4tq7e8/splitting_data_with_r/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "d5jij3m",
        "body": "If I understood you correctly, you want to group data by a variable and then plot it. You can use the dplyr package. It has functions like filter (to filter out unwanted rows) and group_by (to split the data by one or more variables). Then you can plot these data. Alternatively, you can use the ggplot2 package. You can group the data using the color argument or divide the data into facets using facet_wrap function, etc. You can also combine the functions from both packages. Search on stackoverflow to see examples.",
        "score": 2,
        "created_utc": 1469021760.0,
        "author": "yaymayhun",
        "is_submitter": false,
        "parent_id": "t3_4tq7e8",
        "depth": 0
      },
      {
        "id": "d5jmw8x",
        "body": "No I actually have 2 variables, x and y and have \"missing data\" in between and cant take my cells as (75:333). So I need to leave out a section of the data. \n\nExample: I measured say temperature for the whole year but my thermometer broke at day 120 and was fixed at day 200 so I need to leave these out.",
        "score": 1,
        "created_utc": 1469028248.0,
        "author": "TheBaldManCry",
        "is_submitter": true,
        "parent_id": "t1_d5jij3m",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "4p5ln4",
    "title": "[Survey] how do you interact with data at work (x/post r/datascience)",
    "selftext": "Hello fellow data workers!\nLately I‚Äôve been getting rather frustrated with some things at work, and was wondering if this was endemic to just my workplace, or to the field as a whole. Like a good statistician, I‚Äôm reaching out to all of you in the hopes that you‚Äôll answer a 5 minute (okay, so far it takes the average responder 6.5 minutes to finish), 16 question survey, but like a bad statistician, the input text fields are free form. For every person who fills out the survey, I‚Äôll donate $1 to CodeNow, a non-profit that helps inner city kids learn to program (up to $1000).\n\n[Survey here. Thanks in advance for the help!](https://docs.google.com/forms/d/1dT4FuUgYTPblxCGFSWs1kmMVC28ZxDjEsY0kCMV4mNQ/viewform)\n\n\n\nSorry for formatting; on mobile. ",
    "url": "https://www.reddit.com/r/datacleaning/comments/4p5ln4/survey_how_do_you_interact_with_data_at_work/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1466527909.0,
    "author": "talameetsbetty",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/4p5ln4/survey_how_do_you_interact_with_data_at_work/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "d4mqh17",
        "body": "Will you make the survey results publicly available?",
        "score": 1,
        "created_utc": 1466801179.0,
        "author": "friendlyburrito",
        "is_submitter": false,
        "parent_id": "t3_4p5ln4",
        "depth": 0
      },
      {
        "id": "d5jyxep",
        "body": "i will fill out the survey as i have also had big problems with this same thing at work.\n\nit's why i now work here: http://www.acquiro.com\n\nwould love to take the survey if it's still available and see the results. thanks!",
        "score": 1,
        "created_utc": 1469043597.0,
        "author": "DataGeekDenver",
        "is_submitter": false,
        "parent_id": "t3_4p5ln4",
        "depth": 0
      },
      {
        "id": "d4ngcaj",
        "body": "Yep! I'm going to total results either today or tomorrow. ",
        "score": 1,
        "created_utc": 1466859364.0,
        "author": "talameetsbetty",
        "is_submitter": true,
        "parent_id": "t1_d4mqh17",
        "depth": 1
      },
      {
        "id": "d5lnkqp",
        "body": "Stopped accepting responses.  Thanks though!  I'll probably PM you. :-)",
        "score": 1,
        "created_utc": 1469144215.0,
        "author": "talameetsbetty",
        "is_submitter": true,
        "parent_id": "t1_d5jyxep",
        "depth": 1
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "4muped",
    "title": "Cleaning Content so that it is \"HTML Free\"",
    "selftext": "So I am building an online recommendation tool based on topic modelling and the data I need to work on is from blog posts. Now, these blog posts are from my college's MongoDB system and I can fetch it through querying but the problem is that this data has HTML formatting and CSS settings which makes it really hard to work with and adds a lot of noise in the topic model if applied without filtering for obvious reasons. I am currently using python to build a flask app to do everything  and is there a good way to remove everything that would  be included in \"<\" and \">\" tags. I am not so well versed with string processing in python and the  help will be really appreciated. ",
    "url": "https://www.reddit.com/r/datacleaning/comments/4muped/cleaning_content_so_that_it_is_html_free/",
    "score": 3,
    "upvote_ratio": 0.81,
    "num_comments": 2,
    "created_utc": 1465240558.0,
    "author": "kmishra23",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/4muped/cleaning_content_so_that_it_is_html_free/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "d3ysl9d",
        "body": "Either use beautifulsoup or lxml. Beautiful Soup is perhaps easier to start with and lxml may have features you want.\n\nbeautifulsoup: https://www.crummy.com/software/BeautifulSoup/\n\nsome beautifulsoup tutorial: http://www.pythonforbeginners.com/beautifulsoup/beautifulsoup-4-python\n\nlxml: http://lxml.de/",
        "score": 2,
        "created_utc": 1465258706.0,
        "author": "nimbletine_beverages",
        "is_submitter": false,
        "parent_id": "t3_4muped",
        "depth": 0
      },
      {
        "id": "d3ysp1g",
        "body": "Awesome. Will check it out. I thought would have to do string processing but this looks good. Will update here if it worked. Thanks for the help.",
        "score": 1,
        "created_utc": 1465258863.0,
        "author": "kmishra23",
        "is_submitter": true,
        "parent_id": "t1_d3ysl9d",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "4jnv8p",
    "title": "Someone among you have experienced this issue when your are clustering in Open Refine?",
    "selftext": "http://stackoverflow.com/questions/37263700/browser-cluster-link-does-not-work-properly-in-open-refine",
    "url": "https://www.reddit.com/r/datacleaning/comments/4jnv8p/someone_among_you_have_experienced_this_issue/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1463439908.0,
    "author": "estebanpdl",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/4jnv8p/someone_among_you_have_experienced_this_issue/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "4g0boc",
    "title": "Dataproofer - new tool for proof-reading data",
    "selftext": "",
    "url": "https://vimeo.com/163511997",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1461356644.0,
    "author": "SherbertHerbert",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/4g0boc/dataproofer_new_tool_for_proofreading_data/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "478hbc",
    "title": "The role of human collaboration in data preparation.",
    "selftext": "",
    "url": "http://blog.datachili.com/collaboration.html",
    "score": 5,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1456257874.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/478hbc/the_role_of_human_collaboration_in_data/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "44iuwg",
    "title": "Cleaning the Imagenet 2014 dataset collected notes",
    "selftext": "",
    "url": "http://da-data.blogspot.com/2016/02/cleaning-imagenet-dataset-collected.html",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1454802561.0,
    "author": "dga-dave",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/44iuwg/cleaning_the_imagenet_2014_dataset_collected_notes/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "czqtmvu",
        "body": "Nice article! I have a question- given that some of the bounding boxes occur completely outside of the images, how do you know that the other bounding boxes were accurately defined?",
        "score": 1,
        "created_utc": 1454825488.0,
        "author": "friendlyburrito",
        "is_submitter": false,
        "parent_id": "t3_44iuwg",
        "depth": 0
      },
      {
        "id": "czr42m1",
        "body": "I doubt they're all correct.  Visual inspection suggests that \"most\" (i.e., all the ones that I checked that weren't numerically bogus, but that was only about 30) are, though.  The imagenet paper in section 3.2.1:  http://arxiv.org/abs/1409.0575   describes how they collected the bboxes.  (Appendix E also describes why 1762 of them are excluded from object localization - mostly overlapping/ambiguous bboxes, which, in my mind, means that they probably don't deserve to be excluded from pure categorization.  But they appear in the blacklist file for that contest in '14.)  They estimate in 7.2 that they have about 97.9% bbox accuracy (all instances of the target in the image are labeled with a bbox).",
        "score": 1,
        "created_utc": 1454860884.0,
        "author": "dga-dave",
        "is_submitter": true,
        "parent_id": "t1_czqtmvu",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "439by1",
    "title": "Suggestions for cleaning email",
    "selftext": "Hey Redditors,\n\nI have mulitple text files of basically email dumps from the past years. What I want to do is properly form the emails from initial correspondence down to the last reply. \n\nOne problem is that within the email thread there is repeated \"replies\" and what I do not want to do is essentialy index the same data.\n\nAre there any python libraries out there that would detect the beginning and end of the message? \n\nThe end product I'm want to do is these email have questions with answers within the reply. I'd like to create a knowledge base based off this data. \n\nAny direction would be greatly appreciated!\n",
    "url": "https://www.reddit.com/r/datacleaning/comments/439by1/suggestions_for_cleaning_email/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1454082530.0,
    "author": "joules32",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/439by1/suggestions_for_cleaning_email/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "d0z5t5t",
        "body": "Hey /u/joules32, I have a similar requirement  - I want to split an email thread into different emails and also parse an individual email into different sections like email body, signature and email meta. \nI tried using https://github.com/mailgun/talon, but it did not work for me - maybe it will for you.\nSo, did you figure out a solution for this?",
        "score": 1,
        "created_utc": 1457947243.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_439by1",
        "depth": 0
      },
      {
        "id": "d2ixazm",
        "body": "i haven't tried but will be looking into this library. Thanks for the info there...If you had any luck yourself, I'd love hear it!",
        "score": 1,
        "created_utc": 1461729420.0,
        "author": "joules32",
        "is_submitter": true,
        "parent_id": "t1_d0z5t5t",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "423wjo",
    "title": "Roundup Of Analytics, Big Data & Business Intelligence Forecasts And Market Estimates (2015)",
    "selftext": "",
    "url": "http://www.forbes.com/sites/louiscolumbus/2015/05/25/roundup-of-analytics-big-data-business-intelligence-forecasts-and-market-estimates-2015",
    "score": 3,
    "upvote_ratio": 0.8,
    "num_comments": 0,
    "created_utc": 1453436454.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/423wjo/roundup_of_analytics_big_data_business/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "40och4",
    "title": "Show /r/datacleaning: R in Visual Studio",
    "selftext": "",
    "url": "https://www.youtube.com/watch?v=Y1_0XN-p3Hs",
    "score": 8,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1452631797.0,
    "author": "smortaz",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/40och4/show_rdatacleaning_r_in_visual_studio/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "3y84z8",
    "title": "Common Data Pitfalls for Recurring Machine Learning Systems",
    "selftext": "",
    "url": "http://www.willmcginnis.com/2015/12/20/common-data-pitfalls-for-recurring-machine-learning-systems/",
    "score": 5,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1451084243.0,
    "author": "wdm006",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/3y84z8/common_data_pitfalls_for_recurring_machine/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "3xufef",
    "title": "Data Starved ¬∑ Racial Segregation in Ohio Today",
    "selftext": "",
    "url": "http://abdalah.github.io/Racial-Segregation-in-Ohio-Today/",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1450799184.0,
    "author": "aarmhe",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/3xufef/data_starved_racial_segregation_in_ohio_today/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "3x0tq6",
    "title": "Bad data guide : problems seen in real-world data along with suggestions on how to resolve them.",
    "selftext": "",
    "url": "http://github.com/Quartz/bad-data-guide",
    "score": 10,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1450230674.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/3x0tq6/bad_data_guide_problems_seen_in_realworld_data/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "cy1hhn2",
        "body": "Cool list! Not sure if it is yours, but I think it would benefit from some structure in each point. Like:\n\n###The problem title\n\n**Description**: What the problem is, or how to identify that this is an occurring issue.\n\n**Common solutions/fixes**: Common techniques/ways of solving this issue, or even just pointing out that there's really nothing great to do. (things that may mitigate would also fit)",
        "score": 1,
        "created_utc": 1450301884.0,
        "author": "relativer",
        "is_submitter": false,
        "parent_id": "t3_3x0tq6",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "3uabr4",
    "title": "Open Refine - A open source tool from google to clean data and connect it with open data",
    "selftext": "Video Demo: [youtube](https://www.youtube.com/watch?v=5tsyz3ibYzk)\n\nMore info at: [OpenRefine](http://openrefine.org/)",
    "url": "https://www.reddit.com/r/datacleaning/comments/3uabr4/open_refine_a_open_source_tool_from_google_to/",
    "score": 10,
    "upvote_ratio": 0.92,
    "num_comments": 0,
    "created_utc": 1448498308.0,
    "author": "raus22",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/3uabr4/open_refine_a_open_source_tool_from_google_to/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "3u4mm2",
    "title": "Bad data costing US businesses $700 billion a year (2010 report).",
    "selftext": "",
    "url": "http://about.datamonitor.com/media/archives/4871",
    "score": 3,
    "upvote_ratio": 0.8,
    "num_comments": 0,
    "created_utc": 1448399206.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/3u4mm2/bad_data_costing_us_businesses_700_billion_a_year/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "3tb1ez",
    "title": "How have you been obfuscating your data, or found it obfuscated?",
    "selftext": "In January, my friend Travis and I are doing a talk at Data Wranglers DC entitled ‚ÄòBlack Hat Data Wrangling‚Äô where we‚Äôll be discussing different ways of obfuscating data, and how to make that data accessible.\n\nWe have a growing list of online and offline methods including:\n\n* No API\n* Javascript overlays\n* Rendering to images\n* Printing it all out, scanning it, faxing it and then scanning it again (yes, this has happened)\n* Snail mail a hard drive (people still do this too)\n\nWhat methods of data obfuscation have you used or encountered? We‚Äôd love to add them to our talk.\n\nThanks for your help!",
    "url": "https://www.reddit.com/r/datacleaning/comments/3tb1ez/how_have_you_been_obfuscating_your_data_or_found/",
    "score": 4,
    "upvote_ratio": 0.76,
    "num_comments": 4,
    "created_utc": 1447862070.0,
    "author": "robertdempsey",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/3tb1ez/how_have_you_been_obfuscating_your_data_or_found/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "cx7b8mf",
        "body": "I have written a few techniques in the context of data publishing. The idea is to maintain privacy while also maintaining the quality of the published data.\n\n- Generalizing values (e.g., the specific age of a user in a table might be 24 years old and a generalized version of this is the range 20-30). This technique can be used on sensitive values in sensitive columns (e.g., age is a sensitive value that you might not want to reveal to people). There are metrics like k-anonymity and l-diversity that can be used to control the extent of generalization.\n\n- Randomly perturbing values in the table while preserving the statistical information in the table. For example, a specific value can be randomly modified within the table or swapped with another value in this table. This will degrade the quality of the statistical queries that are called on this table. This technique is typically used for publishing statistical databases. An example paper here is \"The boundary between privacy and utility\nin data publishing\" by Rastogi et Al.\n\n- There are metric embedding techniques that can be used to obfuscate data values within tables that preserve certain properties within that table. For example, in the paper \"Privacy preserving schema and data matching\" by Scannapieco et Al., the authors use SparseMap embedding to obfuscate tables.\n\nThe common theme in data publishing is that you want to preserve some amount of privacy by transforming the original dataset but you also want the resulting dataset to be useful somehow. Let me know if you want more references on this topic.",
        "score": 3,
        "created_utc": 1448053517.0,
        "author": "datachili",
        "is_submitter": false,
        "parent_id": "t3_3tb1ez",
        "depth": 0
      },
      {
        "id": "cyn1t97",
        "body": "Super cool! Please post after the event with links to resources / follow-up thoughts if you can. :)",
        "score": 1,
        "created_utc": 1452019763.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_3tb1ez",
        "depth": 0
      },
      {
        "id": "cxa2sfa",
        "body": "Awesome. Thanks!",
        "score": 2,
        "created_utc": 1448283321.0,
        "author": "robertdempsey",
        "is_submitter": true,
        "parent_id": "t1_cx7b8mf",
        "depth": 1
      },
      {
        "id": "cyn1ory",
        "body": "Fascinating. Re: Perturbing values, do you know whether these methods could be used for spatial data and/or networked data?",
        "score": 1,
        "created_utc": 1452019595.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t1_cx7b8mf",
        "depth": 1
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "3r30bo",
    "title": "Cleaning data with python tutorial from the University of Toronto",
    "selftext": "",
    "url": "https://data.library.utoronto.ca/cleaning-data-python",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1446388866.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/3r30bo/cleaning_data_with_python_tutorial_from_the/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "cwxx9ov",
        "body": "Does anybody have a link to cchs2012.csv? I can't find it using the hints provided.",
        "score": 1,
        "created_utc": 1447352089.0,
        "author": "dbabbitt",
        "is_submitter": false,
        "parent_id": "t3_3r30bo",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "3o6y6u",
    "title": "Anyone know of a good data cleaning API?",
    "selftext": "Hi everyone. Looking for a simple RESTful API for data cleaning. I looked at MTurk but the API is super old school and complicated to use. I also don't really want to manage workers and tasks. I just want to make an API call and get back a high confidence response. any suggestions?",
    "url": "https://www.reddit.com/r/datacleaning/comments/3o6y6u/anyone_know_of_a_good_data_cleaning_api/",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1444457997.0,
    "author": "husseinfazal",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/3o6y6u/anyone_know_of_a_good_data_cleaning_api/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "cvumis4",
        "body": "What kind of data cleaning do you need? I recently worked on a project to clean data to determine targets, predictors, unique identifiers and to merge string and numerical data.",
        "score": 1,
        "created_utc": 1444464879.0,
        "author": "gram3000",
        "is_submitter": false,
        "parent_id": "t3_3o6y6u",
        "depth": 0
      },
      {
        "id": "cw8k2x5",
        "body": "Mostly just researching the space. Trying to understand the market for 'human powered data cleaning'. Seeing if there are competitors to crowdsourcing companies like MTurk.",
        "score": 1,
        "created_utc": 1445491628.0,
        "author": "husseinfazal",
        "is_submitter": true,
        "parent_id": "t3_3o6y6u",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "3mp1ub",
    "title": "A Data Cleaning Example in R",
    "selftext": "",
    "url": "http://justanotherdatablog.blogspot.ie/2015/09/a-data-cleaning-example.html",
    "score": 3,
    "upvote_ratio": 0.67,
    "num_comments": 0,
    "created_utc": 1443445651.0,
    "author": "srkiboy83",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/3mp1ub/a_data_cleaning_example_in_r/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "3hfeoc",
    "title": "Data Quality: An Introduction to Data Profiling",
    "selftext": "",
    "url": "http://www.datalytyx.com/data-quality-an-introduction-to-data-profiling/",
    "score": 3,
    "upvote_ratio": 0.8,
    "num_comments": 0,
    "created_utc": 1439891159.0,
    "author": "gibran_kazi",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/3hfeoc/data_quality_an_introduction_to_data_profiling/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "3g1dxm",
    "title": "How to test the quality of your web data",
    "selftext": "",
    "url": "http://blog.import.io/post/how-to-test-the-quality-of-web-data?utm_source=reddit.com&utm_medium=referral&utm_campaign=how-to-test-the-quality-of-web-data&utm_content=r/datacleaning",
    "score": 2,
    "upvote_ratio": 0.67,
    "num_comments": 0,
    "created_utc": 1438887831.0,
    "author": "jmethvin88",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/3g1dxm/how_to_test_the_quality_of_your_web_data/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "3bbqb4",
    "title": "16 Free Data Science Books [x-post from /r/machinelearning]",
    "selftext": "",
    "url": "http://www.wzchen.com/data-science-books",
    "score": 11,
    "upvote_ratio": 0.77,
    "num_comments": 0,
    "created_utc": 1435429236.0,
    "author": "deltawk",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/3bbqb4/16_free_data_science_books_xpost_from/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "3aygqk",
    "title": "Glossary of data quality terms from the IAIDQ",
    "selftext": "",
    "url": "http://iaidq.org/main/glossary.shtml",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1435157254.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/3aygqk/glossary_of_data_quality_terms_from_the_iaidq/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "36n4f1",
    "title": "\"Data quality problems cost U.S. businesses more than $600 billion a year\"- a report from 2002.",
    "selftext": "",
    "url": "http://download.101com.com/pub/tdwi/Files/DQReport.pdf",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1432139263.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/36n4f1/data_quality_problems_cost_us_businesses_more/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "crfjc24",
        "body": "thought exercise: if it was that much in 2002 is it less now (because we have more mature tools), or is it greater now because we have so much more data?",
        "score": 1,
        "created_utc": 1432150052.0,
        "author": "sputknick",
        "is_submitter": false,
        "parent_id": "t3_36n4f1",
        "depth": 0
      },
      {
        "id": "crgk6vm",
        "body": "It's difficult to be definitive without a reference of some sort. I suspect its greater because the 80% number is still thrown around (data scientists cap spend up to 80% of their time correcting data errors). Since there is a lot more data (internet of things, etc), its probably higher. \n\nIf you find a more up to date number, do let me know.\n\nEDIT : Found a 2010 report which estimates this number to $700 billion (http://about.datamonitor.com/media/archives/4871).",
        "score": 1,
        "created_utc": 1432230756.0,
        "author": "datachili",
        "is_submitter": true,
        "parent_id": "t1_crfjc24",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "364h54",
    "title": "New scala distributed entity resolution system",
    "selftext": "",
    "url": "http://sampleclean.org/release.html",
    "score": 3,
    "upvote_ratio": 0.81,
    "num_comments": 1,
    "created_utc": 1431739714.0,
    "author": "dataflooder",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/364h54/new_scala_distributed_entity_resolution_system/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "crfcivd",
        "body": "Nice project!",
        "score": 1,
        "created_utc": 1432139587.0,
        "author": "datachili",
        "is_submitter": false,
        "parent_id": "t3_364h54",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "35odas",
    "title": "data cleaning using multi-target decision trees",
    "selftext": "",
    "url": "https://github.com/AmmsA/DTCleaner",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1431405666.0,
    "author": "BScHolder",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/35odas/data_cleaning_using_multitarget_decision_trees/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "cr7aftk",
        "body": "Nice project. So if a tuple violates even 1 CFD, then it is added to the test set? I suppose there is no distinction between a tuple which violates 1 CFD vs n CFDs? \n\nOne interesting extension would be to add user feedback on the predictions. Selected repairs can be used to retrain the classifier.",
        "score": 1,
        "created_utc": 1431481251.0,
        "author": "friendlyburrito",
        "is_submitter": false,
        "parent_id": "t3_35odas",
        "depth": 0
      },
      {
        "id": "cr7eel3",
        "body": "It depends, if the tuple violates a set of CFDs 'x' that have the same premise (LHS) then its added to the test and we'd predict the LHS(x) and the RHS(x) attributes of that tuple. If a tuple violates a single CFD, it is also added to the test and we'd predict values of the attributes in the CFD that the tuple violates. Now if a tuple violates CFDs that don't have the same premise (disjoint CFDs), then things get a little complicated and I haven't completed the code for this scenario.\n\nI agree. Having user feedback would help with the accuracy of future rounds of predictions. ",
        "score": 1,
        "created_utc": 1431488154.0,
        "author": "BScHolder",
        "is_submitter": true,
        "parent_id": "t1_cr7aftk",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "331ksd",
    "title": "Data tidying with R",
    "selftext": "",
    "url": "http://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html",
    "score": 4,
    "upvote_ratio": 0.71,
    "num_comments": 2,
    "created_utc": 1429376577.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/331ksd/data_tidying_with_r/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "cqgokeb",
        "body": "Thanks for this! ",
        "score": 2,
        "created_utc": 1429380733.0,
        "author": "DontTrustGandhi",
        "is_submitter": false,
        "parent_id": "t3_331ksd",
        "depth": 0
      },
      {
        "id": "cqgsg4a",
        "body": ":)",
        "score": 1,
        "created_utc": 1429388007.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t1_cqgokeb",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "32itoi",
    "title": "[SIGMOD '15] A Data Cleaning System Powered by Knowledge Bases and Crowdsourcing",
    "selftext": "",
    "url": "http://projects.qcri.org/~ntang/Site/pubs/katara.pdf",
    "score": 4,
    "upvote_ratio": 0.84,
    "num_comments": 0,
    "created_utc": 1428981566.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/32itoi/sigmod_15_a_data_cleaning_system_powered_by/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2zxde5",
    "title": "KDnugget's list of data transformation and data cleaning software",
    "selftext": "",
    "url": "http://www.kdnuggets.com/software/data-cleaning.html",
    "score": 1,
    "upvote_ratio": 0.6,
    "num_comments": 0,
    "created_utc": 1427049443.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2zxde5/kdnuggets_list_of_data_transformation_and_data/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2ww6yo",
    "title": "Informative thread about the data pipeline from r/datascience",
    "selftext": "",
    "url": "http://www.reddit.com/r/datascience/comments/2vv485/getting_cleaning_combining_and_storing_data/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1424715419.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2ww6yo/informative_thread_about_the_data_pipeline_from/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2vkriv",
    "title": "Data quality : Getting value from Big Data.",
    "selftext": "",
    "url": "http://wp.sigmod.org/?p=1519",
    "score": 2,
    "upvote_ratio": 0.67,
    "num_comments": 0,
    "created_utc": 1423688167.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2vkriv/data_quality_getting_value_from_big_data/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2uaw4g",
    "title": "Great tool for cleaning .csv files: csvkit",
    "selftext": "",
    "url": "https://csvkit.readthedocs.org/en/0.9.0/",
    "score": 6,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1422710333.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2uaw4g/great_tool_for_cleaning_csv_files_csvkit/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "cocs4cn",
        "body": "Yep, it's an amazingly useful tool.  I use csvsql all the time when I have a huge csv and need to dump into sql quickly without an existing table.",
        "score": 1,
        "created_utc": 1423196815.0,
        "author": "nameBrandon",
        "is_submitter": false,
        "parent_id": "t3_2uaw4g",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "2rztgi",
    "title": "[ICDE 2011] A Unified Model for Data and Constraint Repair.",
    "selftext": "",
    "url": "http://dblab.cs.toronto.edu/~fchiang/docs/icde11.pdf",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1420922746.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2rztgi/icde_2011_a_unified_model_for_data_and_constraint/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2reb0o",
    "title": "Chicago's open ETL kit: \"Use Pentaho's open source data integration tool (Kettle) to create Extract-Transform-Load (ETL) processes to update a Socrata open data portal.\"",
    "selftext": "",
    "url": "https://github.com/Chicago/open-data-etl-utility-kit",
    "score": 3,
    "upvote_ratio": 0.81,
    "num_comments": 0,
    "created_utc": 1420461844.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2reb0o/chicagos_open_etl_kit_use_pentahos_open_source/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2otvig",
    "title": "[ICDE 2013 paper] Holistic Data Cleaning",
    "selftext": "",
    "url": "http://cs.uwaterloo.ca/~ilyas/papers/XuICDE2013.pdf",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1418183309.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2otvig/icde_2013_paper_holistic_data_cleaning/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2mwwcr",
    "title": "OpenRefine : Really cool data augmentation features (e.g., entity matching on freebase.com)",
    "selftext": "",
    "url": "https://www.youtube.com/watch?v=5tsyz3ibYzk",
    "score": 3,
    "upvote_ratio": 0.81,
    "num_comments": 1,
    "created_utc": 1416517903.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2mwwcr/openrefine_really_cool_data_augmentation_features/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "cm8tfnz",
        "body": "\nFor those interested, there's [this](http://programminghistorian.org/lessons/cleaning-data-with-openrefine) tutorial on ProgrammingHistorian as well, on how to clean your data with OpenRefine.\nFor those wanting to go further, there's this [book](http://www.goodreads.com/book/show/18502394-using-openrefine) (AFAIK the only book written on OpenRefine). \n(Full disclosure: the book and tutorial have been written by colleagues of mine)\n\n\nIf you want to go even further than data matching, we have developed an extension for OpenRefine which allows to do Named-Entity Recognition within OpenRefine: [click](http://freeyourmetadata.org/named-entity-extraction/). You'll need API keys though, but most of the services offer a \"full-features-but-limited-use\" free API key.",
        "score": 1,
        "created_utc": 1416559615.0,
        "author": "fawkesdotbe",
        "is_submitter": false,
        "parent_id": "t3_2mwwcr",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "2mxaa8",
    "title": "The data science specialization section on cleaning data via github",
    "selftext": "",
    "url": "http://datasciencespecialization.github.io/getclean/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1416524385.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2mxaa8/the_data_science_specialization_section_on/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2lvkn8",
    "title": "Data Munging with Perl",
    "selftext": "",
    "url": "http://perlhacks.com/dmp.pdf",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1415641192.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2lvkn8/data_munging_with_perl/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2lqi29",
    "title": "[SIGMOD '03] Robust and Efficient Fuzzy Match for Online Data Cleaning.",
    "selftext": "",
    "url": "http://dc-pubs.dbs.uni-leipzig.de/files/Chaudhuri2003Robustandefficientfuzzymatchforonlinedatacleaning.pdf",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1415506704.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2lqi29/sigmod_03_robust_and_efficient_fuzzy_match_for/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2kpq4w",
    "title": "\"What Data Cleaning Can and Can't Teach\"",
    "selftext": "",
    "url": "https://infoactive.co/data-design/ch10.html",
    "score": 2,
    "upvote_ratio": 0.76,
    "num_comments": 1,
    "created_utc": 1414616763.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2kpq4w/what_data_cleaning_can_and_cant_teach/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "clrjipz",
        "body": "A brief introduction to MAR, MCAR and MNAR wouldn't go astray in order to clarify the final paragraph, but otherwise a nice beginner's guide to what it's all about.",
        "score": 2,
        "created_utc": 1414998799.0,
        "author": "enigmakthx",
        "is_submitter": false,
        "parent_id": "t3_2kpq4w",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "2khwqe",
    "title": "Where‚Äôs the Data in the Big Data Wave?",
    "selftext": "",
    "url": "http://wp.sigmod.org/?p=786",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1414441330.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2khwqe/wheres_the_data_in_the_big_data_wave/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2jhdot",
    "title": "[SIGMOD '05] A Cost-Based model and effective heuristic for repairing constraints by value modification.",
    "selftext": "",
    "url": "http://homepages.inf.ed.ac.uk/wenfei/papers/sigmod05.pdf",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1413514514.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2jhdot/sigmod_05_a_costbased_model_and_effective/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2iexqh",
    "title": "[VLDB Paper] Guided data repair",
    "selftext": "",
    "url": "http://cs.uwaterloo.ca/~ilyas/papers/YakoutVLDB2011.pdf",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1412565065.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2iexqh/vldb_paper_guided_data_repair/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2i7uyg",
    "title": "Dedupe -- A python library for accurate and scaleable data deduplication and entity-resolution",
    "selftext": "",
    "url": "https://github.com/datamade/dedupe",
    "score": 5,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1412366483.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2i7uyg/dedupe_a_python_library_for_accurate_and/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "cmnohzm",
        "body": "Does anybody have any information on how this package operates in the field?",
        "score": 1,
        "created_utc": 1417943458.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_2i7uyg",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "2hnbgm",
    "title": "[Software] TANE : Automatically detect functional dependencies in your dataset.",
    "selftext": "",
    "url": "http://www.cs.helsinki.fi/research/fdk/datamining/tane/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1411853604.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2hnbgm/software_tane_automatically_detect_functional/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2gs7nu",
    "title": "[ICDE] Conditional Functional Dependencies for Data Cleaning",
    "selftext": "",
    "url": "http://homepages.inf.ed.ac.uk/wenfei/papers/icde07-cfd.pdf",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1411064166.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2gs7nu/icde_conditional_functional_dependencies_for_data/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2gfhqr",
    "title": "Stanford's Data Wrangler",
    "selftext": "",
    "url": "http://vis.stanford.edu/wrangler/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1410754294.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2gfhqr/stanfords_data_wrangler/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "ckio2o3",
        "body": "Care to share? I've always used and promoted OpenRefine (formerly Google Refine and before that Freebase Gridworks), I'm even giving a workshop on how to clean data using it tomorrow.\n\nWould love any first-hand feedback on Data Wrangler though...",
        "score": 1,
        "created_utc": 1410758249.0,
        "author": "fawkesdotbe",
        "is_submitter": false,
        "parent_id": "t3_2gfhqr",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "2g7hgx",
    "title": "[Github] NADEEF- A Generalized Data Cleaning System, with paper published in SIGMOD 2013",
    "selftext": "",
    "url": "https://github.com/Qatar-Computing-Research-Institute/NADEEF",
    "score": 2,
    "upvote_ratio": 0.76,
    "num_comments": 1,
    "created_utc": 1410535728.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2g7hgx/github_nadeef_a_generalized_data_cleaning_system/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "ckktx1a",
        "body": "Any info on how to deploy this system from github? ",
        "score": 1,
        "created_utc": 1410963274.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_2g7hgx",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "2g5qw1",
    "title": "Data cleaning in a physical sense, check out the NSIT's special publication 800-88, \"Guidelines for Media Sanitization\"",
    "selftext": "",
    "url": "http://csrc.nist.gov/publications/PubsSPs.html",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1410484434.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2g5qw1/data_cleaning_in_a_physical_sense_check_out_the/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "ckgc2an",
        "body": "By their definition- \"Sanitization refers to a process that renders access to target data on the media infeasible for a given level of effort\". Sounds like the paper is more concerned with data privacy.",
        "score": 1,
        "created_utc": 1410535577.0,
        "author": "datachili",
        "is_submitter": false,
        "parent_id": "t3_2g5qw1",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "2fiiku",
    "title": "VLDB '01 : Declarative data cleaning- language, model and algorithms.",
    "selftext": "",
    "url": "http://www.dia.uniroma3.it/~vldbproc/041_371.pdf",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1409883033.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2fiiku/vldb_01_declarative_data_cleaning_language_model/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2eu0cb",
    "title": "\"Getting and Cleaning Data\" -- Coursera MOOC offered by Johns Hopkins University faculty, begins September 1st",
    "selftext": "",
    "url": "https://www.coursera.org/course/getdata",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1409242361.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2eu0cb/getting_and_cleaning_data_coursera_mooc_offered/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2esfqo",
    "title": "Record Linkage: Similarity Measures and Algorithms [pdf slides]",
    "selftext": "",
    "url": "http://queens.db.toronto.edu/~koudas/docs/aj.pdf",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1409195872.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2esfqo/record_linkage_similarity_measures_and_algorithms/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2ei5ac",
    "title": "[PDF slides] Classic data cleaning system : Potter's Wheel.",
    "selftext": "",
    "url": "https://www.cise.ufl.edu/class/cis6930fa11lad/cis6930fa11_Potter's_Wheel.pdf",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1408941616.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2ei5ac/pdf_slides_classic_data_cleaning_system_potters/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2egpyb",
    "title": "\"Data Cleaning 101\" from faculty at UCLA's Institute for Digital Research",
    "selftext": "",
    "url": "http://www.ats.ucla.edu/stat/sas/library/nesug99/ss123.pdf",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1408907280.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2egpyb/data_cleaning_101_from_faculty_at_uclas_institute/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2ec00m",
    "title": "Quick introduction to functional dependencies (FDs) on databases. FDs can be used to detect dirty records for data cleaning.",
    "selftext": "",
    "url": "http://www.inf.unibz.it/~nutt/IDBs1011/IDBSlides/15-db-fds-2.pdf",
    "score": 2,
    "upvote_ratio": 0.76,
    "num_comments": 0,
    "created_utc": 1408759484.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2ec00m/quick_introduction_to_functional_dependencies_fds/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2eaet8",
    "title": "Data structures and algorithms in Java, for your data cleaning needs!",
    "selftext": "",
    "url": "https://sites.google.com/site/indy256/algo",
    "score": 2,
    "upvote_ratio": 0.75,
    "num_comments": 1,
    "created_utc": 1408724569.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2eaet8/data_structures_and_algorithms_in_java_for_your/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "cjy0ep1",
        "body": "Man, I wish I had seen this earlier, esp for the kd-tree. I ended up using the kd-tree from here instead : http://home.wlu.edu/~levys/software/kd/",
        "score": 2,
        "created_utc": 1408758502.0,
        "author": "datachili",
        "is_submitter": false,
        "parent_id": "t3_2eaet8",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "2e47ku",
    "title": "Short and sweet illustration of cleaning animal tracking data",
    "selftext": "",
    "url": "http://smathermather.wordpress.com/2014/07/31/cleaning-animal-tracking-data-throwing-away-extra-points/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1408567816.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2e47ku/short_and_sweet_illustration_of_cleaning_animal/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2e0udq",
    "title": "\"An introduction to data cleaning with R\"",
    "selftext": "",
    "url": "http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf",
    "score": 8,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1408484552.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2e0udq/an_introduction_to_data_cleaning_with_r/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "2e0dug",
    "title": "CTO of Cloudera : Data analysts can spent up to 80% of their time on data cleaning.",
    "selftext": "",
    "url": "http://youtu.be/ctU2V7v7-WU?t=2m12s",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1408475711.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2e0dug/cto_of_cloudera_data_analysts_can_spent_up_to_80/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "cjv501z",
        "body": "THis is exactly what my old manager told me before I got hired. 80% of the work is data cleaning and accumulation. 20% is analysis.",
        "score": 2,
        "created_utc": 1408498029.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_2e0dug",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "2dzzn6",
    "title": "Favorite tool for 'on the fly' data cleaning?",
    "selftext": "What's your favorite go-to language or software for data cleaning? ",
    "url": "https://www.reddit.com/r/datacleaning/comments/2dzzn6/favorite_tool_for_on_the_fly_data_cleaning/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1408468566.0,
    "author": null,
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2dzzn6/favorite_tool_for_on_the_fly_data_cleaning/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "cjutfmk",
        "body": "I have used Data Wrangler before (http://vis.stanford.edu/wrangler/app/) and OpenRefine. But typically, if the dataset is not too large, I tend to use Weka. There are many ETL tools our there too (e.g., http://www.talend.com/resource/etl-tool.html) but I've not used them before. Anybody have any good open source ETL tools to recommend?",
        "score": 2,
        "created_utc": 1408475275.0,
        "author": "datachili",
        "is_submitter": false,
        "parent_id": "t3_2dzzn6",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "2dxqa5",
    "title": "Moderators wanted!",
    "selftext": "Hi there. We are seeking 2 more moderators for /r/datacleaning. The tasks for the moderators include :\n\n1. Submitting interesting data cleaning articles at least once a week.\n\n2. Improving the css of /r/datacleaning to be more in line with /r/futurology.\n\n3. Promoting /r/datacleaning to a wider audience (those interested in machine learning, data analysis, statistics, data mining, algorithms, databases, etc).\n\nPM me or comment here if interested with a short description of your goals! Thanks!",
    "url": "https://www.reddit.com/r/datacleaning/comments/2dxqa5/moderators_wanted/",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1408408281.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2dxqa5/moderators_wanted/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "cjupu2p",
        "body": "Hey, I'm happy to help, except I don't know CSS very well. I can gather resources for beginners and interesting technical articles for sure, plus the promotional aspect. I'm not an expert in the field but am definitely interested.",
        "score": 1,
        "created_utc": 1408469052.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_2dxqa5",
        "depth": 0
      },
      {
        "id": "cjuxn9h",
        "body": "PMed you.",
        "score": 1,
        "created_utc": 1408482866.0,
        "author": "datachili",
        "is_submitter": true,
        "parent_id": "t1_cjupu2p",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "2dw5c8",
    "title": "NYtimes : Data munging is essential before big data analysis.",
    "selftext": "",
    "url": "http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?_r=0&referrer=",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1408376512.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/2dw5c8/nytimes_data_munging_is_essential_before_big_data/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "292qyp",
    "title": "An introduction to data cleaning- problems and current approaches.",
    "selftext": "",
    "url": "http://dc-pubs.dbs.uni-leipzig.de/files/Rahm2000DataCleaningProblemsand.pdf",
    "score": 2,
    "upvote_ratio": 0.63,
    "num_comments": 0,
    "created_utc": 1403718901.0,
    "author": "datachili",
    "subreddit": "datacleaning",
    "permalink": "/r/datacleaning/comments/292qyp/an_introduction_to_data_cleaning_problems_and/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  }
]