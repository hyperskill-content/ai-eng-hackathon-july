[
  {
    "id": "1lqnqt5",
    "title": "Is prompt engineering becoming part of the modern data science stack?",
    "selftext": "I’ve been noticing a shift lately more data teams are blending LLMs into their pipelines, and suddenly prompt engineering is part of the workflow.\n\nNot just for fun, either. I’ve seen it used in:\n\n* Auto-generating documentation\n* Summarizing messy datasets\n* Querying with natural language\n* Speeding up feature engineering\n\nBut here's my question:  \nIs this a trend that’s here to stay—or just a flashy add-on that’ll fade out once things settle?\n\nAre you or your team actively using tools like bbai, or GPT APIs in real workflows?  \nWhere’s the value showing up for you and where does it still fall short?\n\nWould love to hear how others in the field are (or aren't) adapting to this shift.",
    "url": "https://www.reddit.com/r/data/comments/1lqnqt5/is_prompt_engineering_becoming_part_of_the_modern/",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1751544007.0,
    "author": "NoPressure__",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lqnqt5/is_prompt_engineering_becoming_part_of_the_modern/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "n14619u",
        "body": "Once it gets to full momentum it will be unstoppable and intergrated into the workflow",
        "score": 1,
        "created_utc": 1751545654.0,
        "author": "Wierdbeardo",
        "is_submitter": false,
        "parent_id": "t3_1lqnqt5",
        "depth": 0
      },
      {
        "id": "n16cvdk",
        "body": "LLMs are quickly moving from novelty to essential tool in real data workflows!",
        "score": 1,
        "created_utc": 1751569042.0,
        "author": "No-Sprinkles-1662",
        "is_submitter": false,
        "parent_id": "t3_1lqnqt5",
        "depth": 0
      },
      {
        "id": "n17e08n",
        "body": "It's a no brainer for two reasons; it's the I didn't think approach (easy route) as well as a powerful tool to design schemas, rule sets.. different approaches to a project. I believe it also demonstrates a person's logical approach to a problem. One shot reports are easy, but complex systems or programs need systems knowledge, code knowledge, OSI etc just to ensure it will work.",
        "score": 1,
        "created_utc": 1751580260.0,
        "author": "SithLordRising",
        "is_submitter": false,
        "parent_id": "t3_1lqnqt5",
        "depth": 0
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1lqr0sl",
    "title": "cry for help",
    "selftext": "what can i do to land a data analyst job! my resume is not landing me interviews",
    "url": "https://www.reddit.com/r/data/comments/1lqr0sl/cry_for_help/",
    "score": 2,
    "upvote_ratio": 0.75,
    "num_comments": 2,
    "created_utc": 1751552968.0,
    "author": "Expensive-Builder-91",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lqr0sl/cry_for_help/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "n182ns8",
        "body": "I'm hearing this more often every day, seems to be a real shitshow right now",
        "score": 1,
        "created_utc": 1751588791.0,
        "author": "CrumbCakesAndCola",
        "is_submitter": false,
        "parent_id": "t3_1lqr0sl",
        "depth": 0
      },
      {
        "id": "n18mo7c",
        "body": "😔😔",
        "score": 1,
        "created_utc": 1751596297.0,
        "author": "Expensive-Builder-91",
        "is_submitter": true,
        "parent_id": "t1_n182ns8",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1lqtrnl",
    "title": "Automatic Report Generation from Questionnaire Data",
    "selftext": "Hi all,\n\nI am trying to find a way for ai/software/code to create a safety culture report (and other kinds of reports) simply by submitting the raw data of questionnaire/survey answers. I want it to create a good and solid first draft that i can tweak if need be. I have lots of these to do, so it saves me typing them all out individually.\n\n My report would include things such as an introduction, survey item tables, graphs and interpretative paragraphs of the results, plus a conclusion etc. I don't mind using different services/products.\n\n I have a budget of a few hundred dollars per months - but the less the better. The reports are based on survey data using questions based on 1-5 Likert statements such as from strongly disagree to strongly agree.  \n\nPlease, if you have any tips or suggestions, let me know!! Thanksssss",
    "url": "https://www.reddit.com/r/data/comments/1lqtrnl/automatic_report_generation_from_questionnaire/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 1,
    "created_utc": 1751559564.0,
    "author": "BodyFun5162",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lqtrnl/automatic_report_generation_from_questionnaire/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "n1848oj",
        "body": "Are the results already consolidated somewhere? A database would be ideal but even if it's just a comma delimited file. If not, that's your first step before anything else.",
        "score": 1,
        "created_utc": 1751589384.0,
        "author": "CrumbCakesAndCola",
        "is_submitter": false,
        "parent_id": "t3_1lqtrnl",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1lqm7qi",
    "title": "Education Resources Data Collection",
    "selftext": "Hi everyone,\n\nI've been struggling with this for the past few weeks and I honestly have no idea where else to ask this question, so I’m hoping someone here might be able to help, even some small advice would be appreciated.\n\nI’m currently working on a project to build a dashboard for computing education resources in the community. The focus is on out-of-school programs, things like after-school coding clubs, library events, university outreach programs, summer camps, etc.\n\nThe problem is: there’s no existing dataset for this kind of information, so I need to build a database from scratch. I’m stuck on how to collect these data in an efficient and scalable way. I don’t have much experience with data collection, and right now, the only way I can think of is manually searching and entering the information, which obviously is not ideal considering the time and effort, and wouldn't be a solution for long term.\n\nI was thinking about using something like the Yelp API, but it doesn’t really cover academic or nonprofit events very well.\n\nHas anyone encountered something like this before or have any idea on how to approach it? I’d really appreciate any advice, tools, or suggestions!",
    "url": "https://www.reddit.com/r/data/comments/1lqm7qi/education_resources_data_collection/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1751538714.0,
    "author": "CherryLetter",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lqm7qi/education_resources_data_collection/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "n18bag4",
        "body": "It sounds like you'll want a CRM type setup, such as [the open source SuiteCRM](https://github.com/SuiteCRM/SuiteCRM-Core). Initially you'll have to do some manual gathering of data, but once you get rolling it's easy to maintain.\n\nLibraries often keep collections of exactly this kind of data (and try the university outreach groups as well). Local newspapers or magazines usually keep track of these things too, or even local radio stations. Non-profits often help out other non-profits by allowing them to post flyers or leave business cards or advertise in newsletters.\n\nWhenever you identify a resource be sure to get specific contact names and the emails/phone to reach those specific contacts (as opposed to just calling the university you'll want to call the actual group inside the university that runs that program). Grab website and socials if they have em.\n\nOnce you have the database of groups you can start looking at automating dashboards, etc.",
        "score": 1,
        "created_utc": 1751592036.0,
        "author": "CrumbCakesAndCola",
        "is_submitter": false,
        "parent_id": "t3_1lqm7qi",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1lq602n",
    "title": "Hello mates I scrape bet365. If you wan't access to the API please write me a message.",
    "selftext": "Hello mates I scrape bet365. If you wan't access to the API please write me a message.",
    "url": "https://www.reddit.com/r/data/comments/1lq602n/hello_mates_i_scrape_bet365_if_you_want_access_to/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1751487071.0,
    "author": "Hot-Muscle-7021",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lq602n/hello_mates_i_scrape_bet365_if_you_want_access_to/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1lpz2tt",
    "title": "Where Can I Find Free & Reliable Live and Historical Indian Market Data?",
    "selftext": "Hey guys I was working on some tools and I need to get some Indian stock and options data. I need the following data Option Greeks (Delta, Gamma, Theta, Vega), Spot Price (Index Price), Bid Price, Ask Price, Open Interest (OI), Volume, Historical Open Interest, Historical Implied Volatility (IV), Historical Spot Price, Intraday OHLC Data, Historical Futures Price, Historical PCR, Historical Option Greeks (if possible), Historical FII/DII Data, FII/DII Daily Activity, MWPL (Market-Wide Position Limits), Rollout Data, Basis Data, Events Calendar, PCR (Put-Call Ratio), IV Rank, IV Skew, Volatility Surface, etc.. \n\nYeah I agree that this list is a bit too chunky. I'm really sorry for that.. I need to fetch this data from several sources( since no single source would be providing all this). Please drop some sources that provide data for fetching for a web tool. Preferably via API, scraping, websocket, repos and csvs. Please drop any source that can provide even a single data from the list, It would be really thankful. \n\nThanks in advance !   ",
    "url": "https://www.reddit.com/r/data/comments/1lpz2tt/where_can_i_find_free_reliable_live_and/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1751470711.0,
    "author": "-InvictusShadow",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lpz2tt/where_can_i_find_free_reliable_live_and/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1lprfn0",
    "title": "Select a dataset, Ask questions, get SQL queries and run them as you wish!",
    "selftext": "I've been working on this feature that lets you have actual conversations with your data. Drop any CSV/Excel/Parquet file into the DataKit and start asking questions. You can select your model as you wish with your own API key.\n\n**The privacy angle:** Everything runs locally. The AI only sees your schema (column names/types), never your actual data. Your sensitive info stays on your machine.\n\n**Data sources:** You can now pull directly from HuggingFace datasets, S3, or any URL. Been having fun exploring random public datasets - asking \"what's interesting here?\" and seeing what comes up.\n\nTry it: [https://datakit.page](https://datakit.page/)\n\nWhat's the hardest data question you're trying to answer right now?",
    "url": "https://v.redd.it/5blc8ad5kfaf1",
    "score": 4,
    "upvote_ratio": 0.84,
    "num_comments": 1,
    "created_utc": 1751448709.0,
    "author": "Sea-Assignment6371",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lprfn0/select_a_dataset_ask_questions_get_sql_queries/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "n0x37hq",
        "body": "It's a clever idea for more than one reason. Uploading data sets to the cloud is a great way of harvesting data for them. \n\nI find it easier just to import a database schema and build queries based on that, so it's local and unlimited in size which I imagine data set is not (context).",
        "score": 2,
        "created_utc": 1751452241.0,
        "author": "SithLordRising",
        "is_submitter": false,
        "parent_id": "t3_1lprfn0",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1lpxi5l",
    "title": "DATACAMP",
    "selftext": "I want to improve my data analysis skills and i saw this website, what do you think about?",
    "url": "https://www.reddit.com/r/data/comments/1lpxi5l/datacamp/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1751466995.0,
    "author": "RA106E",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lpxi5l/datacamp/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1lpd9i4",
    "title": "Weight Loss vs predicted based on calorie counting.",
    "selftext": "I thought I would share with the world my data on my weight vs how much I was predicted to lose based on calorie counting that included exercise.  It was way more accurate than I would have guessed.  For my experiment, I have had a minimum 500 calorie deficit during this time.",
    "url": "https://i.redd.it/ekm8lgvzsbaf1.png",
    "score": 7,
    "upvote_ratio": 0.89,
    "num_comments": 5,
    "created_utc": 1751403492.0,
    "author": "Much-Bit3531",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lpd9i4/weight_loss_vs_predicted_based_on_calorie_counting/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "n0umz7m",
        "body": "6/9 must’ve sucked for you.",
        "score": 1,
        "created_utc": 1751412631.0,
        "author": "cheeze_whizard",
        "is_submitter": false,
        "parent_id": "t3_1lpd9i4",
        "depth": 0
      },
      {
        "id": "n0we4k4",
        "body": "You should add hydration + defecation error clamps, weight yourself before and after morning toilet and (assuming we don't know which position is expected by calorie counting) clamps would be difference up and difference down form counted value each day (you can calculate error values once assuming volume of your bladder and wherever poop is stored don't change (and it's density doesn't too))",
        "score": 1,
        "created_utc": 1751437861.0,
        "author": "PimBel_PL",
        "is_submitter": false,
        "parent_id": "t3_1lpd9i4",
        "depth": 0
      },
      {
        "id": "n1b125t",
        "body": "I have a very similar chart in a Google sheet. I’m on my phone so my screenshot looks like trash jut I’ll try to remember to grab it later. How did you calculate your daily expenditure?",
        "score": 1,
        "created_utc": 1751636857.0,
        "author": "amosmj",
        "is_submitter": false,
        "parent_id": "t3_1lpd9i4",
        "depth": 0
      },
      {
        "id": "n0vdpip",
        "body": "Yes it did.   I was like wtf is going on.",
        "score": 1,
        "created_utc": 1751422002.0,
        "author": "Much-Bit3531",
        "is_submitter": true,
        "parent_id": "t1_n0umz7m",
        "depth": 1
      },
      {
        "id": "n185pii",
        "body": "Is this based on a single measurement at a certain time of day? You might try 2 or 3 measurements throughout the day to get a sense how much it fluctuates.",
        "score": 1,
        "created_utc": 1751589937.0,
        "author": "CrumbCakesAndCola",
        "is_submitter": false,
        "parent_id": "t1_n0vdpip",
        "depth": 2
      }
    ],
    "comments_extracted": 5
  },
  {
    "id": "1lposyu",
    "title": "Thriving in the Agentic Era: A Case for the Data Developer Platform",
    "selftext": "",
    "url": "https://moderndata101.substack.com/p/thriving-in-the-agentic-era-a-case",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1751438039.0,
    "author": "growth_man",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lposyu/thriving_in_the_agentic_era_a_case_for_the_data/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1lnwzox",
    "title": "Repositories where US government data has been backed-up, large projects and public archives that serve as alternatives to federal data sources, and subscription-based library databases. Visit these sources in the event that federal data becomes unavailable.",
    "selftext": "",
    "url": "https://libguides.brown.edu/socscidata/alternate_govdata",
    "score": 6,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1751252591.0,
    "author": "johnabbe",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lnwzox/repositories_where_us_government_data_has_been/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1lmn3di",
    "title": "What do you guys use to keep track of all your personal information? I was thinking of an editable document I can access anywhere where I can put my TIN, SSS, investments, insurance policies, account credentials etc. Any recommendations?",
    "selftext": "",
    "url": "https://www.reddit.com/r/data/comments/1lmn3di/what_do_you_guys_use_to_keep_track_of_all_your/",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 4,
    "created_utc": 1751118051.0,
    "author": "Excellent_Pause_220",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lmn3di/what_do_you_guys_use_to_keep_track_of_all_your/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "n0acwp7",
        "body": "DOGE team has entered the sub.",
        "score": 2,
        "created_utc": 1751137731.0,
        "author": "Thiseffingguy2",
        "is_submitter": false,
        "parent_id": "t3_1lmn3di",
        "depth": 0
      },
      {
        "id": "n0axby1",
        "body": "Encrypted text. Plenty of solutions or use a pastebin like https://0bin.net/",
        "score": 1,
        "created_utc": 1751144361.0,
        "author": "SithLordRising",
        "is_submitter": false,
        "parent_id": "t3_1lmn3di",
        "depth": 0
      },
      {
        "id": "n0ll6no",
        "body": "Just go to haveibeenpwned.com and look up where to find your data /s",
        "score": 1,
        "created_utc": 1751300007.0,
        "author": "ChevyRacer71",
        "is_submitter": false,
        "parent_id": "t3_1lmn3di",
        "depth": 0
      },
      {
        "id": "n13tpem",
        "body": "I explored encrypted websites, also like [https://www.protectedtext.com](https://www.protectedtext.com) or [https://www.codedpad.com](https://www.codedpad.com), but I'm not that tech savvy so I'm not sure how safe encryption is. \n\nI'm worried about my data getting leaked, yes. I'm also worried about forgetting my data. What do people usually use to manage these? I know there are free password managers like on Safari and Chrome which I genuinely appreciate, but if someone gains access to my computer, it's all gone. Also what about changing devices?\n\nThe data I'm planning to store are my personal data like personal accounts/passwords, government numbers and IDs, addresses, etc.",
        "score": 1,
        "created_utc": 1751540490.0,
        "author": "Excellent_Pause_220",
        "is_submitter": true,
        "parent_id": "t1_n0axby1",
        "depth": 1
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "1lmn35v",
    "title": "What do you guys use to keep track of all your personal information? I was thinking of an editable document I can access anywhere where I can put my TIN, SSS, investments, insurance policies, account credentials etc. Any recommendations?",
    "selftext": "",
    "url": "https://www.reddit.com/r/data/comments/1lmn35v/what_do_you_guys_use_to_keep_track_of_all_your/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1751118034.0,
    "author": "Excellent_Pause_220",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lmn35v/what_do_you_guys_use_to_keep_track_of_all_your/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "n0b28x9",
        "body": "Try r/pkms\n\nI like Tana",
        "score": 1,
        "created_utc": 1751146001.0,
        "author": "11111v11111",
        "is_submitter": false,
        "parent_id": "t3_1lmn35v",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1lm8pzn",
    "title": "A data storage server  for my small business",
    "selftext": "I want to buy a data storage server for my work stuff, but I don't know how to start.Hey everyone, I'm hoping someone can give me some advice. I'm looking to set up a data storage server for my work files, but I feel a bit lost on where to even begin. There are so many options out there, and I'm not sure which one would be best for my needs. Any guidance on choosing the right hardware or software would be greatly appreciated! Any tips would be a huge help.",
    "url": "https://www.reddit.com/r/data/comments/1lm8pzn/a_data_storage_server_for_my_small_business/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1751067895.0,
    "author": "Sufficient-Fuel4837",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lm8pzn/a_data_storage_server_for_my_small_business/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "n078ki6",
        "body": "Small setups generally benefit from a Synology or Qnap. Both good out of the box. They will serve, programs just like a web server and are more than a simple storage drive, but can be. I've enjoyed Synology but some complaints long term regarding use of proprietary drives rather than off the shelf. 1821+ is a good unit unless you need GPU for encoding video otherwise it's pretty reliable and easy to use.",
        "score": 1,
        "created_utc": 1751092291.0,
        "author": "SithLordRising",
        "is_submitter": false,
        "parent_id": "t3_1lm8pzn",
        "depth": 0
      },
      {
        "id": "n0k0zej",
        "body": "Uh how much storage are we talking here? If its less than 16GB, I'd say get you a consumer grade NAS. Synology is expensive but youre paying for the easy to use software. Which as a SB, that's what you want. Ugreen is also amazing from what ive heard, but no idea on software ease. \n\nIf you're doing more than 20TB of data and are looking for offsite backups (as you absolutely should have offsite backups for a business, period.) You honestly should look into consulting with a local MSP. They know how to configure NASes, setup file share permissions for yoir workstations, setup a VPN to the NAS for remote access, setup cloud backups, and depending on the MSP, can actually get you better rates for higher end hardware. \n\nAnd some hardware is just not accessible if youre not an MSP, like a Datto BCDR which is literally the best in the industry, objectively. But you have to be an MSP to access it. We use them for our clients and its unbelievable how good they are\n\n. And its unbelievable how royally tucked up people try to setup their own NASes and cause so many issues thst they inevitably have to call us to come fix it. And im telling you now: its going to cost more than you think to fix it. Just pay a professional to set it up for you properly the first time.",
        "score": 1,
        "created_utc": 1751280524.0,
        "author": "gojira_glix42",
        "is_submitter": false,
        "parent_id": "t3_1lm8pzn",
        "depth": 0
      },
      {
        "id": "n0arox9",
        "body": "I was thinking to use the Dxp4800 from Ugreen, do you know if its good to start?",
        "score": 1,
        "created_utc": 1751142536.0,
        "author": "Sufficient-Fuel4837",
        "is_submitter": true,
        "parent_id": "t1_n078ki6",
        "depth": 1
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1ll6mii",
    "title": "The Dashboard Doppelgänger: When GenAI Meets the Human Gaze",
    "selftext": "",
    "url": "https://moderndata101.substack.com/p/building-dashboards-with-genai",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1750959970.0,
    "author": "growth_man",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ll6mii/the_dashboard_doppelgänger_when_genai_meets_the/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ll16or",
    "title": "We will build a comprehensive collection of data quality project",
    "selftext": "We will build a comprehensive collection of data quality project: [https://github.com/MigoXLab/awesome-data-quality](https://github.com/MigoXLab/awesome-data-quality), welcome to contribute with us.",
    "url": "https://www.reddit.com/r/data/comments/1ll16or/we_will_build_a_comprehensive_collection_of_data/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1750947189.0,
    "author": "chupei0",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ll16or/we_will_build_a_comprehensive_collection_of_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ll0v7s",
    "title": "Zip Codes or Addresses by legislative district",
    "selftext": "I'm sorry if this is the wrong subreddit, but I feel like this should be way easier than it's turning out to be, and I'm struggling to find an answer.\n\nI am working on a data project that categorizes a list of addresses by their Michigan state House district and Michigan state Senate district, and I'm running into 2 challenges.\n\n1. There has to be a publicly available spreadsheet that lists all Michigan house and senate districts and the addresses within them. I can't find this data anywhere. I've made inquiries to the Census bureau and the Secretary of State, but have not received a response.\n\n2. Based on some maps I've seen, it looks like districts cut through zip codes. Am I looking for a massive data file that has every home address in Michigan along with their district? Is there some otehr way that this data is organized?\n\nI am NOT trying to create a map. There are tons of maps out there.\n\n  \nThank you in advance, and sorry again if this is not the right place.",
    "url": "https://www.reddit.com/r/data/comments/1ll0v7s/zip_codes_or_addresses_by_legislative_district/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1750946389.0,
    "author": "SlightlyTwistedGames",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ll0v7s/zip_codes_or_addresses_by_legislative_district/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1lkrr11",
    "title": "How to encrypt ssd drive with password",
    "selftext": "How to encrypt ssd drive with password",
    "url": "https://www.reddit.com/r/data/comments/1lkrr11/how_to_encrypt_ssd_drive_with_password/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1750914750.0,
    "author": "skyastrophile",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lkrr11/how_to_encrypt_ssd_drive_with_password/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mzu2c5r",
        "body": "Try VeraCrypt?",
        "score": 1,
        "created_utc": 1750916301.0,
        "author": "Darkk0910",
        "is_submitter": false,
        "parent_id": "t3_1lkrr11",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1lkcxn6",
    "title": "DataViz Challenge",
    "selftext": "County Health Rankings and Roadmaps is hosting a dataviz challenge! Submissions are due Aug 1. The only requirement is that you use some of their data (which seems to pop up on this and other subreddits regularly :))  \n[https://www.countyhealthrankings.org/findings-and-insights/blog/announcing-chrrs-2025-data-viz-challenge](https://www.countyhealthrankings.org/findings-and-insights/blog/announcing-chrrs-2025-data-viz-challenge)",
    "url": "https://www.reddit.com/r/data/comments/1lkcxn6/dataviz_challenge/",
    "score": 6,
    "upvote_ratio": 0.88,
    "num_comments": 0,
    "created_utc": 1750874806.0,
    "author": "These-Toe9031",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lkcxn6/dataviz_challenge/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1lk0nnv",
    "title": "Starting Out in Medical AI Annotation, Advice Needed",
    "selftext": "Hi\n\nI’m trying to start a small business selling medically annotated data. I have access to affordable medical students and radiology residents who I can teach to label the data, but I’m still unsure about a few things and would really appreciate your advice:\n\n1. How viable is an annotation service as a business?\n2. What should I look for in a labeled dataset?\n3. What kind of data is best to start with? I was thinking maybe public X-ray datasets like NIH or VinDr-CXR.\n4. Is there anything important I should avoid or be careful about?\n\nI’d really appreciate any honest feedback or thoughts. Thanks a lot.",
    "url": "https://www.reddit.com/r/data/comments/1lk0nnv/starting_out_in_medical_ai_annotation_advice/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 0,
    "created_utc": 1750841923.0,
    "author": "Brilliant_Fig_7120",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lk0nnv/starting_out_in_medical_ai_annotation_advice/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ljlv9q",
    "title": "Top 100 List Compiling",
    "selftext": "Hi! For a personal project, I’m trying to compile a ton of metrically ordered data of all sorts of categories. I’m looking for things like the largest lakes, highest population dense countries, baseball players with the most home runs, highest grossing movies of all time, etc. While I could individually go and search for thing I can think of, I was want to find categories that don’t come to mind. I’ve tried to mess around with data scraping Wikipedia but the data is gathered inconsistently. Any suggestions for websites or methods I could use to gather a ton of these lists? Any suggestions are helpful!",
    "url": "https://www.reddit.com/r/data/comments/1ljlv9q/top_100_list_compiling/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1750796459.0,
    "author": "NowYouShallSee",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ljlv9q/top_100_list_compiling/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mzmk4eu",
        "body": "Exa\n\nCan create custom data sets via the API",
        "score": 1,
        "created_utc": 1750817525.0,
        "author": "MatricesRL",
        "is_submitter": false,
        "parent_id": "t3_1ljlv9q",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1ljml87",
    "title": "Depositors from investments companies for sale (2024 / 2025)",
    "selftext": "All the info including investment amount and company name, TG: @Dani_walltee\n",
    "url": "https://www.reddit.com/r/data/comments/1ljml87/depositors_from_investments_companies_for_sale/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1750798156.0,
    "author": "WoodpeckerDapper6408",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ljml87/depositors_from_investments_companies_for_sale/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ljcggw",
    "title": "data scientist",
    "selftext": "hi all,\n\ni am a  data scientist with 5+ years of experience and have worked in nbfc, pharmaceutical and supply chain domain. please do let me know if any vacancies available ",
    "url": "https://www.reddit.com/r/data/comments/1ljcggw/data_scientist/",
    "score": 3,
    "upvote_ratio": 0.8,
    "num_comments": 0,
    "created_utc": 1750775035.0,
    "author": "gorbong",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ljcggw/data_scientist/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ljdh2g",
    "title": "Feedback wanted: Pricing for 110M product database with UPC/pricing data",
    "selftext": "I've spent months building a comprehensive database with 110M products, UPC codes, and multi-store pricing. Originally for my own ecommerce business, but getting requests from others.\n\nWhat would you consider fair pricing for this type of dataset? Any thoughts on licensing vs one-time sale?",
    "url": "https://www.reddit.com/r/data/comments/1ljdh2g/feedback_wanted_pricing_for_110m_product_database/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1750777417.0,
    "author": "GlitteringEchidna425",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ljdh2g/feedback_wanted_pricing_for_110m_product_database/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1lj5bmp",
    "title": "I've created a newsletter on Data Governance to share tips",
    "selftext": "As it might help, here is the link : [https://charlotteledoux.substack.com/](https://charlotteledoux.substack.com/)\n\nI post 2 times a month about : \n\n* **Core Concepts** : Understand the core principles of Data & AI Governance\n* **Strategy & Organization** : Define your vision, strategy, roles & responsibilities\n* **Operationalisation** : Explore concrete actions to bring value and scale\n* **Case studies** : Get insights into the latest tools that can help in data governance\n* **Thought leadership & trends** : Explore perspectives shaping the future of Data & AI Governance\n* **My resources** : Find my secret resources to go faster\n\nTell me if you have ideas of topics !!",
    "url": "https://www.reddit.com/r/data/comments/1lj5bmp/ive_created_a_newsletter_on_data_governance_to/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 5,
    "created_utc": 1750752293.0,
    "author": "Charlotte1309",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lj5bmp/ive_created_a_newsletter_on_data_governance_to/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mzk8ntw",
        "body": "I think you can also explore **AI governance** as a topic. It's the next step for data leaders as regulations like the EU AI Act / ISO 42001 kick in and companies have difficulties aligning.",
        "score": 2,
        "created_utc": 1750791127.0,
        "author": "gorkemcetin",
        "is_submitter": false,
        "parent_id": "t3_1lj5bmp",
        "depth": 0
      },
      {
        "id": "mznq4bo",
        "body": "Yes definitely, I've written one article about it so far, but i feel like it's too much in the future for a lot of companies right now...",
        "score": 1,
        "created_utc": 1750836349.0,
        "author": "Charlotte1309",
        "is_submitter": true,
        "parent_id": "t1_mzk8ntw",
        "depth": 1
      },
      {
        "id": "mzokhct",
        "body": "That’s true. For iso42001 management system it’s taking off and demand is there (I know since I own an open source AI governance platform). Maybe focus on that entirely?",
        "score": 1,
        "created_utc": 1750852571.0,
        "author": "gorkemcetin",
        "is_submitter": false,
        "parent_id": "t1_mznq4bo",
        "depth": 2
      },
      {
        "id": "mzor3rq",
        "body": "Good idea! send me the link to your platform also",
        "score": 1,
        "created_utc": 1750855135.0,
        "author": "Charlotte1309",
        "is_submitter": true,
        "parent_id": "t1_mzokhct",
        "depth": 3
      },
      {
        "id": "mzork4x",
        "body": "Sure, it’s https://verifywise.ai",
        "score": 1,
        "created_utc": 1750855301.0,
        "author": "gorkemcetin",
        "is_submitter": false,
        "parent_id": "t1_mzor3rq",
        "depth": 4
      }
    ],
    "comments_extracted": 5
  },
  {
    "id": "1lj76zi",
    "title": "[OC] I turned my economics research into a space mission control center with real-time financial data streams",
    "selftext": "**TL;DR:** Got tired of boring academic portfolios, so I built EconStellar - a cosmic research station that makes economic data analysis feel like piloting a spaceship.\n\n**The Problem:** Academic research dies in PDFs. Complex econometric models that could inform real policy decisions get buried in university websites where nobody finds them.\n\n **The Solution:** EconStellar treats economic research like an active space mission, complete with:\n\n  🚀 **Mission Control Center - Real-time dashboard managing all research projects**\n\n  📊 Live Data Streams - OpenBB financial API integration showing market conditions🌌 Network Visualizations - Financial contagion spreading like cosmic phenomena\n\n  ⚡ Transfer Entropy Models - Policy impact analysis with sci-fi aesthetics\n\n  🎸 Parallel Universe Portal - Because sometimes guitar theory parallels economic modeling\n\n  **The Data Visualization:**\n\n  \\- Real-time cryptocurrency contagion tracking using wavelet analysis\n\n  \\- Environmental policy network effects visualized as interconnected galactic systems\n\n  \\- Financial crisis propagation models displayed like space radar\n\n  \\- Market volatility streams flowing like cosmic particle effects\n\n  **Cool Technical Features:**\n\n  \\- Auto-popup cosmic events that surface relevant research based on current market conditions\n\n  \\- Terminal-style logging that makes data analysis feel like mission control\n\n  \\- Network topology visualization with floating nodes and connection lines\n\n  \\- Responsive design that works on mobile (yes, you can run mission control on your phone)\n\n  **The Tech Stack:**\n\n  \\- CSS animations with hardware acceleration for space effects\n\n  \\- R Shiny dashboards embedded as live mission data\n\n  \\- OpenBB API for real-time financial feeds\n\n  \\- Custom visualization algorithms for network analysis\n\n  **Research Projects as \"Active Missions\":**\n\n  \\- WaveQTE: [https://avishekb9.shinyapps.io/waveqte-dashboard/](https://avishekb9.shinyapps.io/waveqte-dashboard/) \\- Wavelet-based financial contagion analysis\n\n  \\- ManyIVsNets: [https://avishekb9.github.io/ManyIVsNets/index.html](https://avishekb9.github.io/ManyIVsNets/index.html) \\- Environmental economics network analysis\n\n  \\- didTEnets: [https://avishekb9.github.io/didTEnets/](https://avishekb9.github.io/didTEnets/) \\- Transfer entropy for policy evaluation\n\nWhy This Approach Works: Visitors now spend 10x longer exploring the research. Complex econometric models suddenly make sense when presented as \"cosmic data streams\" rather than academic jargon.\n\n   **Live Demo:** [avishekb9.github.io/econstellar](http://avishekb9.github.io/econstellar)\n\n  **Data Sources:**\n\n  \\- OpenBB Terminal API for real-time financial data\n\n  \\- Custom network datasets for policy analysis\n\n  \\- Cryptocurrency market feeds for contagion modeling\n\nSometimes the best way to make serious research accessible is to stop taking the presentation so seriously.\n\n**For fellow researchers:** Your data deserves better than boring static charts. The universe of economic research is vast - time to explore it differently.\n\nWould love feedback from the r/dataisbeautiful community - what other research areas could benefit from the \"space mission\" treatment?\n\nTools used: JavaScript, CSS3, R, Shiny, OpenBB API, lots of coffee, and Rock n Roll!\n\n**LinkedIN:** [https://www.linkedin.com/in/avishek-bhandari-100b77119/](https://www.linkedin.com/in/avishek-bhandari-100b77119/)\n\n\n\n\n\n",
    "url": "https://www.reddit.com/r/data/comments/1lj76zi/oc_i_turned_my_economics_research_into_a_space/",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 0,
    "created_utc": 1750759616.0,
    "author": "Important-Mirror1913",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lj76zi/oc_i_turned_my_economics_research_into_a_space/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1lis6db",
    "title": "Feature-Engineered Mouse Dynamics Dataset For Anomaly Detection",
    "selftext": "**Mouse Dynamics Feature-Engineered Dataset (157K rows, 38 features)**\n\nAfter going through heaps of poorly structured behavioral datasets online, I came across a high-potential raw dataset released by Boğaziçi University. It contains timestamped x and y mouse coordinates recorded during user sessions and is organized into folders of legitimate users and external (anomalous) users.\n\nTo make the dataset usable for real-world modeling tasks, I processed and feature-engineered it into a clean, structured format with 38 features and 157,351 rows (\\~90MB CSV). The result is a session-based behavioral dataset that can be immediately usable in anomaly detection pipelines.\n\n**Feature Groups:**\n\n**Session-level metrics:**  \nsession\\_duration, total\\_distance, num\\_actions, num\\_clicks, num\\_strokes, mean\\_time\\_per\\_action, avg\\_drag\\_time\n\n**Velocity stats:**  \nvel\\_mean, vel\\_std, vel\\_max, vel\\_min, vel\\_median, vel\\_q25, vel\\_q75\n\n**Acceleration stats:**  \naccel\\_mean, accel\\_std, accel\\_max, accel\\_min, accel\\_median, accel\\_q25, accel\\_q75\n\n**Jerk stats:**  \njerk\\_mean, jerk\\_std, jerk\\_max, jerk\\_min, jerk\\_median, jerk\\_q25, jerk\\_q75\n\n**Curvature stats:**  \ncurve\\_mean, curve\\_std, curve\\_max, curve\\_min, curve\\_median, curve\\_q25, curve\\_q75\n\n**Metadata:**  \nsession\\_name, serial\\_no., risk (binary classification: 0 = normal, 1 = anomaly)\n\n**Use Cases:**  \nThis dataset is highly suitable for insider threat detection, remote unauthorized access detection, continuous authentication, user behavior profiling, and time-series anomaly classification experiments.\n\nThose who are interested in ML and DL modes on Anomaly Detection, check it out!  \n[https://figshare.com/articles/dataset/feature\\_engineered\\_mouse\\_data\\_csv/29386898/2?file=55588529](https://figshare.com/articles/dataset/feature_engineered_mouse_data_csv/29386898/2?file=55588529)",
    "url": "https://www.reddit.com/r/data/comments/1lis6db/featureengineered_mouse_dynamics_dataset_for/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1750712112.0,
    "author": "Flash_00007",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lis6db/featureengineered_mouse_dynamics_dataset_for/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1lid9g2",
    "title": "Data set for gambling (non- tournament poker)",
    "selftext": "Hey all,\nI'm building an ML project to detect addiction levels in poker/gambling players but can't find a suitable dataset on Kaggle or elsewhere. I've tried creating one but need help designing a custom dataset for 50 players over 30 days.\n\nProject Details:\nDataset Structure: Two tables:\nplayers_profiledata: Summarized player data (50 rows).\nplayers_activitydata: Transaction-level \n\nWhat I Need:\nSuggested columns for both tables, with relevance to addiction detection.\nIdeas to ensure column correlations for ML.also tell any tips for generating/structuring the dataset (e.g., tools, synthetic data).\n\nAny advice or ideas would be greatly appreciated!\n Thanks in advance.\n",
    "url": "https://i.redd.it/zflhblxrnn8f1.png",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1750675060.0,
    "author": "Addy_002",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lid9g2/data_set_for_gambling_non_tournament_poker/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1lhzz8l",
    "title": "No data",
    "selftext": "Has anyone encountered any ML project where no data exists? Where your boss wants to detects many scenarios in the detection module of ML, but there is no base data. How did you handle this situation? ",
    "url": "https://www.reddit.com/r/data/comments/1lhzz8l/no_data/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1750629833.0,
    "author": "Dismal-Opinion315",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lhzz8l/no_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mzjwf20",
        "body": "Have you ever questioned your boss?",
        "score": 1,
        "created_utc": 1750787687.0,
        "author": "Important-Mirror1913",
        "is_submitter": false,
        "parent_id": "t3_1lhzz8l",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1lhmjfr",
    "title": "Help me choose a topic for my Master's thesis (Data Analysis)",
    "selftext": "I'm currently pursuing a Master's and I'm in the process of choosing a topic for my thesis. I'm very interested in data analysis and machine learning, and I've come up with a few ideas so far:\n\n1.Housing price predictions – using regression models\n\n2.Bitcoin price prediction – using time series forecasting\n\n3.Credit risk analysis – identifying high-risk customers using classification models\n\n4.Customer segmentation – using clustering techniques (e.g. K-means, DBSCAN)\n\nI’d really appreciate your input! Do any of these topics sound interesting or promising from your experience? Also, if you have any other suggestions that could be exciting, especially with real-world applications, feel free to share.\n\nThanks in advance! 🙏\n",
    "url": "https://www.reddit.com/r/data/comments/1lhmjfr/help_me_choose_a_topic_for_my_masters_thesis_data/",
    "score": 5,
    "upvote_ratio": 1.0,
    "num_comments": 9,
    "created_utc": 1750594726.0,
    "author": "Impressive_Wasabi_25",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lhmjfr/help_me_choose_a_topic_for_my_masters_thesis_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mzcsejj",
        "body": "without knowing your program's specifics (and with no intent of being \"that\" asshole), at a masters level you're usually expected to go a bit more \"in-depth\" that just applying an out-of-the-box model to a textbook problem. for instance, I did k-means customer segmentation for a 2nd semester course, and it was a 2-week final project.\n\ni'd recommend exploring novel/niche approaches (in terms of like heuristics, feature engineering, ensemble models, etc) to the problems you've come up with.\n\nfor example:\n\n1. Housing Price Prediction – Integrating Geospatial Data, Economic Indicators, and Machine Learning\n2. Bitcoin Price Forecasting – Combining Deep Learning (LSTMs, Transformers) with Sentiment Analysis from News and Social Media\n3. Explainable AI for Credit Risk Assessment – Hybrid Models with Feature Importance and Fairness Constraints\n\nthose came out of ChatGPT and are not meant to be literal thesis recommendations. they're just to get you thinking about the complexity or depth that is usually expected in a master's thesis",
        "score": 2,
        "created_utc": 1750696512.0,
        "author": "Rodrack",
        "is_submitter": false,
        "parent_id": "t3_1lhmjfr",
        "depth": 0
      },
      {
        "id": "mzciikj",
        "body": "3 or 4! I think clustering methods are fascinating",
        "score": 1,
        "created_utc": 1750693716.0,
        "author": "SingerEast1469",
        "is_submitter": false,
        "parent_id": "t3_1lhmjfr",
        "depth": 0
      },
      {
        "id": "mzdtncs",
        "body": "Second this, OP. There’s a huge need right now for work on explainable AI, and credit risk would be an excellent place to dig in. \n\nStart with a literature review. Your university probably has access to a lot of research papers on their library’s website - start reading recent papers on that’s already been done. You might even start with focusing on meta-analysis papers that will get you up to speed quicker at a high level. \n\nMost research papers will have something in the discussion or conclusion about ways that their research could have been improved, or where certain aspects were lacking. Find those places and try to fill in some gaps. \n\nAs a master’s student, you really should be aspiring to add something to the academic world, not just following along. ",
        "score": 3,
        "created_utc": 1750706899.0,
        "author": "RedditorFor1OYears",
        "is_submitter": false,
        "parent_id": "t1_mzcsejj",
        "depth": 1
      },
      {
        "id": "mzhbahs",
        "body": "Thanks",
        "score": 1,
        "created_utc": 1750755248.0,
        "author": "Impressive_Wasabi_25",
        "is_submitter": true,
        "parent_id": "t1_mzciikj",
        "depth": 1
      },
      {
        "id": "mzhb9na",
        "body": "I work at a fintech company, credit management sector but they won't give me real data they suggested synthetic data. I don't know if this is a good idea. \nAlso to add that I don't have a computer science or statistics degree, so doing something advanced in data analysis will be difficult.",
        "score": 1,
        "created_utc": 1750755233.0,
        "author": "Impressive_Wasabi_25",
        "is_submitter": true,
        "parent_id": "t1_mzdtncs",
        "depth": 2
      },
      {
        "id": "mzi5x65",
        "body": "What field is your masters program in? I would have assumed it’s something related to analytics if these are the kinds of projects you want to do. ",
        "score": 1,
        "created_utc": 1750769550.0,
        "author": "RedditorFor1OYears",
        "is_submitter": false,
        "parent_id": "t1_mzhb9na",
        "depth": 3
      },
      {
        "id": "mzi9p0l",
        "body": "Management",
        "score": 1,
        "created_utc": 1750770828.0,
        "author": "Impressive_Wasabi_25",
        "is_submitter": true,
        "parent_id": "t1_mzi5x65",
        "depth": 4
      },
      {
        "id": "mzii8fa",
        "body": "Ah. In that case I’d advise against anything having to do with decisions directly impacting consumers, as there’s quite a lot of ethical considerations that likely fall under what you consider “something advanced”. See article below for reference. \n\nhttps://rfkhumanrights.org/our-voices/bias-in-code-algorithm-discrimination-in-financial-systems/\n\nI still think the credit risk idea is a good topic to explore BECAUSE of those ethical considerations though, if you’re ok putting in the effort. The problem is that you can often do more harm than good if you try to use ML models you don’t fully understand. \n\nIf you really are interested in doing data analysis like this, I would advise you to not be so quick to write something off as “too advanced”. You don’t need a statistics degree to understand this stuff, just a little extra reading and research. And for what it’s worth, a literature review will be pretty essential regardless of the topic. Your advisors are very likely going to want to see that you understand what has and hasn’t been done in any subject you end up doing your thesis on. ",
        "score": 1,
        "created_utc": 1750773583.0,
        "author": "RedditorFor1OYears",
        "is_submitter": false,
        "parent_id": "t1_mzi9p0l",
        "depth": 5
      },
      {
        "id": "mzog8te",
        "body": "I have a basic idea of ML models — we've also covered some of them in class, but only in very simple applications. Thanks a lot for your response",
        "score": 1,
        "created_utc": 1750850777.0,
        "author": "Impressive_Wasabi_25",
        "is_submitter": true,
        "parent_id": "t1_mzii8fa",
        "depth": 6
      }
    ],
    "comments_extracted": 9
  },
  {
    "id": "1lhwuww",
    "title": "I have 1.8 M recent Upwork job posts—what would you build with them?",
    "selftext": "I run a little Saas that sends AI job alerts for Upwork and, along the way, grabbed the latest 1.8 million public job posts (descriptions, budgets, skills, client spend, timestamps). I’m hunting for cool ways to turn this trove into something useful—or profitable. Got an idea or want to team up? Comment or DM me and let’s talk.",
    "url": "https://www.reddit.com/r/data/comments/1lhwuww/i_have_18_m_recent_upwork_job_postswhat_would_you/",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 3,
    "created_utc": 1750621781.0,
    "author": "the_flip_flop",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lhwuww/i_have_18_m_recent_upwork_job_postswhat_would_you/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mzbuo9y",
        "body": "Build a service giving freelancers market insights like skill demand or budget ranges. You could build dashboards, send reports via email or build a simple API.",
        "score": 2,
        "created_utc": 1750686727.0,
        "author": "garymlin",
        "is_submitter": false,
        "parent_id": "t3_1lhwuww",
        "depth": 0
      },
      {
        "id": "mz7gkbx",
        "body": "Super handy for validating product ideas and gauging real market demand. I already have an AI scoring model that ranks each listing by relevance and value, and my first instinct is to package the trends + scores into a slick dashboard and pitch it to VCs or accelerator programs hunting for signals. Thoughts?",
        "score": 1,
        "created_utc": 1750621927.0,
        "author": "the_flip_flop",
        "is_submitter": true,
        "parent_id": "t3_1lhwuww",
        "depth": 0
      },
      {
        "id": "mznzs6w",
        "body": "Would this identify jobs for e-commerce related projects? I used Upwork years ago to find clients, turned away from it when they went crazy with the credits",
        "score": 1,
        "created_utc": 1750842128.0,
        "author": "bigbossontop",
        "is_submitter": false,
        "parent_id": "t3_1lhwuww",
        "depth": 0
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1lhsm20",
    "title": "Is UHasselt a good choice for an MSc in Data Science and Statistics, and how strong should your computer science background be to succeed in the program?",
    "selftext": "# Hi!\n\nAre there UHasselt students or graduates in this community by any chance? I'd need your advice, please. \n\nI  want to go for the Data Science and Statistics on-site MSc at UHasselt this year, but I come from a non-Comp Sc background. My main goal is to build a solid foundation, particularly in Python and mathematics to further develop these skills and gradually pivot into Data Science/Engineering in several years upon graduation.\n\nI genuinely love the program curriculum and feel excited about the subjects. However, I’m concerned that my academic background might not be technical or computational enough.\n\nWould you say that the program is mainly aimed at students with a strong computer science background, or is there room to catch up and succeed and what are the career perspectives upon graduation ?\n\nThanks!",
    "url": "https://www.reddit.com/r/data/comments/1lhsm20/is_uhasselt_a_good_choice_for_an_msc_in_data/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1750611226.0,
    "author": "Severe_Mark_8333",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lhsm20/is_uhasselt_a_good_choice_for_an_msc_in_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "n130u9z",
        "body": "did you recieved admission?",
        "score": 1,
        "created_utc": 1751524331.0,
        "author": "Low_Lettuce_4893",
        "is_submitter": false,
        "parent_id": "t3_1lhsm20",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1lg8z9k",
    "title": "The data footprints of China’s transnational repression",
    "selftext": "",
    "url": "https://www.icij.org/investigations/china-targets/inside-china-targets-the-data-footprints-of-chinas-transnational-repression/?utm_campaign=news&utm_medium=social&utm_source=reddit",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 0,
    "created_utc": 1750438444.0,
    "author": "ICIJ",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lg8z9k/the_data_footprints_of_chinas_transnational/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1lfu4bc",
    "title": "My experience using ChatGPT and Google",
    "selftext": "Based on my experience using ChatGPT and Google to search for information:\n\nChatGPT responds faster.\nBut Google provides more in-depth information on each topic — written by people who truly understand it.\nChatGPT tries to summarize and explain things in a conversational way.\nOverall, if you want information with certainty, like reading a well-researched book, use Google.\nBut if you want to learn through conversation — where there might be mistakes, but you can keep asking until you understand — talk to ChatGPT.\nI recommend that younger students use each tool appropriately. In the past, people said searching on Google made it easier to forget things. But that doesn't really matter anymore. What matters most now is understanding the information and being able to apply it effectively.",
    "url": "https://www.reddit.com/r/data/comments/1lfu4bc/my_experience_using_chatgpt_and_google/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1750390588.0,
    "author": "Enough-Sport9697",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lfu4bc/my_experience_using_chatgpt_and_google/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "myuzv4j",
        "body": "I've found that LLMs have two very good use cases:\n\n1) You know absolutely nothing and don't even know where to start. You can keep asking questions while you try to piece together the basics. ChatGPT tends to be very good as covering bases.\n\n2) You're already an expert but you just want a second opinion or to save time. It's going to make some mistakes when doing anything \"new\", so you need to know enough to be able to correct it, but even that process can take less time than doing it yourself. You are still the one that provides the creativity and makes the final decisions.\n\nAnywhere in between, and you're either robbing yourself of important educational experiences or allowing yourself to be fooled in asking the tool to do something it's not capable of doing.",
        "score": 1,
        "created_utc": 1750445847.0,
        "author": "Kid_Radd",
        "is_submitter": false,
        "parent_id": "t3_1lfu4bc",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1lfk0hm",
    "title": "Need help understanding the below job description",
    "selftext": "Hi can someone please help me understand what all would the below job description have as day to day activities. What tools would I need to be knowing and to what detail or extent should I be learning them.\n\n“This team will help design the data onboarding process, infrastructure, and best practices, leveraging data and technology to develop innovative solutions to ensure the highest data quality. The centralized databases the individual builds will power nearly all core Research product.\n\nPrimary responsibilities include:\n\nCoordinate with Stakeholders / Define requirements:\n\nCoordinate with key stakeholders within Research, technology teams and third-party data vendors to understand and document data requirements.\nDesign recommended solutions for onboarding and accessing datasets.\nConvert data requirements into detailed specifications that can be used by development team.\nData Analysis:\n\nEvaluate potential data sources for content availability and quality.  Coordinate with internal teams and third-party contacts to setup, register, and enable access to new datasets (ftp, SnowFlake, S3, APIs)\nApply domain knowledge and critical thinking skills with data analysis techniques to facilitate root cause analysis for data exceptions and incidents. \nProject Administration / Project Management:\n\nBreakdown project work items, track progress and maintain timelines for key data onboarding activities.\nDocument key data flows, business processes and dataset metadata.\nQualifications\n\nAt least 3 years of relevant experience in financial services\nTechnical Requirements:\n1+ years of experience with data analysis in Python and/or SQL\nAdvanced Excel\nOptional: q/KDB+\nProject Management experience recommended; strong organizational skills\nExperience with project management software recommended; JIRA preferred\nData analysis experience including profiling data to identify anomalies and patterns\nExposure to financial data, including fundamental data (e.g. financial statement data / estimates), market data, economic data and alternative data\nStrong analytical, reasoning and critical thinking skills; able to decompose complex problems and projects into manageable pieces, and comfortable suggesting and presenting solutions\nExcellent verbal and written communication skills presenting results to both technical and non-technical audiences”",
    "url": "https://www.reddit.com/r/data/comments/1lfk0hm/need_help_understanding_the_below_job_description/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1750362046.0,
    "author": "krishchawla16",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lfk0hm/need_help_understanding_the_below_job_description/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1lf7i3c",
    "title": "Would you find an RSS feed of data related links useful?",
    "selftext": "Hey everyone.\n\nI've been sorting out and merging various sources of blogs, newsletters etc into one list of links and summaries for myself to make it more manageable to keep up with news, stories etc.\n\nWondering if any of you would find it useful if I made a public RSS Feed of the most interesting articles?\n\nIt would not be a new RSS item for every link as that would be way too much - but maybe a few RSS feed posts a week, each post could then contain a curated list links to relevant sources etc (maybe add a short summary of each link too)\n\nCould do a newsletter too but right now I'm just thinking about an RSS feed - anyways just curious if it would be of any use to anyone and if it would be worth looking into further.\n\nThanks!\n\n**PS:** **Anything specific you would like covered, let me know in the comments :)** ... it's meant to be a digest so thinking of just focusing on specific keywords - 'databases' 'analysis' 'information' 'mysql' and so on.",
    "url": "https://www.reddit.com/r/data/comments/1lf7i3c/would_you_find_an_rss_feed_of_data_related_links/",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1750329251.0,
    "author": "eena00",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lf7i3c/would_you_find_an_rss_feed_of_data_related_links/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mzcirbd",
        "body": "Yes",
        "score": 1,
        "created_utc": 1750693784.0,
        "author": "SingerEast1469",
        "is_submitter": false,
        "parent_id": "t3_1lf7i3c",
        "depth": 0
      },
      {
        "id": "mzhhnrl",
        "body": "Thanks, still thinking about it - but it's very tempting to set it up :)",
        "score": 1,
        "created_utc": 1750758973.0,
        "author": "eena00",
        "is_submitter": true,
        "parent_id": "t1_mzcirbd",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1lf75o2",
    "title": "AirBnB Chrome Extension to #1 build your own DB of detailed listing data, and #2 get pricing & occupancy stats from the source itself (replacing external-products like AirDNA, Rabbu, etc.)",
    "selftext": "Hoard your area's Airbnb data with this Chrome extension, directly on Airbnb itself.\n\nI made this and think it provides a lot of value to the right people, hopefully this is allowed here since it's all about data?\n\nIt's a lot different than every external-provider of Pricing & Occupancy data (like AirDNA or Rabbu, etc), and you can export all the data/listings you want without limit. Would love to hear your thoughts",
    "url": "https://v.redd.it/yf6hbz9yxu7f1",
    "score": 2,
    "upvote_ratio": 0.75,
    "num_comments": 0,
    "created_utc": 1750327925.0,
    "author": "DRONE_SIC",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lf75o2/airbnb_chrome_extension_to_1_build_your_own_db_of/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1lf65lx",
    "title": "Bachelor of Science : Computer Science or Data Science?",
    "selftext": "Hello! I am about to start a tech degree soon, just a bit confused as to which degree I should choose! \nFor context, I am interested in few different fields including data science, cyber security, software engineering, computer science, etc.\nI have 3 options to choose from in Curtin uni :\n1. Bachelor of Science in data science and if 80-100%, then advanced science honours as well.\n2.. Bachelor of IT and score 75-80% in first semester or year to transfer to bachelor of computing (either software engineering/cyber security or computer science major)\n3. Bachelor of IT and score 80 to 100% to transfer to Bachelor of Advanced Science in computing\n\nMy main interests include Cybersecurity or Data Science. Which degree would you suggest for this? Some people say data science others say that computer science will provide more options if I want to change career, I am so confused, please help!🙏🏻",
    "url": "https://www.reddit.com/r/data/comments/1lf65lx/bachelor_of_science_computer_science_or_data/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1750323902.0,
    "author": "Billionfairyyass1539",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lf65lx/bachelor_of_science_computer_science_or_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "myq55sq",
        "body": "Take this with a grain of salt but I would do a CS degree. The low-level fundamentals will not change in the next 5 years and always have value, but I have no idea whether the higher-level knowledge gained in a data science degree will be as useful. It might just be abstracted away with automation. The tech industry seems intent on eschewing optimization in favor of burning resources with brute-force algorithms. At some point they'll realize they need to optimize. That's where CS experts come in.",
        "score": 2,
        "created_utc": 1750378504.0,
        "author": "KevDub81",
        "is_submitter": false,
        "parent_id": "t3_1lf65lx",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1leweos",
    "title": "Does anyone have or know where to find historic cs2 betting odds?",
    "selftext": "I am working on building a cs2 esports betting model and this data is crucial! If anyone has this dataset or knows where I can find it, that would be super helpful. I am looking for specifically a site, as I am proficient in scraping data.",
    "url": "https://www.reddit.com/r/data/comments/1leweos/does_anyone_have_or_know_where_to_find_historic/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1750290721.0,
    "author": "Professional_Leg_951",
    "subreddit": "data",
    "permalink": "/r/data/comments/1leweos/does_anyone_have_or_know_where_to_find_historic/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1lf00g9",
    "title": "【New release v1.7.1】Dingo: A Comprehensive Data Quality Evaluation Tool",
    "selftext": "[https://github.com/DataEval/dingo](https://github.com/DataEval/dingo)\n\nwelcome give us a star 🌟🌟🌟",
    "url": "https://www.reddit.com/r/data/comments/1lf00g9/new_release_v171dingo_a_comprehensive_data/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1750301416.0,
    "author": "chupei0",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lf00g9/new_release_v171dingo_a_comprehensive_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1lexii1",
    "title": "Looking for career advise",
    "selftext": "I have almost 10 yearas of data analytics experience. A year ago I left a senior-level job for a role which also advertised as senior. The pay was much higher and I was led to believe during the interviews that I would have considerable ownership.\n\nThe reality has been very different. I am never invited to important meetings, so I have a very narrow view of the projects. Instead, my lead assigns me granular \"tasks\" which honestly are mostly grunt work (cleaning some data, validating a table or at best, building a SQL view). I have tried the \"proactive\" approach of asking questions about the broader-scope and proposing solutions and even though his replies are sympathetic, his actions don't match his words. Sometimes I feel like he treats me like a child pretending to drive—letting me sit in his lap and steer, but never actually giving me control. Even when he ends up being proven wrong on something I warned him about (which happens quite often) he doesn't acknowledge it, just goes on as if it didn't happen.\n\nWhy they pay me what they pay me for work that a new grad could do is beyond me. I'm increasingly afraid that management will eventually wake up to this fact and let me go. What's worse: that'll force me back into the job market with a \"senior\" title and very little to show for it in terms of recent experience. I've been preemptively applying to other jobs, but as everyone on this sub knows, the market is unforgiving right now.\n\nAny advise? Should I escalate this with my manager (who's not the same as my lead) and risk potential conflict if he doesn't handle it correctly? Should I even be drawing that kind of attention to myself given the current climate? Or should I focus on building stuff/skills in parallel so that I can I see my way through a future interview?\n\n",
    "url": "https://www.reddit.com/r/data/comments/1lexii1/looking_for_career_advise/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1750293928.0,
    "author": "tgbelmondo",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lexii1/looking_for_career_advise/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1leguip",
    "title": "Do any other UK data heads use Gov Transport Vehicle stats?",
    "selftext": "Gov Transport have a suite of vehicle data registration stats which have historically proven useful in my line of work.\n\nWhile the site remains active, they have not updated their sets in over 9 months now. \n\nDoes anyone have any contact or insight to this department, have their resources just been slashed?\n\nDoes anyone know of a good alternative? SMMT data is ok, but expensive, and only looks at registrations, making a true net position hard to gauge",
    "url": "https://www.reddit.com/r/data/comments/1leguip/do_any_other_uk_data_heads_use_gov_transport/",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 2,
    "created_utc": 1750252591.0,
    "author": "AffectedWomble",
    "subreddit": "data",
    "permalink": "/r/data/comments/1leguip/do_any_other_uk_data_heads_use_gov_transport/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mysigod",
        "body": "Maybe they update annually? Can you see the release history?",
        "score": 2,
        "created_utc": 1750417623.0,
        "author": "ItsSignalsJerry_",
        "is_submitter": false,
        "parent_id": "t3_1leguip",
        "depth": 0
      },
      {
        "id": "mysjpih",
        "body": "You can, yes. At one point (approx 2022) it was quarterly, then they changed to 6 monthly updates (I can't recall if this was explicitly said or just clear from the actual releases)\n\nI may have summoned them into action by making this post: last update was September 2024, they have just posted what looks to be the rest of 2024 this week!\n\nSo a quasi-result, good to have more data but no more aware of when future updates might be, as even if this is meant as a bi-annual update, it's doing the prior-prior 6 months",
        "score": 1,
        "created_utc": 1750418198.0,
        "author": "AffectedWomble",
        "is_submitter": true,
        "parent_id": "t1_mysigod",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1ldwhhw",
    "title": "How to import numbers and track how often they appear?",
    "selftext": "I have no idea what I'm doing, and only have access to basic Excel and basic programs.\n\nI have a list of 100, 8 digit numbers. There's a new list once a week.\nI want to imput those numbers and see how often the same numbers come up. What's the simplest way to do that?\n\n\nBackstory: There's a small business in my town that picks 100 \"member numbers\" at random once a week, and those people get a prize. I live in a town with about 4,000 people, and in the 5 years I've been a member, I've never won. I understand it's random... but that's 20,000+ numbers... I know I've seen a few numbers come up twice a month. I'm trying to do my own little investigation to see if the same people keep winning prizes, or if I'm nuts. ",
    "url": "https://www.reddit.com/r/data/comments/1ldwhhw/how_to_import_numbers_and_track_how_often_they/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 5,
    "created_utc": 1750188628.0,
    "author": "StoryIsInTheSoil",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ldwhhw/how_to_import_numbers_and_track_how_often_they/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mycngg6",
        "body": "Are you trying to see within the same list of 100, or cumulatively, such that after 3 weeks the list is 300 numbers?\n\nEither way, pretty easy in excel, add the numbers to a column (appending as necessary) then do a =COUNTIF(). So the numbers in column A and the formula in column B \"=COUNTIF(A:A,A2)\" will tell you how many times the value in A2 appears in the column.",
        "score": 2,
        "created_utc": 1750201076.0,
        "author": "jkovach89",
        "is_submitter": false,
        "parent_id": "t3_1ldwhhw",
        "depth": 0
      },
      {
        "id": "mybw5jc",
        "body": "That's actually a really cool job for an intern!\n\nSeriously though\n\n> 8 digit number\n>\n> [...]\n>\n> I know I've seen a few numbers come up twice a month\n\nThe chances of an 8 digits number being picked more than once in the same month are absurdly low. To the point it's probably not really random, if that really happened several times in the 5 years you've been a member.",
        "score": 1,
        "created_utc": 1750192718.0,
        "author": "captain_obvious_here",
        "is_submitter": false,
        "parent_id": "t3_1ldwhhw",
        "depth": 0
      },
      {
        "id": "mydytut",
        "body": "Every week I would add 100 new numbers. I'll give this a try!",
        "score": 1,
        "created_utc": 1750217441.0,
        "author": "StoryIsInTheSoil",
        "is_submitter": true,
        "parent_id": "t1_mycngg6",
        "depth": 1
      },
      {
        "id": "myd7qif",
        "body": "It sounds like there are 4000 max people to choose from, rarher than 100 million.\n\nEach draw, you have 3999 out of 4000 chance of losing, which is 0.99975\n\nIn one year they have 52 drawings, so (0.99975)^52 = 0.9870825 chance of not getting picked.\n\nTake that to 5 years (260 drawings), and it is 93.7 % chance of not getting picked.",
        "score": 1,
        "created_utc": 1750207941.0,
        "author": "scottdave",
        "is_submitter": false,
        "parent_id": "t1_mybw5jc",
        "depth": 1
      },
      {
        "id": "mydzo6x",
        "body": "I don't think it's as random as they say it is. I'm just trying to see if I'm correct or imagining it. :) \n\n\nI get that there's probably a lot more numbers than just the 4000 in my town (people moving, a lot of the tiny towns surrounding, etc), it just seems so weird that I haven't won. Especially when the number that's next in line has won 2 times so far this year.",
        "score": 1,
        "created_utc": 1750217781.0,
        "author": "StoryIsInTheSoil",
        "is_submitter": true,
        "parent_id": "t1_mybw5jc",
        "depth": 1
      }
    ],
    "comments_extracted": 5
  },
  {
    "id": "1ldnvl0",
    "title": "The Reflexive Supply Chain: Sensing, Thinking, Acting",
    "selftext": "",
    "url": "https://moderndata101.substack.com/p/the-reflexive-supply-chain-stack",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1750168803.0,
    "author": "growth_man",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ldnvl0/the_reflexive_supply_chain_sensing_thinking_acting/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ldijod",
    "title": "Crack the Code: Your 2025 Roadmap to a Thriving Data Analyst Career",
    "selftext": "Hey guys,\n\nIve put together a roadmap for Data Analysts on a [medium.com](http://medium.com/) article. Hope it helps you clear some things up :)\n\nI upload weekly articles for DAs! Follow if you enjoy! thnxx \n\n[https://medium.com/@ervisabeido/crack-the-code-your-2025-roadmap-to-a-thriving-data-analyst-career-bfe11895a065](https://medium.com/@ervisabeido/crack-the-code-your-2025-roadmap-to-a-thriving-data-analyst-career-bfe11895a065)",
    "url": "https://www.reddit.com/r/data/comments/1ldijod/crack_the_code_your_2025_roadmap_to_a_thriving/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1750151904.0,
    "author": "ervisa_",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ldijod/crack_the_code_your_2025_roadmap_to_a_thriving/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1lc915s",
    "title": "Senior Project Mngr PMP with Lead DATA Engineer plus Governance background",
    "selftext": "Hi Data folks ,\n\n\n\nHope everyone is doing well.\n\n \n\nI need some help with regards to  data governance certifications. I know the DAMA CDMP certifications are well covered but I do have about 15 years of experience in the data field so can I directly take the data governance related Master certification without taking the fundamentals or specialist Level certifications ? \n\n\n\nI want to save time, money and energy by not sitting for those exams. I already have my  PMP. Thank you so much for any pointers and directions for my career growth. I am open to take other exams but  Governance, compliance, regulation and ethics are my interest other than project management. Please share your suggestions and insights so I can get an opportunity 💫🙏",
    "url": "https://www.reddit.com/r/data/comments/1lc915s/senior_project_mngr_pmp_with_lead_data_engineer/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1750016376.0,
    "author": "grateful_eternally",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lc915s/senior_project_mngr_pmp_with_lead_data_engineer/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1lc5wgq",
    "title": "I need help finding a youtube video",
    "selftext": "I am working on a video based on the history of aba (a Roblox fighting game that was very successful at a time), controversies, struggles, its rise and fall, and an insight on the community. I need this video by \"snake worl gaming\" (I'm not sure if that's how it was spelt. It was a fake account that was supposed to pretend to be Snakeworl) which was an afro samurai (one of the characters in it) video. I believe it not only captivates both controversies and a insight into the community but it has been removed. It is just a video that spams the N word with a clip of the character playing and some other stuff. This helps me a lot to show off the culture of Aba and how toxic it can be. Does anyone have the video downloaded or know how I can get the video back? And I do have the link to the video even though its been removed [https://www.youtube.com/watch?v=yt7qv7czn-s](https://www.youtube.com/watch?v=yt7qv7czn-s)",
    "url": "https://www.reddit.com/r/data/comments/1lc5wgq/i_need_help_finding_a_youtube_video/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1750008542.0,
    "author": "GlobalRacc",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lc5wgq/i_need_help_finding_a_youtube_video/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1laboym",
    "title": "Is it a me thing",
    "selftext": "I’ve noticed over the last few years a few amount of toxic people moving into the data roles , I moved over into data  around 13 years ago from a very toxic  environment as a buyer .\n\nPretty much everyone was cool, quiet people who just wanted to get on with the job and were left alone to do it .\nI’ve noticed over the last few years in management a lot of the people coming through  who just aren’t those cool people anymore, they are paper people who are all about throwing people under a bus and getting one over others . \nIs it a company thing or is the cash side just attracting these undesirables into our industry.\nAre your experiences the same or is it time to find a new company ?\nBe really interested to know other people’s experiences .\n",
    "url": "https://www.reddit.com/r/data/comments/1laboym/is_it_a_me_thing/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1749807329.0,
    "author": "Urdeadagain",
    "subreddit": "data",
    "permalink": "/r/data/comments/1laboym/is_it_a_me_thing/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mxk0jvi",
        "body": "Anecdotal evidence. \n\nThe grass is always greener but it sounds like it’s time for you to find a new work environment.",
        "score": 2,
        "created_utc": 1749819471.0,
        "author": "Lost_Philosophy_",
        "is_submitter": false,
        "parent_id": "t3_1laboym",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1lacx69",
    "title": "What will you change in this given your job role?",
    "selftext": "",
    "url": "https://i.redd.it/hnzm3bando6f1.png",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1749812077.0,
    "author": "ib_bunny",
    "subreddit": "data",
    "permalink": "/r/data/comments/1lacx69/what_will_you_change_in_this_given_your_job_role/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1la49uq",
    "title": "Has anyone accessed images + description from Art Resource(website) before?",
    "selftext": "Hi, as the title says, has anyone accessed data from Art Resource (https://www.artres.com/) before?\n\nI just wanted to know if you access both the images and the description? And if you can get it for free if possible?\n\nThanks!",
    "url": "https://www.reddit.com/r/data/comments/1la49uq/has_anyone_accessed_images_description_from_art/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1749779892.0,
    "author": "hyyhfvr",
    "subreddit": "data",
    "permalink": "/r/data/comments/1la49uq/has_anyone_accessed_images_description_from_art/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1l9mui8",
    "title": "Is GDPR training worth it for government teams?",
    "selftext": "Just read about a government department getting hit with a big GDPR fine due to how they handled personal data. The main issue? Lack of transparency and unclear data use.\n\nMakes me think—shouldn’t GDPR training be a standard for any public-facing team that handles citizen data?\n\nWould love to hear from anyone who’s rolled out GDPR training in a public or large org. Was it helpful? Any tips on what to include?",
    "url": "https://www.reddit.com/r/data/comments/1l9mui8/is_gdpr_training_worth_it_for_government_teams/",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1749735852.0,
    "author": "Few_Chocolate9758",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l9mui8/is_gdpr_training_worth_it_for_government_teams/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mxfl5gg",
        "body": "GDPR doesn't make any difference between companies and public organisations. You have to comply to it, that's it.\n\nThe whole thing is pretty simple., but the training is worth it.",
        "score": 2,
        "created_utc": 1749755412.0,
        "author": "captain_obvious_here",
        "is_submitter": false,
        "parent_id": "t3_1l9mui8",
        "depth": 0
      },
      {
        "id": "mzc3wge",
        "body": "Yep, doing recurrent training is necessary. \n\nBut people who should be trained first are data managers and IT as they must define policies and how to implement them. Then public-facing team will just have a couple things to add like asking for consent when collecting personal data.",
        "score": 1,
        "created_utc": 1750689521.0,
        "author": "Charlotte1309",
        "is_submitter": false,
        "parent_id": "t3_1l9mui8",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1l95n42",
    "title": "I'm looking for import/export data on world trade and have no idea where to look for. Any advice on where to start?",
    "selftext": "I know some entities world wide register the trade movements of goods as wheat, rice, fruit, oil, flowers, and other goods and commodities, but I have no idea where I would start searching and how far I could go. I'm trying to understand the agricultural trade relationship between my country, along with others and what other countries are trading the same product. All advice is welcomed, and I'm not sure how open it is.   \nThanks!  ",
    "url": "https://www.reddit.com/r/data/comments/1l95n42/im_looking_for_importexport_data_on_world_trade/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1749679514.0,
    "author": "--jh--",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l95n42/im_looking_for_importexport_data_on_world_trade/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mxalnky",
        "body": "Maybe start here: https://comtrade.un.org  the UN’s global import/export data reported by counties, country-to-country trade flows by product category, etc.\n\nGood luck!",
        "score": 1,
        "created_utc": 1749686260.0,
        "author": "Gracie305",
        "is_submitter": false,
        "parent_id": "t3_1l95n42",
        "depth": 0
      },
      {
        "id": "mxdeq5j",
        "body": "Thanks! I'll check it out.",
        "score": 1,
        "created_utc": 1749732433.0,
        "author": "--jh--",
        "is_submitter": true,
        "parent_id": "t1_mxalnky",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1l8x9ye",
    "title": "how does clay/kaspr/apollo/zoominfo work?",
    "selftext": "How do they get their customer/business data b/c their business models are focused on enrichment or high user licenses, I simply can't afford them and not a data expert. But where are data banks or places to find this data.  My idea would be to build my own scraping platform for e-mails phone numbers etc for sales or b2b sales and then have my own data base to enrich them.  Is it that complicated?",
    "url": "https://www.reddit.com/r/data/comments/1l8x9ye/how_does_claykasprapollozoominfo_work/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1749659618.0,
    "author": "makiset",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l8x9ye/how_does_claykasprapollozoominfo_work/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1l8uex0",
    "title": "Flourish line chart race",
    "selftext": "I'm using flourish for a school project and I'm making a line chart race. Everything is working well, but when I open my project or post it somewhere, the line chart race automatically starts. You can then replay it, but you will have already seen the outcome so it isnt as interesting. Is there a way that it doesn't start automatically or no? And if there is, where can I find it?",
    "url": "https://www.reddit.com/r/data/comments/1l8uex0/flourish_line_chart_race/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1749652810.0,
    "author": "Jaciedacie",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l8uex0/flourish_line_chart_race/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1l7wx5k",
    "title": "Universal Truths of How Data Responsibilities Work Across Organisations",
    "selftext": "",
    "url": "https://moderndata101.substack.com/p/key-data-roles-and-distribution-of-responsibilities",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1749556911.0,
    "author": "growth_man",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l7wx5k/universal_truths_of_how_data_responsibilities/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1l7ft2p",
    "title": "Hur får jag behörighet till låsta mappar på datorn?",
    "selftext": "Hur får jag behörighet till dessa låsta mappar på datorn? 🤔 Macbook.",
    "url": "https://i.redd.it/mte0xn65qy5f1.jpeg",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1749501488.0,
    "author": "CattyMaria",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l7ft2p/hur_får_jag_behörighet_till_låsta_mappar_på_datorn/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1l74brc",
    "title": "How to create a ranking for potential universities?",
    "selftext": "Hello! I'm not sure if this is the best place for this or not, but basically I'm trying to create a way to narrow down my list of potential universities to apply to in a more objective and consistent way by creating some kind of ranking system in a google sheet or excel (or something else). Problem being, I am an English student (albeit with a mild STEM background) and I'm not entirely sure how to actually do this in terms of setting up the sheet and the formulas and all of that. I would really appreciate any advice or guidance you guys could offer on this. Thanks!",
    "url": "https://www.reddit.com/r/data/comments/1l74brc/how_to_create_a_ranking_for_potential_universities/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1749474419.0,
    "author": "HPswl_cumbercookie",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l74brc/how_to_create_a_ranking_for_potential_universities/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1l7dlny",
    "title": "How to sell my data ?",
    "selftext": "I have my app users preference of food drinks and etc . Data around 14000 users . Is there any way to sell those data ? ",
    "url": "https://www.reddit.com/r/data/comments/1l7dlny/how_to_sell_my_data/",
    "score": 0,
    "upvote_ratio": 0.25,
    "num_comments": 6,
    "created_utc": 1749496435.0,
    "author": "Crafty-Seaweed7434",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l7dlny/how_to_sell_my_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mwws8lc",
        "body": "Create a listing on a data marketplace. Snowflake or Databricks have two good ones. \n\nOtherwise you can broker the deals yourself and handle all the transfer on your own.\n\nIt's work either way.",
        "score": 2,
        "created_utc": 1749506498.0,
        "author": "DistanceOk1255",
        "is_submitter": false,
        "parent_id": "t3_1l7dlny",
        "depth": 0
      },
      {
        "id": "mx0tq6v",
        "body": "I am guessing that you have informed users theough the privacy policy that their data might be sold? I'm just saying...\n\nAlso remember that there are different rules depending on where the user resides - EU and then certain states in USA have varying privacy laws.",
        "score": 2,
        "created_utc": 1749565826.0,
        "author": "scottdave",
        "is_submitter": false,
        "parent_id": "t3_1l7dlny",
        "depth": 0
      },
      {
        "id": "mxlalp2",
        "body": "Maybe also some targeted outreach. Give an excerpt of your data to  chatgpt and ask it to identify potential interested companies. Then reach out to fitting roles via email. Works, if high relevancy.",
        "score": 1,
        "created_utc": 1749833189.0,
        "author": "dawnofdata_com",
        "is_submitter": false,
        "parent_id": "t3_1l7dlny",
        "depth": 0
      },
      {
        "id": "mwyd3bi",
        "body": "Is there any websites to sell ?",
        "score": 0,
        "created_utc": 1749525652.0,
        "author": "Crafty-Seaweed7434",
        "is_submitter": true,
        "parent_id": "t1_mwws8lc",
        "depth": 1
      },
      {
        "id": "mx1rg84",
        "body": "Yes clearly informed ! 😌",
        "score": 2,
        "created_utc": 1749575365.0,
        "author": "Crafty-Seaweed7434",
        "is_submitter": true,
        "parent_id": "t1_mx0tq6v",
        "depth": 1
      }
    ],
    "comments_extracted": 5
  },
  {
    "id": "1l5usrd",
    "title": "How long do companies keep data before erasing it.",
    "selftext": "I wanted to test it out on quora.\n\nI uploaded a picture then I dragged it over to my browser where I then copied its url. I then deleted the image and left.\n\nI saved the url.\nI wanted to see how long it stores.  A day's go by and I paste it on a browser and the image came up. Then a few weeks later.\n\nIt's been several months and when I paste the url the image still shows.\n\nI'm just curious how long does it last.  Now if I posted the image I get that it would be there forever but for deleted posts ",
    "url": "https://www.reddit.com/r/data/comments/1l5usrd/how_long_do_companies_keep_data_before_erasing_it/",
    "score": 3,
    "upvote_ratio": 0.81,
    "num_comments": 10,
    "created_utc": 1749329251.0,
    "author": "No-Ear9852",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l5usrd/how_long_do_companies_keep_data_before_erasing_it/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mwjyof9",
        "body": "Data is free (not really but sort of).  In many circumstances there is no D in CRUD.\n\nJust status=disabled.",
        "score": 1,
        "created_utc": 1749331308.0,
        "author": "dotben",
        "is_submitter": false,
        "parent_id": "t3_1l5usrd",
        "depth": 0
      },
      {
        "id": "mwlx889",
        "body": "Have you cleared the cache of saved images on your computer?",
        "score": 1,
        "created_utc": 1749359025.0,
        "author": "ReturningSpring",
        "is_submitter": false,
        "parent_id": "t3_1l5usrd",
        "depth": 0
      },
      {
        "id": "mwqdir1",
        "body": "Deleting a record is often an expensive database operation, more so than updating a record or creating a new record. Databases have been optimized for writing new records, reading records, and updating records, but there's never been good money or a good use case to optimize deletes. As a result, some databases and companies do this thing where they just have a flag for data to hide and delete later. And then they do it in batches during periods of low-load.\n\nFurthermore, a company like Quora might have more than one copy of their database at different servers all over the world, which might need some time to catch up with your delete request.\n\nFinally, your browser also stores various thumbnails and server responses in a cache, which lets it load various resources faster.\n\nAll of these could be complicating factors.",
        "score": 1,
        "created_utc": 1749421355.0,
        "author": "mathbbR",
        "is_submitter": false,
        "parent_id": "t3_1l5usrd",
        "depth": 0
      },
      {
        "id": "mwk58lm",
        "body": "What ?",
        "score": 1,
        "created_utc": 1749333547.0,
        "author": "No-Ear9852",
        "is_submitter": true,
        "parent_id": "t1_mwjyof9",
        "depth": 1
      },
      {
        "id": "mwquzmv",
        "body": "I delete my cache regularly. ",
        "score": 1,
        "created_utc": 1749427430.0,
        "author": "No-Ear9852",
        "is_submitter": true,
        "parent_id": "t1_mwqdir1",
        "depth": 1
      },
      {
        "id": "mx5ea0q",
        "body": "'CRUD' is a slightly tongue-in-cheek to refer to the four major DML (Data Modification Language) operations:\n\nC-reate\n\nR-ead\n\nU-pdate\n\nD-elete\n\nFor the most part, long term storage is cheap.  It's better to 'flag' a row of data as 'disabled' and instruct all your software to ignore it.  So, it's practically deleted, but the data still remains \"under the hood\" for audit purposes.  So the comment above is saying to do it this way.  \"switch from Delete to Disable\" in CRUD.",
        "score": 1,
        "created_utc": 1749618254.0,
        "author": "NbdySpcl_00",
        "is_submitter": false,
        "parent_id": "t1_mwk58lm",
        "depth": 2
      },
      {
        "id": "mwkd97j",
        "body": "What would you like me to clarify?\n\nHave you tried running my answer through ChatGPT?",
        "score": -2,
        "created_utc": 1749336422.0,
        "author": "dotben",
        "is_submitter": false,
        "parent_id": "t1_mwk58lm",
        "depth": 2
      },
      {
        "id": "mwkppc0",
        "body": "I never asked anything about prices and chatgbt isn't a reliable source of information ",
        "score": 3,
        "created_utc": 1749340937.0,
        "author": "No-Ear9852",
        "is_submitter": true,
        "parent_id": "t1_mwkd97j",
        "depth": 3
      },
      {
        "id": "mwl480y",
        "body": "I think this guy is saying that data typically aren’t deleted. Once it hits the open internet it’s somewhat forever. I’m far from an expert here, but given that people are still able to dig up content from the earliest days of the internet, I guess there’s some validity to it.",
        "score": 2,
        "created_utc": 1749346607.0,
        "author": "Dataphiliac",
        "is_submitter": false,
        "parent_id": "t1_mwkppc0",
        "depth": 4
      },
      {
        "id": "mwnib51",
        "body": "Yeah but I never posted the image I uploaded it onto the page copied its url then deleted it.  To test how long it stays ",
        "score": 1,
        "created_utc": 1749389047.0,
        "author": "No-Ear9852",
        "is_submitter": true,
        "parent_id": "t1_mwl480y",
        "depth": 5
      }
    ],
    "comments_extracted": 10
  },
  {
    "id": "1l46cuc",
    "title": "Does anyone know any available third party API's/Web Scraper software to retrieve follower/following data on instagram?",
    "selftext": "Does anyone know any available third party API's/Web Scraper software to retrieve follower/following data on instagram? ",
    "url": "https://www.reddit.com/r/data/comments/1l46cuc/does_anyone_know_any_available_third_party/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1749147272.0,
    "author": "Odd-Fix-3467",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l46cuc/does_anyone_know_any_available_third_party/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1l40v5j",
    "title": "DataKit now let you bring a file from S3, GoogleSheets and other public URLs",
    "selftext": "Hey folks, imagine you got some public datasets in format of either PARQUET/JSON/XLSX/TXT or CSV hosted on S3, Github or anywhere else and you wanna just give them a look, do some quality check, have some charts around them and run your query. This should be a \"one\" minute job with [https://datakit.page](https://datakit.page/) right now. S3, Google sheets and any URL on the web are supported. This is a \"all\" client-side app (I don't have any server - with power of DuckDB-WASM). If you wanna self host the app please check: [https://docs.datakit.page](https://docs.datakit.page/) (With Docker, brew, etc).  \n**Question:** know what other data sources this could have, what's missing in the tool and how I can improve it.",
    "url": "https://v.redd.it/drd74p96e45f1",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1749134364.0,
    "author": "Sea-Assignment6371",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l40v5j/datakit_now_let_you_bring_a_file_from_s3/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1l3s848",
    "title": "Building a platform to create hyper-focused data analyst agents in seconds for your databases",
    "selftext": "That feeling of having to script database queries and then having to reason with the data yourself, I'm sure many of you know, is honestly pretty tedious. If you're on a team, then you dump it off to the data specialist to deal with that. \n\nWhat if you could spin up a data specialist for any specific topic in your database on demand? That’s what Nexus does. It lets you build domain-focused analyst agents who can analyze, reason, and act on your data to provide analysis and insights .From one-off queries to recurring monitoring and insight generation, Nexus gives every team small or big, technical or non-technical access to powerful, always-on data analysts.\n\nHoping to launch the platform soon, so if this seems interesting and want to be one of the early users, Join the Nexus waitlist here: [https://tally.so/r/3l187v](https://tally.so/r/3l187v)\n\nAppreciate the support!!",
    "url": "https://www.reddit.com/r/data/comments/1l3s848/building_a_platform_to_create_hyperfocused_data/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1749104906.0,
    "author": "niklbj",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l3s848/building_a_platform_to_create_hyperfocused_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1l34rae",
    "title": "What's the least painful way to do near real-time sync from PostgreSQL to Snowflake?",
    "selftext": "We don't need sub-second latency, but something close to real-time would be ideal. Our current batch pipeline has way too much lag and that's breaking downstream dashboards. I've looked at Fivetran and Stitch but wondering if there's anything more flexible (or less pricey)?",
    "url": "https://www.reddit.com/r/data/comments/1l34rae/whats_the_least_painful_way_to_do_near_realtime/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 10,
    "created_utc": 1749041487.0,
    "author": "pUkayi_m4ster",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l34rae/whats_the_least_painful_way_to_do_near_realtime/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "myb5wqj",
        "body": "Integrate has been smooth for near real-time syncs.",
        "score": 5,
        "created_utc": 1750185281.0,
        "author": "RoosterHuge1937",
        "is_submitter": false,
        "parent_id": "t3_1l34rae",
        "depth": 0
      },
      {
        "id": "mvzd5yc",
        "body": "\"Wondering if there's anything more flexible\", what limits did you see with Fivetran and Stitch?",
        "score": 1,
        "created_utc": 1749056409.0,
        "author": "seanlynch",
        "is_submitter": false,
        "parent_id": "t3_1l34rae",
        "depth": 0
      },
      {
        "id": "mvzsjw6",
        "body": "My company used Stitch for a while, but we had issues. We worked with them to solve them, and they were pretty great to work with, but we had way too much data to move around so it didn't work out.\n\nIn a classic scenario where you move a reasonable amount of data, Stitch is a very good tool.\n\nWe ended up building our own solution, which I don't recommend in your case.",
        "score": 1,
        "created_utc": 1749060665.0,
        "author": "captain_obvious_here",
        "is_submitter": false,
        "parent_id": "t3_1l34rae",
        "depth": 0
      },
      {
        "id": "mw2i1cy",
        "body": "Sling or watch the snowflake news for their new solution.",
        "score": 1,
        "created_utc": 1749091524.0,
        "author": "redditreader2020",
        "is_submitter": false,
        "parent_id": "t3_1l34rae",
        "depth": 0
      },
      {
        "id": "mw4v7ea",
        "body": "What does near real-time mean to you?\n\n6 seconds?  60 seconds?",
        "score": 1,
        "created_utc": 1749131467.0,
        "author": "kenfar",
        "is_submitter": false,
        "parent_id": "t3_1l34rae",
        "depth": 0
      },
      {
        "id": "myb60pz",
        "body": "Integrateio gave us control over how often we sync and lets us monitor pipeline health more transparently.",
        "score": 1,
        "created_utc": 1750185313.0,
        "author": "Responsible_Guava508",
        "is_submitter": false,
        "parent_id": "t3_1l34rae",
        "depth": 0
      }
    ],
    "comments_extracted": 6
  },
  {
    "id": "1l38dwt",
    "title": "Extracting strings from text files in Azure Data Factory",
    "selftext": "Hello all,\n\nI have a small project I need help with.\n\nI am using Data Factory to help synchronize our HR Management system in order to create user accounts. Fairly simple. Until we get a better HR solution I need to do it piecemeal.\n\nWhen an employee is added to the HR System, the application sends an email notification in which I have them saved as text files in a storage account.\n\nThe text file has fields:\n\nEmployee Name: John Doe\n\nEmployee ID: 012345\n\nJob title: Assembler\n\nSupervisor ID: 024682\n\nSupervisor Name: Kyle Smith\n\n\nA few more fields here and there. My plan was to have data factory grab these files, extract the fields from them and their values, and consolidate them into one CSV file that I can use to create user accounts and such.\n\nI don’t know how to ask google properly, and the results I get are for things like extracting values from file names or metadata. Not what I’m looking for.\n\nCan someone point me in the right direction to get something working? \n\nEach text file is one record, and in each text file are strings I want to extract and derive columns from them. \n\nThink of them as each file acts like a separate record, and each file has columns eliminated by lines.\n\nHope I explained it clearly.\n\n",
    "url": "https://www.reddit.com/r/data/comments/1l38dwt/extracting_strings_from_text_files_in_azure_data/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1749050637.0,
    "author": "Jazzlike-Ad-1522",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l38dwt/extracting_strings_from_text_files_in_azure_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1l2v88s",
    "title": "How to convert .isav file back into videol",
    "selftext": "I need help, I would like to know how to recover some files that I put in the private folder on my cell phone. It is a Redmi Note 10, but I forgot the private password",
    "url": "https://www.reddit.com/r/data/comments/1l2v88s/how_to_convert_isav_file_back_into_videol/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1749006705.0,
    "author": "Ecstatic_Reporter_29",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l2v88s/how_to_convert_isav_file_back_into_videol/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1l2mvr0",
    "title": "From Chaos to Clarity: A Step-by-Step Guide to Organising a Data Analytics Project",
    "selftext": "hey guys,\n\nwhen i first started as a DA one of my biggest dark spots was how can understand what should i do to organise a project? where do i start? how the seniors know how to tackle stakeholders and communicate with them? So what i did is to put down all the steps that a data analytics or data science project can be divided to and tried to implement that since then. Of course in each project i could remove some steps or even add something depending on the project but the core was always the same and i can say that it has helped me a lot since then to make everything clear.\n\nIn this medium article I show all these steps. Let me know what do you think and if there is anything different that you guys do. [https://medium.com/@ervisabeido/from-chaos-to-clarity-a-step-by-step-guide-to-organising-a-data-analytics-project-94939ac8c84a](https://medium.com/@ervisabeido/from-chaos-to-clarity-a-step-by-step-guide-to-organising-a-data-analytics-project-94939ac8c84a)\n\nI upload these kind of content every week so if you enjoy it follow for more :) ",
    "url": "https://www.reddit.com/r/data/comments/1l2mvr0/from_chaos_to_clarity_a_stepbystep_guide_to/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1748983524.0,
    "author": "ervisa_",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l2mvr0/from_chaos_to_clarity_a_stepbystep_guide_to/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1l2l60o",
    "title": "I have an idea for a project, not I'm sure how to get from 'website' to 'spreadsheet'",
    "selftext": "So long story short, I have access to some 'daily stats' (the data actually changes every 5 minutes) published by an online 'game' that I frequent. Their stats are available in a variety of plaintext, XML, and their own homebrew version of XML.\n\nI'd like to monitor some historical trends over time.\n\nI understand that I need some kind of program, script, or process to execute daily, hourly, whatever.. that will load the URL of the 'daily' data feeds, then 'scrape' that data for the current values (like \"get numeric value on the line, following the string \"users ingame\"). Then some magic happens and it becomes a line entry in a spreadsheet.\n\nI'm unable to put my finger on whatever the tool(s) is(are).. that can 'get' the data, trim it up into useful chunks, and then 'put' that data someplace I can actually use it (add today's data to a new line in Google Sheets for example).\n\nCan anyone help enlighten me as to what I'm missing here? I'd really hate for the solution to be 'set an alarm to remind you to do it manually'.\n\nIf possible, something that can be done via Linux would be the bee's knees.",
    "url": "https://www.reddit.com/r/data/comments/1l2l60o/i_have_an_idea_for_a_project_not_im_sure_how_to/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 10,
    "created_utc": 1748979549.0,
    "author": "zebragrrl",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l2l60o/i_have_an_idea_for_a_project_not_im_sure_how_to/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mvujmq6",
        "body": "You’ve got two solid options:\n\n**Google Sheets + Apps Script** – Write a simple script that pulls the data from the URL, parses what you need, and logs it to the sheet. Set a time-based trigger to run it every 5 mins. No server needed.\n\n**Python + cron job**– Use a Python script to fetch and parse the data(need to know web scraping), then append it to a CSV or push to Google Sheets via API. Use `cron` to automate it.\n\nRecommend the first one, it will cost you zero, but for the second, you need a server to keep running the cron job instances.",
        "score": 3,
        "created_utc": 1748988279.0,
        "author": "Direct_Week9103",
        "is_submitter": false,
        "parent_id": "t3_1l2l60o",
        "depth": 0
      },
      {
        "id": "mvvjosn",
        "body": "Start by deciding storage, be it on your computer, or a cloud provider. \n\nNext, I wouldn’t recommend scraping directly into another format, as any errors will make it so you lose that requests data, so first save it to a bucket/directory or whatever you decided on for storage in the exact way you scraped it. The file name should be the timestamp from when you got the data.\n\nThen you have 2 options, \n\n1. if you are going to scrap very frequently, like every 5-10minutes, you will have a lot of files, so it would be better to have another script to convert these files to a CSV format and add the timestamp column filled with the files name.\n2. If you are only getting data once day, you can load all the files in one go in the XML format and transform them into one big CSV whenever you want to visualize. \n\nThe second option doesn’t scale, but would be easier considering you won’t need a second cron job.\n\nThings to look up:\n- python cron job -> there are a few articles online that can walk you through it, but basically you’ll create an application that will have to stay running to fetch periodically \n- xmltodict -> transforms the standard XML to a dictionary\n- pandas -> transforms your dictionary to CSV\n- matplotlib -> graph library \n\nYou’ll notice that I didn’t mention about we scraping because your comment on different formats makes me believe there is an API of some sort, so you’d probably just make a request on that URL and receive the formatted response. My first option would be the XML.",
        "score": 2,
        "created_utc": 1749000572.0,
        "author": "markx15",
        "is_submitter": false,
        "parent_id": "t3_1l2l60o",
        "depth": 0
      },
      {
        "id": "mvuijt5",
        "body": "Claude. Tell it everything, tell it what you want.",
        "score": -1,
        "created_utc": 1748987935.0,
        "author": "ChevyRacer71",
        "is_submitter": false,
        "parent_id": "t3_1l2l60o",
        "depth": 0
      },
      {
        "id": "mvukbls",
        "body": "The problem here is that I don't just want the data \"now\", I want yesterday's data, and the day before's. Obviously I can't magic up data from the past, but I need to 'store' data again and again.\n\nEverything I've seen about this mythical \"apps\" solution for Google Sheets will just 'read in' the current values, and replace values in a sheet.. like checking the 'current best prices' from a dozen websites.\n\nI'm trying to track daily numbers as they change, over months, or even years.",
        "score": 2,
        "created_utc": 1748988500.0,
        "author": "zebragrrl",
        "is_submitter": true,
        "parent_id": "t1_mvujmq6",
        "depth": 1
      },
      {
        "id": "mvw8kvq",
        "body": "Thank you very much.\n\nIf you're interested in the 'data sources' I'm looking at, the feeds are basically 'different formats' based on which \\_\\_\\_\\_\\_.xml or \\_\\_\\_\\_\\_.txt that you ping at the site. The company puts all the data in there with transparency and scraping in mind, all stacked  in an orderly fashion.\n\nExamples:\n\n* https://api.secondlife.com/datafeeds/secondlife.xml   XML\n* https://api.secondlife.com/datafeeds/homepage.xml Non-standard XML \"LLSD\"\n* https://api.secondlife.com/datafeeds/homepage.txt Plain Text\n\n--\n\n* https://api.secondlife.com/datafeeds/lindex.xml LLSD again\n* https://api.secondlife.com/datafeeds/lindex.txt Plain Text\n\nThere's a lot there, so breaking it all down to the most interesting bits, and watching as they change over time, seems like a very interesting project to me.. but getting it from 'data out there' to something I'm recording.. is the 'hard part' it seems.\n\nThank you again! This is some good stuff to google.",
        "score": 1,
        "created_utc": 1749010164.0,
        "author": "zebragrrl",
        "is_submitter": true,
        "parent_id": "t1_mvvjosn",
        "depth": 1
      },
      {
        "id": "mvw6smk",
        "body": "Since you don't have access to the dataset that is practically impossible. However if the \n\nData on the site is open you can always have a way OR \n\nThe site has an api endpoint(step number 1 of Web scaping) you can write a script that hits the api and you can get any data you if and only if they have an exposed api endpoint",
        "score": 1,
        "created_utc": 1749009388.0,
        "author": "Direct_Week9103",
        "is_submitter": false,
        "parent_id": "t1_mvukbls",
        "depth": 2
      },
      {
        "id": "mvwdbt1",
        "body": "Yea, I sent you a DM, and I’ll send over a gist so you can get started, just too tired to bother right now. Send me a message to remind me tomorrow as it’s 2AM right now.",
        "score": 1,
        "created_utc": 1749012331.0,
        "author": "markx15",
        "is_submitter": false,
        "parent_id": "t1_mvw8kvq",
        "depth": 2
      },
      {
        "id": "mvw8z4k",
        "body": "Well I have access to the 'current' data, at any given moment.\n\nThe job I'm trying to accomplish, is setting up some kind of robot that checks the website, and writes down what it sees.. then checks again later, and writes it down again, on another line.\n\nI don't want to keep replacing C16:C18 with the latest data.\n\nI need the data that was written there '5 minutes ago' (or whatever) to be kept, along with the new data.\n\nBasically, I'm trying to BUILD that data set into the future, starting 'now'.",
        "score": 1,
        "created_utc": 1749010339.0,
        "author": "zebragrrl",
        "is_submitter": true,
        "parent_id": "t1_mvw6smk",
        "depth": 3
      },
      {
        "id": "mvwd8cb",
        "body": "Also, avoid appending data using fixed cell references. Instead, use a function that automatically targets the next available row and appends your new data there. This makes the script more flexible and avoids overwriting existing entries.",
        "score": 3,
        "created_utc": 1749012288.0,
        "author": "Direct_Week9103",
        "is_submitter": false,
        "parent_id": "t1_mvw8z4k",
        "depth": 4
      },
      {
        "id": "mvwcwnc",
        "body": "I get what you're saying now. Based on my initial suggestion, there are two solid options for setting up this kind of automation:\n\n1. Python Script + Cron Job\nThis is probably the easiest to set up in terms of code. You'd need to write three core functions:\n\nA scraper to collect the data,\n\nA save/append function to store the scraped data—ideally appending it to an existing file or database,\n\nAnd a cron job to run the script periodically (e.g., every 5 minutes).\n\n\nThe downside here is that you’ll need a server or a system that stays on to make sure the cron job runs consistently. So while the scripting part is simple, hosting might incur some cost or setup effort.\n\n\n\n\n2. Google Sheets + Apps Script\nThis is a serverless option and can be done entirely for free. Apps Script is like the automation language for Google Sheets—similar to how VBA macros works for Excel.\nWith this approach:\n\nYou’d write a script using Google Apps Script to scrape or pull the data.\n\nThe script can be set to run on a time-based trigger (e.g., every 5 minutes), so you don’t need to worry about hosting or cron jobs.\n\nIt appends the data directly into your Google Sheet, which also acts as your database or log.\n\n\nThe only catch is that Apps Script has a bit of a learning curve if you're not familiar with it. But once you get the hang of it, it's a really efficient and zero-cost solution. Also another could the limit of 1 million rows of Google sheet.\n\nI’ve used both methods, and both are totally doable—it just depends on your preference, setup, and how comfortable you are with the tools.",
        "score": 1,
        "created_utc": 1749012137.0,
        "author": "Direct_Week9103",
        "is_submitter": false,
        "parent_id": "t1_mvw8z4k",
        "depth": 4
      }
    ],
    "comments_extracted": 10
  },
  {
    "id": "1l2oluz",
    "title": "data and software",
    "selftext": "What term describes a person who works at the hybrid of data and software?",
    "url": "https://www.reddit.com/r/data/comments/1l2oluz/data_and_software/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1748987654.0,
    "author": "Direct_Week9103",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l2oluz/data_and_software/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mw3lszl",
        "body": "They build and maintain the data infrastructure, bridging the gap between raw data and usable insights. Other related terms include MLOps Engineer or Analytics Engineer, depending on the specific focus.",
        "score": 1,
        "created_utc": 1749110960.0,
        "author": "eb0373284",
        "is_submitter": false,
        "parent_id": "t3_1l2oluz",
        "depth": 0
      },
      {
        "id": "mw3mgk2",
        "body": "Thanks, professionally am a data analyst, but I want to be build data Web apps /mobile / desktop apps  focus on data consumption. What would be the best roadmap?",
        "score": 1,
        "created_utc": 1749111355.0,
        "author": "Direct_Week9103",
        "is_submitter": true,
        "parent_id": "t1_mw3lszl",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1l2b1vi",
    "title": "Data Quality: A Cultural Device in the Age of AI-Driven Adoption",
    "selftext": "",
    "url": "https://moderndata101.substack.com/p/data-quality-a-cultural-device",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1748955002.0,
    "author": "growth_man",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l2b1vi/data_quality_a_cultural_device_in_the_age_of/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1l1q4fy",
    "title": "Fell headfirst into data analyst role, career feedback",
    "selftext": "Hi all, A few years ago, my boss found themselves needing a data analyst, and I naturally stepped into the role. I'm the type of person who jumps in first and figures things out later. Since then, I've self-taught and leaned on friends to develop skills in advanced Excel formulas, Power Query, Power BI, moderate SQL (enough to navigate and get what I need), and even a bit of Python.\n\nDuring this time, I handled company forecasts, product purchase predictions, revamped Power BI visuals, and worked closely with top executives in a small to medium company that was acquired in 22. However, despite my experience, I've never formally studied data analytics, and I feel like I'm missing some important fundamentals.\n\nJust as I was starting to explore a more formal education—because I realized I genuinely enjoy this work—I was laid off without warning (two days after getting a new puppy, no less 🙈). Now, I feel uncertain about applying for traditional data analyst roles, struggling with how to properly articulate my skills and bridge any knowledge gaps. \n\nSo, I ask—what are the best certificates, courses, books, or resources that could help round out my skills and prepare me to secure my next role? Any insights or recommendations would be greatly appreciated!\n\n  \nI would also love to hear any stories or just plain something to watch out for advice!",
    "url": "https://www.reddit.com/r/data/comments/1l1q4fy/fell_headfirst_into_data_analyst_role_career/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1748890004.0,
    "author": "Never_elle",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l1q4fy/fell_headfirst_into_data_analyst_role_career/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1l1qyds",
    "title": "Alternative to Stata xtlogit",
    "selftext": "Hi everyone,\n\nI'm currently working on a panel data analysis involving a logistic regression model, and my advisor suggested exploring spatial refinements — specifically, incorporating a spatial component into a logistic regression model for panel data (i.e., a spatial panel logistic regression).\n\nUnfortunately, there doesn’t seem to be a package readily available in R that supports this type of model directly. My advisor mentioned that Stata offers something close with the xtlogit command, which handles panel logistic regression — and it appears that spatial extensions might be possible there as well.\n\nI'm now looking for alternatives in Python or R that could approximate the functionality of xtlogit in Stata, preferably with the ability to include spatial dependence (e.g., spatial lag or spatial error components).\n\nDoes anyone know of packages or methods that could help implement a spatial panel logistic regression in R or Python?\nAny guidance, even partial solutions or workarounds, would be greatly appreciated!\n\nThanks in advance!",
    "url": "https://www.reddit.com/r/data/comments/1l1qyds/alternative_to_stata_xtlogit/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1748891909.0,
    "author": "Puzzleheaded-Rest132",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l1qyds/alternative_to_stata_xtlogit/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1l1pne9",
    "title": "Using R to improve patient care with outpatient rehab and chronic pain program data — what data would you pull?",
    "selftext": "Hi all,\nI’m working on a short project where I’ll be using R to explore how data can improve care in outpatient programs specifically in neurological rehab, brain injury, sickle cell (hemoglobinopathy), and integrated chronic pain management.\n\nI’d love to get ideas or insights from this community on \nWhat kinds of data points or metrics would you pull from EMRs or patient systems in these kinds of settings?\nAny R packages or workflows you’ve found useful for working with clinical or patient-centered data?\nCan you please give me suggestions on how to present this kind of data clearly?\n\nEven apart from R and Excel what other tools I can use?  I want to know the simplest way of getting the job done. ",
    "url": "https://www.reddit.com/r/data/comments/1l1pne9/using_r_to_improve_patient_care_with_outpatient/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 6,
    "created_utc": 1748888912.0,
    "author": "Cypherventi",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l1pne9/using_r_to_improve_patient_care_with_outpatient/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mvnrhrr",
        "body": "Speak to the outpatient rehab team for their problems and pain points.",
        "score": 3,
        "created_utc": 1748899049.0,
        "author": "dangerroo_2",
        "is_submitter": false,
        "parent_id": "t3_1l1pne9",
        "depth": 0
      },
      {
        "id": "mvnu1zp",
        "body": "Are you part of a medical organisation? Getting access to private medical information isn't easy. Usually this is done via formal research grants, there is a boat load of considerations before getting approval. It's also big money so is usually overseen by legal agreements.\n\nMy advice would be to start communicating with your local health provider, look at what data they actually hold and what you can get access to and see if they have a recommended pathway for your research.\n\nThen I'd start with a clear goal. What are you trying to improve? Clinical data rarely points to an easy way to improve outcomes all the easy wins have been won by now. There are so many factors such as age, co-morbs, mental health issues etc.",
        "score": 2,
        "created_utc": 1748899813.0,
        "author": "k00_x",
        "is_submitter": false,
        "parent_id": "t3_1l1pne9",
        "depth": 0
      },
      {
        "id": "mvvtjyq",
        "body": "This doesn’t make sense and not how any healthcare projects are done . I have been in healthcare analytics for over a decade and have worked with electronic health records . I never went into the data blindly looking . The data is a reflection of what happens in the patient care setting. You go to the clinicians and the patient care folks - you figure out the processes and where the pain points are . Then you go and seek the data to quantify the issues. Then you work out with a group on a solution that can be implemented and then you use the data to measure the progress and if the solution is working . \n\nSuccessful data projects have stakeholder input and have stakeholders that share ownership in project success",
        "score": 1,
        "created_utc": 1749004099.0,
        "author": "Vervain7",
        "is_submitter": false,
        "parent_id": "t3_1l1pne9",
        "depth": 0
      },
      {
        "id": "mvoub3z",
        "body": "No, I was given these questions as part of interview. I thought it would be good to do as part of a project and work on it. I am not sure where to start.",
        "score": 1,
        "created_utc": 1748911854.0,
        "author": "Cypherventi",
        "is_submitter": true,
        "parent_id": "t1_mvnu1zp",
        "depth": 1
      },
      {
        "id": "mvvtsp1",
        "body": "Thank you! I was given these questions as part of an interview. I thought it would be good if I can do a small project and work on it. I understand now though. \n\n@Vervain7 I have been trying to get into the field. Is it possible for you to give me some advice and input on how the field is and what are some major software’s medical organisations use?",
        "score": 1,
        "created_utc": 1749004189.0,
        "author": "Cypherventi",
        "is_submitter": true,
        "parent_id": "t1_mvvtjyq",
        "depth": 1
      },
      {
        "id": "mvvve4c",
        "body": "At the hospitals there is different EHR. Epic is extremely popular and holds large market share in US. The data for reporting is in relational databases - you should know SQL.  The depending on team you are on and depending on organization it could be some sort of viz software - power bi, tableau , qlik, spotfire. Some places will have R or python as an option but depends on the job . All the software is company dependent . \n\nYou need to have a fundamental understanding of the healthcare system and how it works. Understand the patient journey. Understand different quality metrics or at least that they exist and that hospitals are measured using these metrics . This is US specific but I assume other countries have similar metrics .\n\nThere is different teams , finance , reporting , business analytics , research , etc … so all of these teams can have data roles and they all would be different if",
        "score": 1,
        "created_utc": 1749004780.0,
        "author": "Vervain7",
        "is_submitter": false,
        "parent_id": "t1_mvvtsp1",
        "depth": 2
      }
    ],
    "comments_extracted": 6
  },
  {
    "id": "1l0rzzy",
    "title": "Need Urgent Job Assistance - Data Analytics Fresher (India)",
    "selftext": "Hi All. \n\nI'm just going to put it out there - I hold an MBA in Data Science, graduated June last year. Started job hunting since March 2024. \n\nSo far - 3000+ applications (all customized with keywords and attached cover letters, at least those that I tracked), less than 5 callbacks. Make it at least 4500+ , if you include blindly applying as well. \n \n1. I'm well-versed in Python, SQL, Power BI, AWS - have done multiple projects indicating my skillset. \n2. Got my resume reviewed by at least 50 \"experts\" (got in touch with them through Topmate or references). They said while it's not a MAANG level resume, I should have no problem getting interviews from mid-size and small companies. \n3. Exhausted all options - LinkedIn DMs to Hiring managers and recruiters (1000+ in the last 8 months, less than 10 replies, 0 leads), cold emails (only rejections so far, around 500 emails here, in total), referrals. Nothing seems to work. \n\nI know I'm capable. Just need an interview callback to prove myself. It seems impossible to get that right now. It's a complete ghost town. \n\nAny job leads / advice would be greatly and sincerely helpful right now. I'm having sleepless nights - haven't slept more than 3 hours a day for the past 3-4 months - the constant stress, anxiety, helplessness - everything has taken a great toll on me. ",
    "url": "https://www.reddit.com/r/data/comments/1l0rzzy/need_urgent_job_assistance_data_analytics_fresher/",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1748792444.0,
    "author": "Weak-Oil-8461",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l0rzzy/need_urgent_job_assistance_data_analytics_fresher/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mw3luvo",
        "body": "Are you lacking experience?",
        "score": 1,
        "created_utc": 1749110992.0,
        "author": "Hembhatt_",
        "is_submitter": false,
        "parent_id": "t3_1l0rzzy",
        "depth": 0
      },
      {
        "id": "mvkhqoa",
        "body": "Hey a question not related to this post can ai replace a data scientist?",
        "score": 0,
        "created_utc": 1748862603.0,
        "author": "Nervous-Ingenuity-35",
        "is_submitter": false,
        "parent_id": "t3_1l0rzzy",
        "depth": 0
      },
      {
        "id": "mvm6tr5",
        "body": "Not really.",
        "score": 1,
        "created_utc": 1748882898.0,
        "author": "_Oduor",
        "is_submitter": false,
        "parent_id": "t1_mvkhqoa",
        "depth": 1
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1l0wopk",
    "title": "Survey? Yes!",
    "selftext": "Hot take:\n\nData people who don’t participate in surveys have no rights to complain about not having enough data to analyze on\n\n😂",
    "url": "https://www.reddit.com/r/data/comments/1l0wopk/survey_yes/",
    "score": 0,
    "upvote_ratio": 0.33,
    "num_comments": 0,
    "created_utc": 1748803929.0,
    "author": "IamNothing7890",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l0wopk/survey_yes/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1kzwoyp",
    "title": "The moment you realize you’re not analysing, you’re babysitting.",
    "selftext": "That’s the sentence I heard from an analyst last month.\n\nThey said they hadn’t actually done analysis in weeks.\n\nIt was all:\n\n* Debugging broken dashboards\n* Rewriting the same SQL with different filters\n* Explaining why “this metric doesn’t match the other one”\n\nSound familiar?\n\nIf you’ve been there, I’d love to hear how you broke out of it. \n\n",
    "url": "https://www.reddit.com/r/data/comments/1kzwoyp/the_moment_you_realize_youre_not_analysing_youre/",
    "score": 10,
    "upvote_ratio": 0.92,
    "num_comments": 9,
    "created_utc": 1748696881.0,
    "author": "EasternAggie",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kzwoyp/the_moment_you_realize_youre_not_analysing_youre/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mv8nkdj",
        "body": "That's the job. Stabilize and you will break out of it. Some companies dedicate teams to production support like this.",
        "score": 5,
        "created_utc": 1748697243.0,
        "author": "DistanceOk1255",
        "is_submitter": false,
        "parent_id": "t3_1kzwoyp",
        "depth": 0
      },
      {
        "id": "mvdfina",
        "body": "For 3 years, I supported \"business\" data analytics at a major govt agency, and my stats were cited in front of Congress on a semi-monthly basis. I was quite good at it.\n\nThat \"babysitting\"? That's success! You built working products that people are using and engaging with on a regular enough basis to recognize when a number looks off, or to bother making a feature request ticket for. The alternatives are \"I spend all day doing stats and analysis and nobody wants to use my work product\" or \"someone used my work product, saw how shit it was, and immediately gave up on it\". So no, this is a great sign!\n\nThat being said, the requests can get to be a chore, so the unofficial \"discipline\" rules we put in place were:\n\n0. Do not assume the meaning of a column in a table, even if the name sounds straightforward. Do not assume you understand the workflow, and how it maps to the tables. Do not even assume the stakeholder you are talking to is correct, as they are often missing critical knowledge. This is where headaches come from. Always, always, always verify. Poke around in system code, trace the data, talk to the people doing the work.\n\n1. Every request must be a ticket, so it can be prioritized and triaged.\n\n2. Every number you produce MUST be fully automated and replicable by another person on the team (scripts+documentation in a shared location, not excel clicks, never an unsaved query).\n\n3. Have a single source of truth for each business area, so nobody has to recreate the wheel, you will always end up paying for that.\n\n4. Rigorously define and enforce metric definitions and methodologies. If you get certain types of requests more than once, better start figuring out how to define and codify it. Keep it simple. If someone asks for a variation on it, make sure that variation is well documented and prominent on any place where the numbers may appear.\n\n5. If you rush through a last minute urgent request from your bosses bosses boss (etc) that came down on Friday afternoon, you will almost certainly have a confused, frantic email and a big headache on Monday morning.\n\n6. Attack data quality issues at the source, and understand how they were introduced. Do not monkey-patch. Call up the person in charge of the thing and let them know there's a need for input validation, a schema change, etc. They will fight you on it, so bring receipts, and an Impact / FTE estimate for how much time the issue is wasting.\n\nIf you do these, you'll spend less time babysitting. We had about 40+ production dashboards and still had time to do cool analytics and some deep dives.\n\nOften though, if you work with a stakeholder while babysitting, they will come to you with novel business questions that will trigger another round of deep analysis and fun projects. Babysitting led to me doing dynamic budget projections, classifiers for predicting the meaning of data in a decades old schema, survival analsis on cycle times, etc.\n\nI was able to do a lot of good there.",
        "score": 5,
        "created_utc": 1748758370.0,
        "author": "mathbbR",
        "is_submitter": false,
        "parent_id": "t3_1kzwoyp",
        "depth": 0
      },
      {
        "id": "mv9ahq2",
        "body": "That babysitting line hit me hard too. So many smart analysts just stuck fixing stuff instead of doing real work.",
        "score": 2,
        "created_utc": 1748704907.0,
        "author": "matthewd1123",
        "is_submitter": false,
        "parent_id": "t3_1kzwoyp",
        "depth": 0
      },
      {
        "id": "mvae5sq",
        "body": "> I’d love to hear how you broke out of it\n\nI couldn't shut it down, but it ended up costing my team so much time and effort, that we ended up shutting down the application.",
        "score": 1,
        "created_utc": 1748717312.0,
        "author": "captain_obvious_here",
        "is_submitter": false,
        "parent_id": "t3_1kzwoyp",
        "depth": 0
      },
      {
        "id": "my81hrj",
        "body": "Very interesting experience. What are your thoughts on providing time estimates ? (e.g. when someone asks you “how much time will this take?”)",
        "score": 1,
        "created_utc": 1750143567.0,
        "author": "heresacorrection",
        "is_submitter": false,
        "parent_id": "t1_mvdfina",
        "depth": 1
      },
      {
        "id": "mv9c25r",
        "body": "The course walks through how to shift from ad hoc responder to system architect. It’s all free, and pretty practical too.",
        "score": 1,
        "created_utc": 1748705397.0,
        "author": "EasternAggie",
        "is_submitter": true,
        "parent_id": "t1_mv9ahq2",
        "depth": 1
      },
      {
        "id": "mve4a10",
        "body": "Which course are you referring to?",
        "score": 1,
        "created_utc": 1748773002.0,
        "author": "Existing-Kale",
        "is_submitter": false,
        "parent_id": "t1_mv9c25r",
        "depth": 2
      }
    ],
    "comments_extracted": 7
  },
  {
    "id": "1l0iovv",
    "title": "How we stopped drowning in dashboards and actually got answers.",
    "selftext": "We used to have 89 dashboards. Everyone had their own. No one trusted any of them.\n\nIt took one analyst to say: “We’re doing this wrong. Let me build the system once, then you can explore all you want.”\n\nFast-forward: self-service dashboards, one SQL source of truth, clean structure. Way fewer arguments in meetings.\n\nJust helped launch a free course about this shift, especially for analysts who feel like they’re stuck in the middle\n\n",
    "url": "https://www.reddit.com/r/data/comments/1l0iovv/how_we_stopped_drowning_in_dashboards_and/",
    "score": 0,
    "upvote_ratio": 0.13,
    "num_comments": 8,
    "created_utc": 1748761231.0,
    "author": "Known-Enthusiasm-818",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l0iovv/how_we_stopped_drowning_in_dashboards_and/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mvdln7s",
        "body": "That shift from “everyone builds their own” to “one clean system + access controls” is a game-changer. Not sexy, but *so* effective.",
        "score": 2,
        "created_utc": 1748761851.0,
        "author": "matthewd1123",
        "is_submitter": false,
        "parent_id": "t3_1l0iovv",
        "depth": 0
      },
      {
        "id": "mvdlzyq",
        "body": "Master data management is critical - sh\\*t in, sh\\*t out.",
        "score": 1,
        "created_utc": 1748762058.0,
        "author": "Additional_Fault2853",
        "is_submitter": false,
        "parent_id": "t3_1l0iovv",
        "depth": 0
      },
      {
        "id": "mvdvugn",
        "body": "We did something similar but without stopping people from creating their own as this is also important for growing their own data literacy.\n\nWe created a big list of reports, this contained information on the report, refresh rates, purpose etc.\n\nThen when we had 3,000+ reports we built an app on top of it so that they can access the reports really quickly as we organised them into buckets by process and business area.\n\nWe could then track who used them, add training content, have people request issues really seamlessly.\n\nThis has built trust, evolved knowledge and creative freedom and we’ve just an award for our app too :)",
        "score": 1,
        "created_utc": 1748767931.0,
        "author": "megablocks516",
        "is_submitter": false,
        "parent_id": "t3_1l0iovv",
        "depth": 0
      },
      {
        "id": "mveb8lv",
        "body": "If you're a data analyst that needs a course to know this then you should consider a different trade.",
        "score": 1,
        "created_utc": 1748776782.0,
        "author": "Independent-Ice256",
        "is_submitter": false,
        "parent_id": "t3_1l0iovv",
        "depth": 0
      },
      {
        "id": "mvdmowx",
        "body": "If you’re early-stage, don’t wait too long to structure things. Chaos gets expensive. That’s one of the things we cover in the mini-course.",
        "score": 2,
        "created_utc": 1748762469.0,
        "author": "jspectre79",
        "is_submitter": false,
        "parent_id": "t1_mvdln7s",
        "depth": 1
      },
      {
        "id": "mvemcmf",
        "body": "Plenty of analysts know the right things to do/best practices and all that, but they don't have the authority to implement changes. Most analytics jobs are glorified customer service and order takers.",
        "score": 1,
        "created_utc": 1748781740.0,
        "author": "Welcome2B_Here",
        "is_submitter": false,
        "parent_id": "t1_mveb8lv",
        "depth": 1
      },
      {
        "id": "mvdnpe7",
        "body": "Would love to hear how others handled the transition. Especially when leadership still thinks “just build a dashboard” is the solution to every data request.",
        "score": 1,
        "created_utc": 1748763066.0,
        "author": "Jiffrado",
        "is_submitter": false,
        "parent_id": "t1_mvdmowx",
        "depth": 2
      }
    ],
    "comments_extracted": 7
  },
  {
    "id": "1l03owa",
    "title": "What tool or process actually helped you reduce duplicate dashboards?",
    "selftext": " Every team wants a slightly different cut of the data. But soon you’ve got 7 dashboards saying “Revenue” and none of them match. Everyone’s confused. You get pulled into 10 threads asking “which one is right?” We tried documentation, templates, even training, still ended up with a mess. Has anything worked for you to stop the proliferation of almost-identical dashboards?",
    "url": "https://www.reddit.com/r/data/comments/1l03owa/what_tool_or_process_actually_helped_you_reduce/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1748715248.0,
    "author": "Known-Enthusiasm-818",
    "subreddit": "data",
    "permalink": "/r/data/comments/1l03owa/what_tool_or_process_actually_helped_you_reduce/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mvapkrh",
        "body": "Yes, not accepting every team's request to make them a separate dashboard. I make one dasboard with a way to download raw data. If you want your own dashboard, you can download the data and build it yourself.\n\nNo data team can realistically keep up with these requests to create different variations of the same report. Accepting this practice is what causes duplication and confusion.\n\nIn my team, you need a VP level executive sponsor before asking for another dashboard. Leadership knows how much work creating and maintaining these reports, so not just anyone can request a new one.\n\nAnother thing, from personal experience, 9/10 of these requests can be resolved with powerquery and an excel file instead of a production dashboard. You only find this out after an intake meeting with stakeholders. People ask for the shiny dashboard because they saw someone else has it. The data team should decide what the appropriate output is based on the needs expressed during intake. The client should never get to start this process backwards by demanding the answer before we even know what questions they're actually asking.",
        "score": 3,
        "created_utc": 1748720945.0,
        "author": "FelixVulgaris",
        "is_submitter": false,
        "parent_id": "t3_1l03owa",
        "depth": 0
      },
      {
        "id": "mvad5g6",
        "body": "We started using OWOX BI to create a reusable SQL library tied to report elements, now I can at least *see* what people are querying. In which report and how. It's not perfect, but way better than the black box we had before.",
        "score": 1,
        "created_utc": 1748716991.0,
        "author": "jspectre79",
        "is_submitter": false,
        "parent_id": "t3_1l03owa",
        "depth": 0
      },
      {
        "id": "mvaeu9h",
        "body": "Honestly, the only thing that slowed it down was locking down dashboard creation. Everything goes through one request form now. Still a bottleneck, but it reduced chaos.",
        "score": 1,
        "created_utc": 1748717530.0,
        "author": "EasternAggie",
        "is_submitter": false,
        "parent_id": "t1_mvad5g6",
        "depth": 1
      },
      {
        "id": "mvagocn",
        "body": " \\+1 on metric consistency being the real issue. We built a semantic layer with dbt, then piped that into Looker. But people still create workarounds. Tempted to try OWOX BI,  heard it's more analyst-focused.",
        "score": 1,
        "created_utc": 1748718113.0,
        "author": "Jiffrado",
        "is_submitter": false,
        "parent_id": "t1_mvaeu9h",
        "depth": 2
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "1kzz7ey",
    "title": "Best way to present this large data set?",
    "selftext": "What is the best way to present the frequency of words used in a survey (600+ words), but based on categories they’re tagged in. \n\nSome words only belong to one category, some words belong to multiple categories. \n\nWhat is the best way to display both the frequency of each keyword, but also which category tags are associated with each word?  \n\nI hope this explanation makes sense - any help is greatly appreciated. ",
    "url": "https://www.reddit.com/r/data/comments/1kzz7ey/best_way_to_present_this_large_data_set/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1748703846.0,
    "author": "jpisbay",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kzz7ey/best_way_to_present_this_large_data_set/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mvy3am0",
        "body": "Bump",
        "score": 1,
        "created_utc": 1749043025.0,
        "author": "jpisbay",
        "is_submitter": true,
        "parent_id": "t3_1kzz7ey",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1kzvjtt",
    "title": "Durable ssd drive recommendations",
    "selftext": "Any model or brand recommendations for durable ssd drive ? Looking for most durable one that can last longer ",
    "url": "https://www.reddit.com/r/data/comments/1kzvjtt/durable_ssd_drive_recommendations/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1748693278.0,
    "author": "skyastrophile",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kzvjtt/durable_ssd_drive_recommendations/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1kzcyq0",
    "title": "What’s the ugliest thing in your reporting stack?",
    "selftext": "I don’t mean the charts.\n\nI mean the part that silently breaks things over time.\n\n* Metrics that get redefined without version control\n* 14 reports all calculating CAC slightly differently\n* Someone deleting a JOIN in a shared query, and no one notices until a client call\n\nWe talk a lot about pretty visuals here, but what’s the one invisible thing that makes your job harder?\n\nI’ve been helping (as a side expert) launch a free mini-course on exactly this, building scalable, maintainable reporting systems. It’s called “From Bottleneck to Data Hero.”\n\n",
    "url": "https://www.reddit.com/r/data/comments/1kzcyq0/whats_the_ugliest_thing_in_your_reporting_stack/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1748632923.0,
    "author": "jspectre79",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kzcyq0/whats_the_ugliest_thing_in_your_reporting_stack/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mv4zlz1",
        "body": "You touch on the thing I harp on repeatedly in a couple of your bullets. And I did myself no favors early in my transition into my current role, but I was very much learning as I went along. What can you do.\n\nThe easiest way to break it down would be to say: \n\n* Anywhere we have metrics/reporting calculations or aggregations being done close to the report output that can be pushed back towards the data source.\n\nExample: Multiple places where simple things (think resolution rates on task records at a customer call center) are performed on weekly/monthly/quarterly reports, and each one is a possible point of failure.\n\nObviously this is a simple example, but you can extrapolate this out to someone performing a complicated window function on a YTD report in February and not having the foresight to work with a dba to turn it into a stored procedure. Then, by October, they're wondering why their Power BI report is taking an hour to load.",
        "score": 1,
        "created_utc": 1748639260.0,
        "author": "rali3gh",
        "is_submitter": false,
        "parent_id": "t3_1kzcyq0",
        "depth": 0
      },
      {
        "id": "mv8vswr",
        "body": "Totally agree, the scariest part is when everyone’s working from slightly different numbers and doesn’t even realize it. Reporting debt is real.",
        "score": 1,
        "created_utc": 1748700224.0,
        "author": "Jiffrado",
        "is_submitter": false,
        "parent_id": "t3_1kzcyq0",
        "depth": 0
      },
      {
        "id": "mv8z0ht",
        "body": "That’s exactly why we made the course. Not about dashboards, about the stuff behind them. Like how to structure SQL libraries and control metric definitions.",
        "score": 1,
        "created_utc": 1748701299.0,
        "author": "matthewd1123",
        "is_submitter": false,
        "parent_id": "t1_mv8vswr",
        "depth": 1
      },
      {
        "id": "mv90pit",
        "body": "Curious, do you use any kind of version control or ownership system for metric definitions? I keep hearing teams struggle with this, especially at scale.",
        "score": 1,
        "created_utc": 1748701841.0,
        "author": "EasternAggie",
        "is_submitter": false,
        "parent_id": "t1_mv8z0ht",
        "depth": 2
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "1kyg8me",
    "title": "Built a data quality inspector that actually shows you what's wrong with your files (in seconds)",
    "selftext": "You know that feeling when you deal with a CSV/PARQUET/JSON and have no idea if it's any good? Missing values, duplicates, weird data types... normally you'd spend forever writing pandas code just to get basic stats.  \n**So now in** [datakit.page](https://datakit.page/) **you can:** Drop your file → visual breakdown of every column.  \n**What it catches:**\n\n* Quality issues (Null, duplicates rows, etc)\n* Smart charts for each column type\n\n**The best part:** Handles multi-GB files entirely in your browser. Your data never leaves your browser.\n\nTry it: [datakit.page](http://datakit.page/)\n\n**Question:** What's the most annoying data quality issue you deal with regularly?",
    "url": "https://v.redd.it/0ngz0k2b6r3f1",
    "score": 8,
    "upvote_ratio": 0.91,
    "num_comments": 23,
    "created_utc": 1748538527.0,
    "author": "Sea-Assignment6371",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kyg8me/built_a_data_quality_inspector_that_actually/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mv0lkzi",
        "body": "I’m just on my phone right now, but I’m very curious to take a look. Where is the data being analyzed, is it truly off your server, like you aren’t harvesting while you’re providing a service? \n\nAnswer: the most annoying data quality issue I deal with is coworkers providing data from forms which don’t have any validation, so I spend time cleaning data rather than making pipelines",
        "score": 2,
        "created_utc": 1748581179.0,
        "author": "ChevyRacer71",
        "is_submitter": false,
        "parent_id": "t3_1kyg8me",
        "depth": 0
      },
      {
        "id": "mv43jqw",
        "body": "Looks fantastic OP, \nsmall feedback : when I click on inspect data quality , if the action was to perform quality on the entirety of dataset, that would great instead of preview dataset only.",
        "score": 2,
        "created_utc": 1748629772.0,
        "author": "istockustock",
        "is_submitter": false,
        "parent_id": "t3_1kyg8me",
        "depth": 0
      },
      {
        "id": "mv7mtwo",
        "body": "This looks similar to describe() for each field. The biggest problem I have that only a few tools do, like Erwin and only to a limited degree, is speculatively execute joins between synonymous fields across multiple tables then tell me how many records from one schema/table/field will overlap with another field from a separate schema. \n\nE.g. you have 5 different ways to describe a thing, like say company industry classifications, and you want to see which one will yield the most complete matching for a universe of companies coming from an accounting tool. (Which will then in turn also be matched to something like a industry benchmark... Which there is an even bigger variety of)",
        "score": 2,
        "created_utc": 1748678216.0,
        "author": "andylikescandy",
        "is_submitter": false,
        "parent_id": "t3_1kyg8me",
        "depth": 0
      },
      {
        "id": "mvktzq3",
        "body": "Neat project.\n\nSomething like a data profiler is useful, but to me, nulls/dupes/low variance columns are not necessarily problematic data quality issues. What if most of the columns are well-intentioned but irrelevant? What if the table is recording duplicate events on purpose? These are good to know about when transforming data, but they aren't always data quality issues, they could accurately reflect reality.\n\nWhen I'm hunting data bugs, I'm not just looking at table contents, I am cross-referencing oral histories, operator interviews, business logic, workflow diagrams, database schema diagrams, and documentation, if I'm lucky enough to have any.\n\nI think that if you really want to tell clients what's wrong with their data, you're going to need a way to gather, encode, and test business logic. It helps if you know the schema well and how it possibly allows for deviations from the logic. You're also going to need a way to understand how the issue impacts the business, or it's going to be hard to get people together to fix it.",
        "score": 2,
        "created_utc": 1748867804.0,
        "author": "mathbbR",
        "is_submitter": false,
        "parent_id": "t3_1kyg8me",
        "depth": 0
      },
      {
        "id": "mvlmgix",
        "body": "Wow this looks cool!  Great job!  How many records have you tested?  Have you noticed any lag for large files?",
        "score": 2,
        "created_utc": 1748877038.0,
        "author": "Match_Data_Pro",
        "is_submitter": false,
        "parent_id": "t3_1kyg8me",
        "depth": 0
      },
      {
        "id": "mv4bif8",
        "body": "Heyy! Thanks for the comment and your answer!\nHave you had the chance to take a look? \nI dont have any server. I have explained here what made me make this tool so might gave more insights on how I got to this:\nhttps://thoughts.amin.contact/posts/why-I-built-a-query-tool\nAlso on the very first share on reddit:\nhttps://www.reddit.com/r/SQL/s/H1IECcFJOE\n\nLet me know if you bumped to any questions!",
        "score": 2,
        "created_utc": 1748632108.0,
        "author": "Sea-Assignment6371",
        "is_submitter": true,
        "parent_id": "t1_mv0lkzi",
        "depth": 1
      },
      {
        "id": "mv4a6op",
        "body": "Heyy!! Thanks a lot for checking it out! Inspect quality works on the whole dataset not just the preview. May I ask what gave you the impression thats just on the preview?",
        "score": 1,
        "created_utc": 1748631713.0,
        "author": "Sea-Assignment6371",
        "is_submitter": true,
        "parent_id": "t1_mv43jqw",
        "depth": 1
      },
      {
        "id": "mvduewo",
        "body": "This somewhat sounds like a Natural Join in DuckDB  [https://duckdb.org/docs/stable/sql/query\\_syntax/from#natural-joins](https://duckdb.org/docs/stable/sql/query_syntax/from#natural-joins)",
        "score": 2,
        "created_utc": 1748767070.0,
        "author": "ShotgunPayDay",
        "is_submitter": false,
        "parent_id": "t1_mv7mtwo",
        "depth": 1
      },
      {
        "id": "mvln7jl",
        "body": "Around 60-70million I guess has been one of the largest.\nIts been laggy before but almost everyday Im making more optimisations around it! Lemme know what you think! if you had time to give it a spin.\nAlso published self hosted today:\n\nhttps://www.reddit.com/r/dataengineering/s/69YbZUgIxM\n\nYou can find them on: https://docs.datakit.page",
        "score": 1,
        "created_utc": 1748877257.0,
        "author": "Sea-Assignment6371",
        "is_submitter": true,
        "parent_id": "t1_mvlmgix",
        "depth": 1
      },
      {
        "id": "mv4cnoy",
        "body": "Sorry, you’re correct. I downloaded 2 datasets and used smaller one. Tool looks fantastic!.",
        "score": 2,
        "created_utc": 1748632446.0,
        "author": "istockustock",
        "is_submitter": false,
        "parent_id": "t1_mv4a6op",
        "depth": 2
      },
      {
        "id": "mvgvbex",
        "body": "Not quite, that's a nice shortcut for writing queries in a database where everything is neat and foreign keys are nicely groomed. As the number of objects in a database grows and number of data packages included grows - thousands of tables (not exaggerating) - you lose the ability to maintain consistency. This is when metadata management becomes critical just for discoverability, like when you have a ton of cross reference options it helps to have tools just identifying all the objects you CAN POSSIBLY use to accomplish the same goal, and how the quality of the resulting data product differs.",
        "score": 2,
        "created_utc": 1748806865.0,
        "author": "andylikescandy",
        "is_submitter": false,
        "parent_id": "t1_mvduewo",
        "depth": 2
      },
      {
        "id": "mvlyv6z",
        "body": "I see that the data stays private and the processing is done in the users local env?  That is very interesting, but what kind of client resources are needed to maintain speed?",
        "score": 1,
        "created_utc": 1748880600.0,
        "author": "Match_Data_Pro",
        "is_submitter": false,
        "parent_id": "t1_mvln7jl",
        "depth": 2
      },
      {
        "id": "mv4dpl3",
        "body": "When I click on visualize and use pie-chart and export a png, it’s not showing all labeled data.",
        "score": 2,
        "created_utc": 1748632758.0,
        "author": "istockustock",
        "is_submitter": false,
        "parent_id": "t1_mv4cnoy",
        "depth": 3
      },
      {
        "id": "mv7n000",
        "body": "Oh got it! Clear then!",
        "score": 1,
        "created_utc": 1748678315.0,
        "author": "Sea-Assignment6371",
        "is_submitter": true,
        "parent_id": "t1_mv4cnoy",
        "depth": 3
      },
      {
        "id": "mvhc0f3",
        "body": "I guess I'm having trouble seeing what you're trying to do.  One time we had Vendors peppered all over the database and we first did a query get a tables containing a vendor column.  Then using Go we generated queries to Natural Join tables the tables in every combination possible listing row count results.  This made finding unlinked data easier.  Natural Joins made the process much easier, but doesn't work if columns don't share names.",
        "score": 2,
        "created_utc": 1748811999.0,
        "author": "ShotgunPayDay",
        "is_submitter": false,
        "parent_id": "t1_mvgvbex",
        "depth": 3
      },
      {
        "id": "mvlzgx9",
        "body": "Just some memory. (4GB should be good enough)\nThe database behind is a version of webassembley duckdb. That basically boost the db in browser and on top of that I have my own javascript code that gives you the UI.",
        "score": 1,
        "created_utc": 1748880774.0,
        "author": "Sea-Assignment6371",
        "is_submitter": true,
        "parent_id": "t1_mvlyv6z",
        "depth": 3
      },
      {
        "id": "mv7ncnh",
        "body": "Is it like the numbers/values are caught off? Or like not everything from your dataset is there? \nMore context: There should be some improvements there. Like now is limited to 1000 data points (as states in the panel - but this could get better stated or even configured). I will definitely make the experience in this panel more customisable! But please let me know what are the issues.",
        "score": 1,
        "created_utc": 1748678527.0,
        "author": "Sea-Assignment6371",
        "is_submitter": true,
        "parent_id": "t1_mv4dpl3",
        "depth": 4
      },
      {
        "id": "mvhnp81",
        "body": "We're talking about completely different problems, my suggestion to OP was to **edit:** **NOT** spend time on a problem that is already solved by every metadata management solution out there.\n\nMy scenario is you have multiple ambiguous join paths, how you write the code is irrelevant, but let's say for example you have multiple cross-reference sources required to get from Column A to Column B. \n\nThe columns are described using a metadata catalog - for example \"AccountID\" in your bookkeeping system goes through a cross-reference table to match the \"CompanyID\" of the data coming from your supply chain data vendor, and also \"Legal Entity ID\" from a tax solution.\n\nProblem A is finding out that this is your join path. You will never make those column names match. Even after you become a giant data company that sucks up all those other companies, you will still never make those match 100% (speaking from an insider's perspective on this - you're just adding another standard. After a few dozen such acquisitions yeah you have a big superset, but there's always some weird nuance).\n\nProblem B, let's say you're going to group something here by \"Industry\", there are actually a bunch of industry classification systems and the vendors selling that data never have 100% coverage. There are lots of examples like this, I'm just using Industries because it's public and easy to research for samples (GICS, NAICS, etc).",
        "score": 2,
        "created_utc": 1748815843.0,
        "author": "andylikescandy",
        "is_submitter": false,
        "parent_id": "t1_mvhc0f3",
        "depth": 4
      },
      {
        "id": "mv95hxc",
        "body": "I havent heard of webassembly before. Did you choose this because of its efficiency ?. caption on the site says 'data never leaves the browser', is it secure enough to handle healthcare data or finance data?. I am a product person (health data) and building something similar.",
        "score": 2,
        "created_utc": 1748703356.0,
        "author": "istockustock",
        "is_submitter": false,
        "parent_id": "t1_mv7ncnh",
        "depth": 5
      },
      {
        "id": "mvi2q3m",
        "body": "I kinda see what you're saying, but I've haven't done anything that complex without massaging the data first.  Like we literally just draw it down and manually remap it.\n\nFor Problem A DuckDB does fuzzy matching so when I Left Join Local VS Bank it will replace a Bank ID mismatch with data matching Local row even when using ID as the join.\n\nFor Problem B Pattern matching sounds like the easier thing to do programmatically unless you use an AI to infer commonality.\n\nBoth are tough problems without a remap.\n\nMaybe you could open an issue with DuckDB since OP is mostly using SUMMARIZE for their meta query.  [https://duckdb.org/docs/stable/guides/meta/summarize](https://duckdb.org/docs/stable/guides/meta/summarize)",
        "score": 2,
        "created_utc": 1748821076.0,
        "author": "ShotgunPayDay",
        "is_submitter": false,
        "parent_id": "t1_mvhnp81",
        "depth": 5
      },
      {
        "id": "mv96i92",
        "body": "Yes, because of performance and ability to integrate with web interfaces quite smoothly. It should be secure enough as you don’t send anything over the internet. Its like your opening your excel app though through a browser tab.\nThis week Im gonna release all sort of self hosted abilities. Python, npm, docker, brew. You basically run a command and have this exact same interface through a local host on your own machine. Will keep you posted as well!",
        "score": 3,
        "created_utc": 1748703671.0,
        "author": "Sea-Assignment6371",
        "is_submitter": true,
        "parent_id": "t1_mv95hxc",
        "depth": 6
      },
      {
        "id": "mvjhqx9",
        "body": "Never heard of DuckDB until now, so not sure how opening a ticket there will help. I see it's newer tech, but that looks to be more application oriented than where what I'm talking about is really common, which is enterprise data management and the analytics built off that universe of both mastered and unmastered data.\n\nInstead of remapping manually, you just have an abstraction handling it and all the data is described with a metadata catalog like Erwin or Collibra. That is integrated with a metastore like in DataBricks, Starburst (Iceberg/Trino), Snowflake, etc.",
        "score": 2,
        "created_utc": 1748841959.0,
        "author": "andylikescandy",
        "is_submitter": false,
        "parent_id": "t1_mvi2q3m",
        "depth": 6
      }
    ],
    "comments_extracted": 22
  },
  {
    "id": "1kytwfi",
    "title": "Wren AI’s New Charting Engine: Visuals on Demand via Chat! 📊",
    "selftext": "Just came across this latest update from[ Wren AI](http://getwren.ai) on LinkedIn, and it’s pretty exciting for data viz folks! Their new AI charting engine lets you generate *any* chart—think heatmaps, candlesticks, funnels, or geo maps—just by asking a question. No more wrestling with BI tool interfaces; it’s all conversational. Sounds like a huge time-saver for EDA or quick stakeholder reports! Free for 7 days @@\n\nHas anyone here played with Wren AI’s tool yet? How does it compare to stuff like Tableau or Power BI for whipping up visuals? Also, curious about the tech behind it—any guesses on how they’re handling the chart generation under the hood? Check out the full post: [https://getwren.ai/post/announcing-wren-ais-new-ai-powered-charting-engine?utm\\_campaign=14090256-Charting&utm\\_content=334284725&utm\\_medium=social&utm\\_source=linkedin&hss\\_channel=lcp-89794921](https://getwren.ai/post/announcing-wren-ais-new-ai-powered-charting-engine?utm_campaign=14090256-Charting&utm_content=334284725&utm_medium=social&utm_source=linkedin&hss_channel=lcp-89794921)\n\nSelf serve. No drama. \n\n\\#DataScience #DataVisualization #AI",
    "url": "https://www.reddit.com/r/data/comments/1kytwfi/wren_ais_new_charting_engine_visuals_on_demand/",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 0,
    "created_utc": 1748574468.0,
    "author": "expatinporto",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kytwfi/wren_ais_new_charting_engine_visuals_on_demand/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ky7dkk",
    "title": "Analysis of the transmission of shocks from the S&P 500 to major international stock market indices",
    "selftext": "I am working on the transmission of shocks from the S&P 500 to the DAX, FTSE 100, Hang Seng Index, and Nikkei. However, I am encountering problems and I’m wondering if someone could help me, please. This is for my final thesis, and I’m not sure if I am mishandling my data because no method seems to work—VAR, GARCH, ARMA-GARCH, none of them pass the tests. If anyone has any ideas, I would really appreciate it. It’s urgent.",
    "url": "https://www.reddit.com/r/data/comments/1ky7dkk/analysis_of_the_transmission_of_shocks_from_the/",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1748514523.0,
    "author": "Cultural-Hour-9480",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ky7dkk/analysis_of_the_transmission_of_shocks_from_the/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ky4kwn",
    "title": "I urgently need help who is data science or has good knowledge in econometrics and finance please",
    "selftext": "",
    "url": "https://www.reddit.com/r/data/comments/1ky4kwn/i_urgently_need_help_who_is_data_science_or_has/",
    "score": 1,
    "upvote_ratio": 0.66,
    "num_comments": 3,
    "created_utc": 1748503331.0,
    "author": "Cultural-Hour-9480",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ky4kwn/i_urgently_need_help_who_is_data_science_or_has/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "muuxbsd",
        "body": "It is way more likely that you'll get some help if you actually say what it is you need help with.",
        "score": 2,
        "created_utc": 1748514295.0,
        "author": "FlerisEcLAnItCHLONOw",
        "is_submitter": false,
        "parent_id": "t3_1ky4kwn",
        "depth": 0
      },
      {
        "id": "muuxln5",
        "body": "I am working on the transmission of shocks from the S&P 500 to the DAX, FTSE 100, Hang Seng Index, and Nikkei. However, I am encountering problems and I’m wondering if someone could help me, please. This is for my final thesis, and I’m not sure if I am mishandling my data because no method seems to work—VAR, GARCH, ARMA-GARCH, none of them pass the tests. If anyone has any ideas, I would really appreciate it. It’s urgent.",
        "score": 1,
        "created_utc": 1748514437.0,
        "author": "Cultural-Hour-9480",
        "is_submitter": true,
        "parent_id": "t1_muuxbsd",
        "depth": 1
      },
      {
        "id": "muuymi0",
        "body": "I can't tell you how many hours I've spent tracking down typos with single quotes vs double quotes.\n\nThat's to say coding issues are *incredibly* specific.\n\n\"None of them pass the tests\" gives no one any meaningful information to even begin giving you any help.\n\nTake a breath, and explain what you're doing, what you think should be happening *in exact details*. Ideally, actually post your code.",
        "score": 3,
        "created_utc": 1748514960.0,
        "author": "FlerisEcLAnItCHLONOw",
        "is_submitter": false,
        "parent_id": "t1_muuxln5",
        "depth": 2
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1kxp52c",
    "title": "Quarterly Data of Public Companies",
    "selftext": "Hi everyone!\n\nI am conducting a research at university and I need a data set of quarterly data for a 10 companies.\n\nThey are public companies and have quarterly reports available on their websites. What I can do is manually extract these informations that I need, but that would take an eternity as I have a lot of variables.\n\nAre there any websites or databases on the internet that have financial data of companies piled up in a unified space?",
    "url": "https://www.reddit.com/r/data/comments/1kxp52c/quarterly_data_of_public_companies/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1748458232.0,
    "author": "Ok-Director-2591",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kxp52c/quarterly_data_of_public_companies/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mut2kzs",
        "body": "The SEC, or FFIEC. I think the SEC’s database is named Edgar.",
        "score": 2,
        "created_utc": 1748482283.0,
        "author": "Rough_Count_7135",
        "is_submitter": false,
        "parent_id": "t3_1kxp52c",
        "depth": 0
      },
      {
        "id": "muwajyh",
        "body": "Try [EDGAR](https://www.sec.gov/edgar/search/) to see if this meets your needs.",
        "score": 1,
        "created_utc": 1748531642.0,
        "author": "Gracie305",
        "is_submitter": false,
        "parent_id": "t3_1kxp52c",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1kxbyix",
    "title": "Looking for advice for collecting and managing my data.",
    "selftext": "Hello, I'm in need of advice on how to collect/ interpret data relating to my job as a courier.\n\nMy goal would be to make a visualized graphic, however I'm currently still collecting data.\n\nRight now it goes as follows:  \nI open the courier app, set myself to 'online'.  \nOpen komoot and start recording.  \nDrive deliveries for a couple hours.  \nAt the end of my day I stop komoot and the courier app.\n\nThen either in the evening or the next day I enter the data into a google spreadsheet.  \nCurrently I'm tracking: Time, Distance, Deliveries, Earnings, Location\n\ndate, first delivery, last delivery, time active bolt, time in motion komoot, total time komoot\n\ndistance bolt, distance komoot\n\n\\# of deliveries, average delivery worth, earnings, tips, combined income (tips+earnings)\n\nAt the start of a week I get paid out, that's when I log weekly averages, and totals.\n\nNow, i'm looking for advice, what are some other things i can track? What are some tips you can give someone who has never collected data like this before? best practices?\n\nThank you for your time.",
    "url": "https://www.reddit.com/r/data/comments/1kxbyix/looking_for_advice_for_collecting_and_managing_my/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1748421195.0,
    "author": "Bolt_Courier",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kxbyix/looking_for_advice_for_collecting_and_managing_my/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "muuo1tj",
        "body": "Inbox",
        "score": 1,
        "created_utc": 1748509082.0,
        "author": "Luna-sexy1",
        "is_submitter": false,
        "parent_id": "t3_1kxbyix",
        "depth": 0
      },
      {
        "id": "muurfjo",
        "body": "I'm sorry, what do you mean?",
        "score": 1,
        "created_utc": 1748511076.0,
        "author": "Bolt_Courier",
        "is_submitter": true,
        "parent_id": "t1_muuo1tj",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1kwumd6",
    "title": "Considering Schools for MSBA",
    "selftext": "Anyone here get their Masters in Business Analytics? I've applied for a few schools (got in to GTech's OMSA so far) and trying to figure out what my order of preferences is. A couple of other schools I applied to were UC Davis, Cal Poly, and LMU. For a little more background, I have several years of unrelated job experience, so I'm looking for a program that will help me to make a career shift into analytics. Where did you go to school and what was your experience like? (Especially if making a career change). Thanks!",
    "url": "https://www.reddit.com/r/data/comments/1kwumd6/considering_schools_for_msba/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1748370044.0,
    "author": "Alternative-Bank8775",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kwumd6/considering_schools_for_msba/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1kwz2io",
    "title": "Request! TYIA Data nerds -  I need help visualising x amount of people",
    "selftext": "Hi! I'm looking to see if theres any website or something like that where I can put in X amount of people and be able to visualise it. For example: 800 people. I know 800 people is a lot (?) but I want to actually SEE what 800 people would look like. Or 20,000 people?  200 people? I hope this makes sense! thank you. ",
    "url": "https://www.reddit.com/r/data/comments/1kwz2io/request_tyia_data_nerds_i_need_help_visualising_x/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1748380481.0,
    "author": "Iamthemayogod",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kwz2io/request_tyia_data_nerds_i_need_help_visualising_x/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1kwqbkt",
    "title": "The Role of the Data Architect in AI Enablement",
    "selftext": "",
    "url": "https://moderndata101.substack.com/p/the-role-of-the-data-architect",
    "score": 2,
    "upvote_ratio": 0.75,
    "num_comments": 1,
    "created_utc": 1748360054.0,
    "author": "growth_man",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kwqbkt/the_role_of_the_data_architect_in_ai_enablement/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mus1jp2",
        "body": "Make sure Quality and other frameworks are in place , just ask ChatGPT :)",
        "score": 1,
        "created_utc": 1748469840.0,
        "author": "mullerjannie",
        "is_submitter": false,
        "parent_id": "t3_1kwqbkt",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1kwjgni",
    "title": "Looking for Historical Price Data for Chinese Symbols",
    "selftext": "Hey everyone,\n\nI’m looking for historical minute-level price data for a list of Chinese symbols shown in the comment below. If anyone has access to a data provider that includes these symbols or knows where I can get this data—either free or paid (at a reasonable price)—please let me know.\n\nI'm open to working with someone who can help export this data if you have access to Wind, Bloomberg, or any other relevant platform.\n\nAppreciate any help or leads—thanks in advance!",
    "url": "https://www.reddit.com/r/data/comments/1kwjgni/looking_for_historical_price_data_for_chinese/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1748340188.0,
    "author": "Resident_Platypus281",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kwjgni/looking_for_historical_price_data_for_chinese/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "muhnogm",
        "body": "CSI 300 IDX FUTUR\n\nCSI500 IDX Future\n\nChina 10yr Bond\n\nChina 5yr Bond Fu\n\nSSE50 Index Futur\n\nPalm Oil Future\n\nSOY MEAL\n\nSOYBEAN OIL\n\nDCE Iron Ore Fut\n\nPVC Future\n\nHSCEI Futures\n\nHSTECH Futures\n\nGold Future\n\nSilver Future\n\nCOPPER FUT (SHFE)\n\nDeformed Bar Fut\n\nRUBBER FUT (SHFE)\n\nNickel Futures\n\nTin Futures\n\nZINC FUT (SHFE)\n\nALUMINUM FUT\n\nFUEL OIL FUTURE\n\nHot-rolled Coil\n\nRapeseed Oil Fut\n\nGlass Futures\n\nRapeseed Meal\n\nSUGAR WHITE\n\nPTA FUTURE\n\nMethanol Futures",
        "score": 1,
        "created_utc": 1748340207.0,
        "author": "Resident_Platypus281",
        "is_submitter": true,
        "parent_id": "t3_1kwjgni",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1kw4rco",
    "title": "Data Analytics Project: Creating a comprehensive score column for a Fictitious Portuguese Coffee Trade Broker based on trade data, feasibility, bean quality, and growth.",
    "selftext": "Hello everyone!\n\nI am doing a quick analytics project before i start an internship. The main data source I am using is based on the coffee industry, with my inspiration derived from a Kaggle dataset: (https://www.kaggle.com/datasets/michals22/coffee-dataset/data?select=Coffee\\_export.csv)\n\nThe data is just export, import, and some inventory data on a country-level basis, so quite high level. I decided to create a business case/scenario, because i think its fun, tests my creativity, and forces me to learn a little about the industry.\n\nIn short, my fictitious company is a portuguese coffee trade brokerage that has a focus on facilitating and consulting on trade of specialty coffee. We basically are a Mid-size coffee trade facilitator that connects smallholder exporters, currently in Brazil, with a select few specialty coffee importers (and roasters) across european markets in portugal, netherlands, france, and germany. \n\nWhat I have been \"tasked\" to do is determine which coffee-producing and exporting nation to expand our trade facilitation and consulting operations to. We want to expand out of Brazil (where our facilitation is concentrated) to find an emerging market that we can connect importers with. We believe that there could be places with higher margin supply and unique ESG funding, since we have determined that consumers of speciality coffee are more and more demanding traceable, ethical coffee, which could help our PR and put us in the position for NGO partnerships and even grants/additional funding.\n\nI, as the analyst, have decided to create a scaled (z-score), weighted average scoring system that takes into account different categories that are relevant to whether we should expand our business to a particular country AND reporting on whether that country is emerging and ready to produce specialty coffee (think of it as potential). To do this, I decided the following scores were needed to create the \"overall\" score:\n\n1. Feasibility Score: takes into account WGI, LPI, and ease of doing business scores from World Bank data.\n2. Coffee Quality Score: Can either be quantitative or categorical, still deciding. I do not want to give a nationwide score really, since a country's coffee quality varies within locations of that country. however, I do not know what else to do. I may just 1-5 it based on academic research of each countries coffee quality.\n3. 10 yr export growth, production growth, and total exports/production for 10 year period (CAGR?)\n4. Volatility Score (10 year standard deviation; checks for how volatile a country's exports/production has been).\n\nThere is some other data that I will consider for the overall score. My biggest issue is assigning weights.\n\nMy question is: Does this seem like a decent strategy for the problem I am facing? Is this crap, and useless to show in a portfolio? And have I given enough context for answers to those questions?",
    "url": "https://www.reddit.com/r/data/comments/1kw4rco/data_analytics_project_creating_a_comprehensive/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1748291600.0,
    "author": "Curious_Cry1348",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kw4rco/data_analytics_project_creating_a_comprehensive/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1kufol5",
    "title": "Historical Constituents for S&P 1500",
    "selftext": "Hi everyone, I need a list of S&P 1500 constituents from 2014 for my bachelor's thesis. I have access to Eikon and CRSP and while they supposedly should have this data available, I can't for the life of me find the 'historic' part of my query. Eikon does not give an option to set a date, while I can't get CRSP to return anything useful at all. I would know how to do this in Bloomberg quickly but I will only have access to that at my job in about a months time (and I'm not even sure if using it for personal reasons is allowed). Has anyone done something similar before? All help appreciated, thank you.",
    "url": "https://www.reddit.com/r/data/comments/1kufol5/historical_constituents_for_sp_1500/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1748104244.0,
    "author": "DylanIE_",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kufol5/historical_constituents_for_sp_1500/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ku79xi",
    "title": "Is there any data engineers here ?",
    "selftext": "",
    "url": "https://www.reddit.com/r/data/comments/1ku79xi/is_there_any_data_engineers_here/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1748077461.0,
    "author": "Grand-Fix-201",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ku79xi/is_there_any_data_engineers_here/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ktgd75",
    "title": "Where can I get job posting data via API?",
    "selftext": "Hey everyone, I'm working on a project, building a tool for internal use at my company and I would need job openings/job postings data.\n\nBut I've run into a data availability problem. I'm currently scraping company job boards for title, location, description etc, but wondered if anyone knows a good API for job postings. I'd rather not build a scraper myself if I don't have to. \n\nThe cost doesn’t matter much as long as the coverage and accuracy is good. \n\nThanks!",
    "url": "https://www.reddit.com/r/data/comments/1ktgd75/where_can_i_get_job_posting_data_via_api/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1747996908.0,
    "author": "GaandDhaari",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ktgd75/where_can_i_get_job_posting_data_via_api/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mttgq29",
        "body": "For job opening APIs you can check out Crustdata or Simple job data API. They are both data providers via API that aggregate jobs from multiple sources.\n\nBoth of them provide all the datapoints you asked for - title, location and description. I guess the only difference would be the data coverage and timeliness of data.\n\nI think Crustdata would be more accurate for tracking job openings as they offer webhooks to monitor a specific company or job title so your tool can be instantly notified/updated when there are new openings.",
        "score": 1,
        "created_utc": 1748000415.0,
        "author": "Any-Teaching4430",
        "is_submitter": false,
        "parent_id": "t3_1ktgd75",
        "depth": 0
      },
      {
        "id": "mtudihl",
        "body": "Hiring.caffe might have what you need?",
        "score": 1,
        "created_utc": 1748011430.0,
        "author": "Anxious_Current2593",
        "is_submitter": false,
        "parent_id": "t3_1ktgd75",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1kt7yu3",
    "title": "Disappointed with Eastern University, looking for transfer recommendations",
    "selftext": "I’m working on a MS in Data Science at EU. I had no coding experience in work or school. They advertised their program as friendly to those with 0 coding experience. I’ve been very disappointed. Honestly, if I did it over again, I’d just go get an MBA. I don’t think this program is friendly to non-coders. The 7 week blitzes don’t impart any sort of mastery. I’m sure it’s a great program if you have prior experience, but I don’t feel like a master of Python, SQL, R, nor Tableau. Once I start to feel comfortable with one programming language, it’s time to jump to the next class. I’m 6/10 classes done and I’m just sick of this place. I’d like to finish the degree elsewhere and maybe get the time to actually master what I’m learning. Does anyone know of any good online schools for data science/analytics? ",
    "url": "https://www.reddit.com/r/data/comments/1kt7yu3/disappointed_with_eastern_university_looking_for/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1747965060.0,
    "author": "Dachshunds_N_Dragons",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kt7yu3/disappointed_with_eastern_university_looking_for/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mtrn0pv",
        "body": "I did my MSDS at Eastern as a non-coder and while it was tough, I found that working on personal projects helped me gain the necessary skills. I don’t have the answer to your question, but it is possible for non-coders with enough practice and time. Best of luck",
        "score": 1,
        "created_utc": 1747967151.0,
        "author": "duckduckgoose9876",
        "is_submitter": false,
        "parent_id": "t3_1kt7yu3",
        "depth": 0
      },
      {
        "id": "mvny8aa",
        "body": "I'm a software engineer (career changer -- I have my BA in History) and am currently enrolled in Eastern's MSDS program in an effort to transition into data engineering. The reality of programming/coding classes everywhere is you will HAVE to be teaching yourself outside of the assigned coursework. It's normal not to feel like you've mastered a language or a technology and honestly it's kind of how the job feels a lot of the time. It may sound trite, but the most important skill to have in this field is being able to learn on your own and to apply that knowledge to the problem at hand. It feels like a grind at first but it does get easier over time (or at least less stressful). I did obviously have knowledge coming into the program, but the way they teach coding seems to be very in line with how I learned as a beginner.",
        "score": 1,
        "created_utc": 1748901079.0,
        "author": "AxTqka",
        "is_submitter": false,
        "parent_id": "t3_1kt7yu3",
        "depth": 0
      },
      {
        "id": "mvqz3cc",
        "body": "Thank you for this insight. It’s really helped me at a time when I’ve felt down. I’m still plugging along and I’ll get through but you’ve given me hope.",
        "score": 1,
        "created_utc": 1748948552.0,
        "author": "Dachshunds_N_Dragons",
        "is_submitter": true,
        "parent_id": "t1_mvny8aa",
        "depth": 1
      },
      {
        "id": "mvva2wa",
        "body": "I'm glad! You got this! It feels seriously overwhelming when you're first starting out, but once you've put in the work and you're able to look back on the challenges you faced at the beginning you'll surprise yourself with how far you've come. Hopefully the TAs can be of help in your classes, and ChatGPT can be a good resource to use if you're feeling like you don't understand a concept or why something works. I recommend first trying to break it down yourself but if you're still stuck, it can be helpful to ask it to walk you through a piece of code step by step to get a sense for how it's working (and then you can use that knowledge to think it through on your own next time).",
        "score": 1,
        "created_utc": 1748997183.0,
        "author": "AxTqka",
        "is_submitter": false,
        "parent_id": "t1_mvqz3cc",
        "depth": 2
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "1ksrp61",
    "title": "Are We Doomed?",
    "selftext": "I just went through a demo session in my organization done by our internal GEN-AI team\n\nSome background:  I'm in the analytics team in a banking industry which is heavily guarded by RBI guidelines wherein you cannot expose your data to the outside world\n\n\nThey've come up with a full blown agentic AI platform.\nSome of the things it can do:\n1) Have a code base? Need some changes to it basis input from business. Simply upload the file, type in English what are the changes to be done and book! It will do it for you in a minute!\n2) Need to understand how the governance guidelines have changed. Upload the old and new documents and it will summarise for you\n3) You're a data scientist who takes pride in building models? I just saw an agent do it from EDA, feature engineering, feature selection and training followed by hyper tuning in a span of 10 minutes. What the fuck???!!\n4) It can just mimic everything and anything I've been doing in my job \n\nMy question: What next? It's clear this thing is getting democratised at a crazy speed and we won't need to do things which we are doing currently in the next 3_4 years.\nI used to take great pride being in the data science field and considered programming my forte. I can see that disappearing which is sad to some extent\n\n\nWhat is the niche that we need to develop to stay relevant for the upcoming years.\nWhat I saw today, if it goes to perfection, every field is going to go mad!",
    "url": "https://www.reddit.com/r/data/comments/1ksrp61/are_we_doomed/",
    "score": 5,
    "upvote_ratio": 0.67,
    "num_comments": 8,
    "created_utc": 1747923051.0,
    "author": "Pristine-Quiet8464",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ksrp61/are_we_doomed/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mto68yp",
        "body": "Going to sound like a bit of a downer but...\n\n1. No it can't, or the hard part was actually \"typing in english the changes to be done\" which is still your job\n\n2. Useful, happy to spend less time on this. As long as it doesn't hallucinate. Actually maybe I should just run a diff myself...\n\n3. So what? That's more like a homework assignment at school. Who's responsible for the model in production?\n\n4. God I wish it could\n\n  \nIf AI is deployed like this, it will need lots of babysitters. I don't know about you but I foresee many poor souls desperately copying and pasting errors messages asking AI to please fix itself or else they will lose their jobs.\n\nThere's a reason that AI is ALWAYS a demo session...",
        "score": 10,
        "created_utc": 1747928014.0,
        "author": "data_minimal",
        "is_submitter": false,
        "parent_id": "t3_1ksrp61",
        "depth": 0
      },
      {
        "id": "mtol4ty",
        "body": "Welcome to my nightmare \nSince the beginning of this year, I'm saving more than 60% of my salary to let me time to study to blue collar jobs in the coming years (probably electricity)",
        "score": 1,
        "created_utc": 1747932342.0,
        "author": "freedumz",
        "is_submitter": false,
        "parent_id": "t3_1ksrp61",
        "depth": 0
      },
      {
        "id": "mtodfwl",
        "body": "I understand your point. My entire point was this whole new revolution would lead to few resources doing entire work which means a depleting job market. \n1. Yup and that's prompt engineering \n3. I don't think building models is homework. There's ample work needed from deciding on the kind of features to be used, the definition of target variable. There's a lot of human decisioning required. As far as models in production is considered, this could be very well extended to model monitoring too and a CI/CD pipeline \n4. At present it doesn't. But the pace at which it's getting better it would definitely do 80% of our work in the next 3-4 years is what my guess is\n\nWhat I saw today would go beyond DEMO in my org for sure!",
        "score": 2,
        "created_utc": 1747930084.0,
        "author": "Pristine-Quiet8464",
        "is_submitter": true,
        "parent_id": "t1_mto68yp",
        "depth": 1
      },
      {
        "id": "mtolvjp",
        "body": "Don't you think that is the first level of jobs to be eliminated?\nMy only guess is that we need to move towards high iq roles to survive\n\nOR\nSave 60% salary for 15 years and spend rest of our lives in peace",
        "score": 1,
        "created_utc": 1747932552.0,
        "author": "Pristine-Quiet8464",
        "is_submitter": true,
        "parent_id": "t1_mtol4ty",
        "depth": 1
      },
      {
        "id": "mtp16zo",
        "body": "I think it sounds like your main concern might just be capitalism lol. Productivity per employee must always go up.\n\nYou might not build the model in a few years but if you're open minded you would be the go-to person to help audit and orchestrate the models. Sounds fun to me. You'll be fine.\n\nIf that doesn't make you feel better then I think you're underestimating just how many people are STILL stuck in excel spreadsheets and have no proclivity towards learning to code/automate.",
        "score": 5,
        "created_utc": 1747936855.0,
        "author": "data_minimal",
        "is_submitter": false,
        "parent_id": "t1_mtodfwl",
        "depth": 2
      },
      {
        "id": "mtom5o4",
        "body": "Good luck to replace domestic electrician \nBots are not ready to go in houses ( not like a clean factory)",
        "score": 1,
        "created_utc": 1747932632.0,
        "author": "freedumz",
        "is_submitter": false,
        "parent_id": "t1_mtolvjp",
        "depth": 2
      },
      {
        "id": "mz2b71n",
        "body": "AI can replicate IQ higher than most humans. And the way you hear Jensen Huang talk it’s only going to keep improving its ability to reason and problem solve.",
        "score": 1,
        "created_utc": 1750547136.0,
        "author": "ckal09",
        "is_submitter": false,
        "parent_id": "t1_mtolvjp",
        "depth": 2
      },
      {
        "id": "mtp0nvx",
        "body": "Yes, add plumbers, AC installers and most construction jobs.",
        "score": 1,
        "created_utc": 1747936704.0,
        "author": "platinum1610",
        "is_submitter": false,
        "parent_id": "t1_mtom5o4",
        "depth": 3
      }
    ],
    "comments_extracted": 8
  },
  {
    "id": "1kr3wya",
    "title": "Reverse Sampling: Rethinking How We Test Data Pipelines",
    "selftext": "",
    "url": "https://moderndata101.substack.com/p/reverse-sampling-rethinking-data-pipelines",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1747744987.0,
    "author": "growth_man",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kr3wya/reverse_sampling_rethinking_how_we_test_data/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1kqd5x1",
    "title": "Any good data-marketplace out there for data about health?",
    "selftext": "I just came across this data-marketplace online called Opendatabay (https://www.opendatabay.com/ ) \nI want to use one of their advertised dataset on cancer survival per region for a university project. Has anyone used any of their datasets or bought any of their datasets? ",
    "url": "https://www.reddit.com/r/data/comments/1kqd5x1/any_good_datamarketplace_out_there_for_data_about/",
    "score": 3,
    "upvote_ratio": 0.81,
    "num_comments": 1,
    "created_utc": 1747665072.0,
    "author": "Fun_Plum_1526",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kqd5x1/any_good_datamarketplace_out_there_for_data_about/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mvjc7pf",
        "body": "I purchased one of their synthetic datasets about eye cancer. It was like in the description and easy to use. They have many datasets on cancer survival on the website so you got to choose the right one for your project.",
        "score": 3,
        "created_utc": 1748839186.0,
        "author": "Old-Disaster-2669",
        "is_submitter": false,
        "parent_id": "t3_1kqd5x1",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1kqf0em",
    "title": "Do you know where to find historical data of Gold?",
    "selftext": "Hi, I'm doing a research project on my own. I want to compare the different prices of gold with some cryptocurrencies to see if there is any correlation. Right now, I'm struggling to find these gold prices since I would need them in like a montly basis from at least 2015 to the end of 2024. Does anyone know a place where I can get this data in .csv or excel so I can run them on python? I would really appreciate your help!",
    "url": "https://www.reddit.com/r/data/comments/1kqf0em/do_you_know_where_to_find_historical_data_of_gold/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1747669558.0,
    "author": "Super-Ordinary9998",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kqf0em/do_you_know_where_to_find_historical_data_of_gold/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mt92dib",
        "body": "Hey guys! I am in junior college i recently joined a data analyst course by linkedin and microsoft Any advice I should keep in mind.My interest in data is growing every day",
        "score": 1,
        "created_utc": 1747720294.0,
        "author": "Nervous-Ingenuity-35",
        "is_submitter": false,
        "parent_id": "t3_1kqf0em",
        "depth": 0
      },
      {
        "id": "mwyozrq",
        "body": "Go check out Techsalerator. They do offer customizable datasets in CSV/Excel, great for analysis in Python, and even have gold export data if your research needs that angle too.",
        "score": 1,
        "created_utc": 1749530719.0,
        "author": "Virtual-Ball-9643",
        "is_submitter": false,
        "parent_id": "t3_1kqf0em",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1kpxple",
    "title": "How to get live Song/Artist info (student)",
    "selftext": "So I am trying to create a project that basically gives you top artists weekly (and updates it in a CI/CD fashion). Just something simple as I start my learning journey. \n\nThe issue is that there is no way to continuously get that data without scraping. Every tutorial I can see for this is like 5 years old and recommend Spotify but Spotify seems to have waged a war recently because nothing works anymore. I can't even get a playlist \n\nLast fm works but their info is way more limited. And I can't afford sound charts and chartmetric. \n\n\nAny suggestions for an alternative. I wanted to scrape via beautiful soup but I don't want to get ip banned",
    "url": "https://www.reddit.com/r/data/comments/1kpxple/how_to_get_live_songartist_info_student/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1747611436.0,
    "author": "Beneficial_Ad_5874",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kpxple/how_to_get_live_songartist_info_student/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1kpj0pj",
    "title": "Email addresses of mortgage brokers?",
    "selftext": "Is there a data source out there to get the email addresses of mortgage brokers?\n\nThanks!  ",
    "url": "https://www.reddit.com/r/data/comments/1kpj0pj/email_addresses_of_mortgage_brokers/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1747571938.0,
    "author": "cfiatzph",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kpj0pj/email_addresses_of_mortgage_brokers/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1koyd2j",
    "title": "Bitcoin Blockchain data",
    "selftext": "I am trying to build an apache spark application on aws for project purposes to analyse Bitcoin transactions. I am streaming data from BlockCypher.com, but there are API call limits(100 per hour, 1000 per day). For the project, I want to do some user behavior analysis, trend analysis and network activity analysis.\n\nSince I need historical data to create a meaningful model, I have been searching for a downloadable file of size around 2-3GBs. In my streamed data, I have Block, transaction,input and output files. \n\nI cannot find a dataset where I can download this information from. It does not even have to comply completely with my current schema, I can transform it to match my schema. But does anyone know easily downloadable zip files?",
    "url": "https://www.reddit.com/r/data/comments/1koyd2j/bitcoin_blockchain_data/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 9,
    "created_utc": 1747502842.0,
    "author": "data_fggd_me_up",
    "subreddit": "data",
    "permalink": "/r/data/comments/1koyd2j/bitcoin_blockchain_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "msu04fi",
        "body": "Simplest method is to run a node, which will download the database for local use. Then you can use one of several ETL tools (including the baseline bitcoin-cli) to extract transactions for analysis.",
        "score": 1,
        "created_utc": 1747506861.0,
        "author": "of_the_second_kind",
        "is_submitter": false,
        "parent_id": "t3_1koyd2j",
        "depth": 0
      },
      {
        "id": "msv15he",
        "body": "https://console.cloud.google.com/marketplace/product/bitcoin/crypto-bitcoin?",
        "score": 1,
        "created_utc": 1747519353.0,
        "author": "dotben",
        "is_submitter": false,
        "parent_id": "t3_1koyd2j",
        "depth": 0
      },
      {
        "id": "msu0njq",
        "body": "But this will download and verify 500GB+ data since the start of bitcoin? And it will take over 4-5 days until its complete?",
        "score": 1,
        "created_utc": 1747507039.0,
        "author": "data_fggd_me_up",
        "is_submitter": true,
        "parent_id": "t1_msu04fi",
        "depth": 1
      },
      {
        "id": "msvepo5",
        "body": "Found it. Took me a long time before someone let me know that bq or aws has the presynced data.",
        "score": 1,
        "created_utc": 1747524272.0,
        "author": "data_fggd_me_up",
        "is_submitter": true,
        "parent_id": "t1_msv15he",
        "depth": 1
      },
      {
        "id": "msu1rwb",
        "body": "That sounds about right. When you say you looking for 2-3GB, what data are you looking for and what can be omitted?\n\nAlso, it looks like AWS offers nodes as a service, pre-synced",
        "score": 1,
        "created_utc": 1747507417.0,
        "author": "of_the_second_kind",
        "is_submitter": false,
        "parent_id": "t1_msu0njq",
        "depth": 2
      },
      {
        "id": "msu2z0l",
        "body": "2-3GB data as in I need only latest 5-6 months data which includes block information, tx ( current state of a given transaction from Block), TXInput( inputs consumed within a transaction), TX Output( outputs created by a transaction). Anything else can be omitted.\n\nAs for AWS nodes as a service,  I have a student account and will have to check if I can collect this historical data without any limitations.",
        "score": 1,
        "created_utc": 1747507826.0,
        "author": "data_fggd_me_up",
        "is_submitter": true,
        "parent_id": "t1_msu1rwb",
        "depth": 3
      },
      {
        "id": "msu6zun",
        "body": "Take a look at the AWS node offering and see if that works. If not, I can probably help you if you provide a script which extracts the info you want from the json format for blocks (see https://bitcoin.stackexchange.com/questions/55188/download-single-and-specific-block-for-study-purposes#55193), and a place to upload the resulting dataset.",
        "score": 1,
        "created_utc": 1747509184.0,
        "author": "of_the_second_kind",
        "is_submitter": false,
        "parent_id": "t1_msu2z0l",
        "depth": 4
      },
      {
        "id": "msub84t",
        "body": "I found bigquery bitcoin data which I can query and download as csv. Not sure if this was the best way, but got the data. Thanks for the info that aws and others have the presynced data. 👐",
        "score": 1,
        "created_utc": 1747510625.0,
        "author": "data_fggd_me_up",
        "is_submitter": true,
        "parent_id": "t1_msu6zun",
        "depth": 5
      }
    ],
    "comments_extracted": 8
  },
  {
    "id": "1koqpci",
    "title": "LSE Executive program in data analytics",
    "selftext": "I have come across London school of economics' data analysis program throus Times Pro. While the brochure says we need an undergraduate degree\nThe app eligibility criteria says that student who do not fit the criteria above can give an aptitude exam. \nHas anyone done or is currently doing this course?\nShould I go ahead with it?",
    "url": "https://www.reddit.com/r/data/comments/1koqpci/lse_executive_program_in_data_analytics/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1747481159.0,
    "author": "Western-Loan6884",
    "subreddit": "data",
    "permalink": "/r/data/comments/1koqpci/lse_executive_program_in_data_analytics/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mstabjs",
        "body": "Il not sure but what do you mean by LSE. The only one I know is the London Stock Exchange. Sowwy.",
        "score": 2,
        "created_utc": 1747498539.0,
        "author": "Far-Structure-6115",
        "is_submitter": false,
        "parent_id": "t3_1koqpci",
        "depth": 0
      },
      {
        "id": "mstvdu7",
        "body": "London school of economics",
        "score": 1,
        "created_utc": 1747505272.0,
        "author": "Western-Loan6884",
        "is_submitter": true,
        "parent_id": "t1_mstabjs",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1ko1x6g",
    "title": "this site tells you what 8 billion humans are probably doing rn",
    "selftext": "couldn’t stop thinking about how many people are out there just… doing stuff.  \nso i made a site that guesses what everyone’s up to based on time of day, population stats, and vibes.\n\n[https://humans.maxcomperatore.com/](https://humans.maxcomperatore.com/)\n\nwarning: includes stats on sleeping, commuting, and statistically estimated global intimacy.",
    "url": "https://i.redd.it/xbt3k42me51f1",
    "score": 17,
    "upvote_ratio": 0.9,
    "num_comments": 12,
    "created_utc": 1747404875.0,
    "author": "OkNeedleworker6500",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ko1x6g/this_site_tells_you_what_8_billion_humans_are/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "msrbjic",
        "body": "Should add one for people on the website right now lol",
        "score": 3,
        "created_utc": 1747466127.0,
        "author": "jonii-chan",
        "is_submitter": false,
        "parent_id": "t3_1ko1x6g",
        "depth": 0
      },
      {
        "id": "mszc5bf",
        "body": "Hey, I'm finally a 1%'er!",
        "score": 2,
        "created_utc": 1747587678.0,
        "author": "rogerwatersnake",
        "is_submitter": false,
        "parent_id": "t3_1ko1x6g",
        "depth": 0
      },
      {
        "id": "mt1da68",
        "body": "Same number of people working rn as taking a smoking break? Doesn’t maths",
        "score": 2,
        "created_utc": 1747611671.0,
        "author": "shatGippity",
        "is_submitter": false,
        "parent_id": "t3_1ko1x6g",
        "depth": 0
      },
      {
        "id": "mt92cv3",
        "body": "Hey guys! I am in junior college i recently joined a data analyst course by linkedin and microsoft Any advice I should keep in mind.My interest in data is growing every day",
        "score": 2,
        "created_utc": 1747720284.0,
        "author": "Nervous-Ingenuity-35",
        "is_submitter": false,
        "parent_id": "t3_1ko1x6g",
        "depth": 0
      },
      {
        "id": "msnr8ro",
        "body": "how many are jorkin it?",
        "score": 1,
        "created_utc": 1747416588.0,
        "author": "double_dose_larry",
        "is_submitter": false,
        "parent_id": "t3_1ko1x6g",
        "depth": 0
      },
      {
        "id": "mstghhf",
        "body": "added lol",
        "score": 1,
        "created_utc": 1747500547.0,
        "author": "OkNeedleworker6500",
        "is_submitter": true,
        "parent_id": "t1_msrbjic",
        "depth": 1
      },
      {
        "id": "mt000p6",
        "body": "Warfare?",
        "score": 2,
        "created_utc": 1747595140.0,
        "author": "OkNeedleworker6500",
        "is_submitter": true,
        "parent_id": "t1_mszc5bf",
        "depth": 1
      },
      {
        "id": "mt1k86j",
        "body": "Yeah. I fucked up",
        "score": 1,
        "created_utc": 1747614246.0,
        "author": "OkNeedleworker6500",
        "is_submitter": true,
        "parent_id": "t1_mt1da68",
        "depth": 1
      },
      {
        "id": "mtao0oe",
        "body": "ask chatpgt",
        "score": 1,
        "created_utc": 1747749261.0,
        "author": "OkNeedleworker6500",
        "is_submitter": true,
        "parent_id": "t1_mt92cv3",
        "depth": 1
      },
      {
        "id": "mso4o9y",
        "body": "Me at least",
        "score": 6,
        "created_utc": 1747420595.0,
        "author": "OkNeedleworker6500",
        "is_submitter": true,
        "parent_id": "t1_msnr8ro",
        "depth": 1
      },
      {
        "id": "msz5xd6",
        "body": "They made it look classy by calling it *intimacy*",
        "score": 2,
        "created_utc": 1747585702.0,
        "author": "Mo-42",
        "is_submitter": false,
        "parent_id": "t1_mso4o9y",
        "depth": 2
      },
      {
        "id": "mszzzfv",
        "body": "Very",
        "score": 1,
        "created_utc": 1747595129.0,
        "author": "OkNeedleworker6500",
        "is_submitter": true,
        "parent_id": "t1_msz5xd6",
        "depth": 3
      }
    ],
    "comments_extracted": 12
  },
  {
    "id": "1knufwe",
    "title": "Give our personal data to our gouvernement can make GDRP more respected ?",
    "selftext": "I have read an article on Meta wich planned to use personal discussions and comments on posts to feed their AI. This doesn’t respect GDRP for EU citizen. Our data doesn’t seems to be important and protected. It looks different for China citizens data, i know that all their data are centralized by their government. \n\nIf European countries take responsibility over their citizen data, should it be more complicated for Meta to collect data from each country ? Is it preferable to give responsability to your country instead of EU ?",
    "url": "https://www.reddit.com/r/data/comments/1knufwe/give_our_personal_data_to_our_gouvernement_can/",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 1,
    "created_utc": 1747378216.0,
    "author": "AdditionalEstimate19",
    "subreddit": "data",
    "permalink": "/r/data/comments/1knufwe/give_our_personal_data_to_our_gouvernement_can/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mslmrqz",
        "body": "On one hand, giving our data directly to our governments sounds like it could help with enforcing GDPR better. Maybe it would be harder for companies like Meta to just grab our info and do whatever they want with it. But on the other hand, do we really trust our governments with all our data?\n\nThe situation in China is super different: The government there has total control over data, but that’s not really about protecting people’s privacy. It’s more about surveillance and control. I don’t think we’d want that in the EU.\n\nGDPR is supposed to protect us already, but the problem is that not every country enforces it strongly. So yeah, maybe if each country took more responsibility and cracked down on companies misusing data, it would help. But I’d still rather the EU as a whole keep setting the rules, otherwise we end up with 27 versions of data privacy.\n\nSo short answer: I’d like stronger enforcement, but I’m not sure giving our data to the government is the best way to do it.",
        "score": 1,
        "created_utc": 1747390521.0,
        "author": "Super-Mission-2686",
        "is_submitter": false,
        "parent_id": "t3_1knufwe",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1knf7ek",
    "title": "Where to find vin decoded data to use for a dataset?",
    "selftext": "Currently building out a dataset full of vin numbers and their decoded information(Make,Model,Engine Specs, Transmission Details, etc.). What I have so far is the information form NHTSA Api:  \n[https://vpic.nhtsa.dot.gov/api/](https://vpic.nhtsa.dot.gov/api/)\n\nWhich works well, but looking if there is even more available data out there.   \nDoes anyone have a dataset or any source for this type of information that can be used to expand the dataset?",
    "url": "https://www.reddit.com/r/data/comments/1knf7ek/where_to_find_vin_decoded_data_to_use_for_a/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1747332846.0,
    "author": "Danielpot33",
    "subreddit": "data",
    "permalink": "/r/data/comments/1knf7ek/where_to_find_vin_decoded_data_to_use_for_a/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1knc4um",
    "title": "How we use machine learning to find passports and unlock one key to offshore secrecy",
    "selftext": "",
    "url": "https://www.icij.org/inside-icij/2025/05/how-we-use-machine-learning-to-find-passports-and-unlock-one-key-to-offshore-secrecy/?utm_campaign=news&utm_medium=social&utm_source=reddit",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1747325474.0,
    "author": "ICIJ",
    "subreddit": "data",
    "permalink": "/r/data/comments/1knc4um/how_we_use_machine_learning_to_find_passports_and/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1kmieuu",
    "title": "Is 7 day rolling average the same as weekly average",
    "selftext": "basically the title",
    "url": "https://www.reddit.com/r/data/comments/1kmieuu/is_7_day_rolling_average_the_same_as_weekly/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1747237336.0,
    "author": "Sufficient_Bug_2716",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kmieuu/is_7_day_rolling_average_the_same_as_weekly/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "msaeh6s",
        "body": "It depends. In my experience, weekly average usually has a fixed definition (Sunday-Saturday, Monday-Sunday, etc.).",
        "score": 12,
        "created_utc": 1747237797.0,
        "author": "murdercat42069",
        "is_submitter": false,
        "parent_id": "t3_1kmieuu",
        "depth": 0
      },
      {
        "id": "msalg6s",
        "body": "7 day rolling average would match exactly when the window matches, but \"rolling\" indicates that it's the avg of the previous 7 days not a fixed window/week. While they may routinely match each other, they are not the same.\n\nIt's important to note that week and day are different grains. Keeping this in mind will help inform appropriate use of each.",
        "score": 5,
        "created_utc": 1747239801.0,
        "author": "DistanceOk1255",
        "is_submitter": false,
        "parent_id": "t1_msaeh6s",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1kmil03",
    "title": "Project related help",
    "selftext": "Hey everyone,\n\nI’m a final year B.Sc. (Hons.) Data Science student, and I’m currently in search of a meaningful idea for my final year project. Before posting here, I’ve already done my own research - browsing articles, past project lists, GitHub repos, and forums - but I still haven’t found something that really clicks or feels right for my current skill level and interest.\n\nI know that asking for project ideas online can sometimes invite criticism or trolling, but I’m posting this with genuine intention. I’m not looking for shortcuts - I’m looking for guidance.\n\nA little about me:\nIn all honesty, I wasn't the most focused student in my earlier semesters. I learned enough to keep going, but I didn’t dive deep into the field. Now that I'm in my final year, I really want to change that. I want to put in the effort, learn by building something real, and make the most of this opportunity.\n\nMy current skills:\n>> Python\n>> SQL and basic DBMS\n>> Pandas, NumPy, basic data analysis\n>> Beginner-level experience with Machine Learning\n>> Used Streamlit to build simple web interfaces\n\n(Leaving out other languages like C/C++/Java because I don’t actively use them for data science.)\n\n\nI’d really appreciate project ideas that:\n>> Are related to real-world data problems\n>> Are doable with intermediate-level skills\n>> Have room to grow and explore concepts like ML, NLP, data visualization, etc.\n\nInvolve areas like:\n>> Sustainability & environment\n>> Education/student life\n>> Social impact\n>> Or even creative use of open datasets\n\n\nIf the idea requires skills or tools I don’t know yet, I’m 100% willing to learn - just point me toward the right direction or resources. And if you’re open to it, I’d love to reach out for help or feedback if I get stuck during the process.\n\nI truly appreciate:\n>> Any realistic and creative project suggestions\n>> Resources, tutorials, or learning paths you recommend\n>> Your time, if you’ve read this far!\n\n\nNote: I’ve taken the help of ChatGPT to write this post clearly, as English is not my first language. The intention and thoughts are mine, but I wanted to make sure it was well-written and respectful.\n\nThanks a lot. This means a lot to me.",
    "url": "https://www.reddit.com/r/data/comments/1kmil03/project_related_help/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1747237749.0,
    "author": "No_One_77777",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kmil03/project_related_help/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1klx9he",
    "title": "UT Statistics and Data Science OR UWashington Informatics",
    "selftext": "Hi! I was recently admitted to the University of Texas at Austin for Statistics and Data Science and the University of Washington for the School of Informatics.\n\nWhat do the Class sizes, funding, Research opportunities, Career fairs, and Computer Science overlap look like in both schools? Which one would set me up for the most success in STEM?",
    "url": "https://www.reddit.com/r/data/comments/1klx9he/ut_statistics_and_data_science_or_uwashington/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1747169973.0,
    "author": "Long-Competition1260",
    "subreddit": "data",
    "permalink": "/r/data/comments/1klx9he/ut_statistics_and_data_science_or_uwashington/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1klloaq",
    "title": "Data Privacy in Trump 2.0 and LGBTQ Rights: What You Need to Know",
    "selftext": "# Americans are “constantly shedding data.” What does that mean for LGBTQ people under the current administration?",
    "url": "https://www.unclosetedmedia.com/p/data-privacy-in-trump-20-and-lgbtq",
    "score": 0,
    "upvote_ratio": 0.33,
    "num_comments": 1,
    "created_utc": 1747142185.0,
    "author": "UnclosetedMedia",
    "subreddit": "data",
    "permalink": "/r/data/comments/1klloaq/data_privacy_in_trump_20_and_lgbtq_rights_what/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "ms34e0r",
        "body": "For those interested, Uncloseted Media is a recently-launched investigative news publication focused on examining the anti-LGBTQ ecosystem in the U.S. while amplifying LGBTQ stories and voices. You can learn more and subscribe for free at [https://www.unclosetedmedia.com/](https://www.unclosetedmedia.com/)",
        "score": 1,
        "created_utc": 1747142201.0,
        "author": "UnclosetedMedia",
        "is_submitter": true,
        "parent_id": "t3_1klloaq",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1klhr98",
    "title": "Stuck after labelling dataset with roboflow.",
    "selftext": "we are a group of students working on our bachelors thesis. for this we are using yolov9 and have annotated our dataset which consists of 27.8k images using roboflow's auto label. as we are students and have limited financial resources, we used 11 different roboflow account to breakdown our dataset for the autolabel process since our free plan only allows 30credits per workspace which uses 100 images for 1 credit. our mistake was we didnt know that generating the annotated dataset will also cost credits and have used up all the credits from the accounts we created. no idea how to navigate from here on and we cant label 27.8k images manually as we dont have much time and cant even change our topic now or use a smaller dataset as we are building an ensemble model with yolov9 and efficientNetb7 which requires large dataset. if somebody could please help us out urgently it would be great. if this sub is also not the right fit for this post directing towards a more relevant one would also be a huge help.thanks",
    "url": "https://www.reddit.com/r/data/comments/1klhr98/stuck_after_labelling_dataset_with_roboflow/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1747129103.0,
    "author": "Niaz_uix",
    "subreddit": "data",
    "permalink": "/r/data/comments/1klhr98/stuck_after_labelling_dataset_with_roboflow/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "ms5wlx5",
        "body": "We give free credits for academic projects: [https://roboflow.com/contribute#details-research](https://roboflow.com/contribute#details-research)",
        "score": 1,
        "created_utc": 1747171790.0,
        "author": "aloser",
        "is_submitter": false,
        "parent_id": "t3_1klhr98",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1kk953s",
    "title": "What is this graph called and how do I create it?",
    "selftext": "(picture relevant)  \nI stumbled across this very fancy looking graph and do only know it as a \"Schemaball\" and fell in love.  \nDoes anyone know if it has another name? I want to create one for myself from a covariance matrix, but can not find a lot of resources.\n\nhttps://preview.redd.it/0ymdgtmgf70f1.jpg?width=800&format=pjpg&auto=webp&s=b3f88c4d55d64c7881a31296fad7185ff8c5803d\n\n",
    "url": "https://www.reddit.com/r/data/comments/1kk953s/what_is_this_graph_called_and_how_do_i_create_it/",
    "score": 3,
    "upvote_ratio": 0.8,
    "num_comments": 3,
    "created_utc": 1746991713.0,
    "author": "DrunkWithAnUzi",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kk953s/what_is_this_graph_called_and_how_do_i_create_it/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mrt2kko",
        "body": "That’s a radial network diagram. Or a chord diagram. Some more info if you’re comfortable with R: https://jokergoo.github.io/circlize_book/book/the-chorddiagram-function.html",
        "score": 3,
        "created_utc": 1746996148.0,
        "author": "Thiseffingguy2",
        "is_submitter": false,
        "parent_id": "t3_1kk953s",
        "depth": 0
      },
      {
        "id": "mrvnigc",
        "body": "https://observablehq.com/@d3/hierarchical-edge-bundling/2",
        "score": 3,
        "created_utc": 1747036143.0,
        "author": "bic-boy",
        "is_submitter": false,
        "parent_id": "t3_1kk953s",
        "depth": 0
      },
      {
        "id": "mrtibvu",
        "body": "This is perfect, thank you so much! ",
        "score": 3,
        "created_utc": 1747001469.0,
        "author": "DrunkWithAnUzi",
        "is_submitter": true,
        "parent_id": "t1_mrt2kko",
        "depth": 1
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1kihaly",
    "title": "How Do You Handle Massive Datasets? What’s Your Stack and How Do You Scale?",
    "selftext": "Hi everyone!  \nI’m a product manager working with a team that recently started dealing with datasets in the tens of millions of rows—think user events, product analytics, and customer feedback. Our current tooling is starting to buckle under the load, especially when it comes to real-time dashboards and ad hoc analyses.\n\nI’m curious:\n\n* **What’s your current stack for storing, processing, and analyzing large datasets?**\n* **How do you handle scaling as your data grows?**\n* **Any tools or practices you’ve found especially effective (or surprisingly expensive)?**\n* **Tips for keeping costs under control without sacrificing performance?**\n\n",
    "url": "https://www.reddit.com/r/data/comments/1kihaly/how_do_you_handle_massive_datasets_whats_your/",
    "score": 5,
    "upvote_ratio": 0.86,
    "num_comments": 7,
    "created_utc": 1746793828.0,
    "author": "Ambrus2000",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kihaly/how_do_you_handle_massive_datasets_whats_your/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mrf8p8k",
        "body": "For speed at scale, consider using a real-time analytical database like ClickHouse, Druid, Pinot, or StarRocks.\n\nhttps://www.rilldata.com/blog/scaling-beyond-postgres-how-to-choose-a-real-time-analytical-database",
        "score": 3,
        "created_utc": 1746800182.0,
        "author": "No_Money_6221",
        "is_submitter": false,
        "parent_id": "t3_1kihaly",
        "depth": 0
      },
      {
        "id": "mrifcma",
        "body": "what's your tooling, tens of millions isn't a big deal for most databases, try duckdb or asking in r/dataengineering",
        "score": 3,
        "created_utc": 1746836844.0,
        "author": "thinkingatoms",
        "is_submitter": false,
        "parent_id": "t3_1kihaly",
        "depth": 0
      },
      {
        "id": "mrlaval",
        "body": "Use Clickhouse and be smart about encoding, low cardinality strings, numbers with delta encoding, where appropriate store floats as integers with fixed point notation and choose a PK that stores correlated values close together.",
        "score": 2,
        "created_utc": 1746886364.0,
        "author": "ElPeque222",
        "is_submitter": false,
        "parent_id": "t3_1kihaly",
        "depth": 0
      },
      {
        "id": "mrmm0co",
        "body": "I do data engineering for a fortune 100 company. We use Qlik, I've done several apps with 10's only millions of rows and the software handles it really well.",
        "score": 2,
        "created_utc": 1746901850.0,
        "author": "FlerisEcLAnItCHLONOw",
        "is_submitter": false,
        "parent_id": "t3_1kihaly",
        "depth": 0
      },
      {
        "id": "mrnjiyw",
        "body": "If U can afford BigQuery",
        "score": 2,
        "created_utc": 1746913475.0,
        "author": "NetZealousideal5466",
        "is_submitter": false,
        "parent_id": "t3_1kihaly",
        "depth": 0
      },
      {
        "id": "mrwdpqx",
        "body": "Thank you for the comments, we are considering to try Clickhouse for their scaling features, any experience with analytics tools working with Clickhouse?",
        "score": 1,
        "created_utc": 1747051060.0,
        "author": "Ambrus2000",
        "is_submitter": true,
        "parent_id": "t3_1kihaly",
        "depth": 0
      },
      {
        "id": "mrwdl2o",
        "body": "Thanks, what about the analytics tools, I mean which you use for Clickhouse?",
        "score": 1,
        "created_utc": 1747051004.0,
        "author": "Ambrus2000",
        "is_submitter": true,
        "parent_id": "t1_mrf8p8k",
        "depth": 1
      }
    ],
    "comments_extracted": 7
  },
  {
    "id": "1ki2k18",
    "title": "How to remove personal data off the Internet.",
    "selftext": "I've been online since I was 6 and have recently become aware of just how much of my private personal data is floating around out there. \n\nIs there any way for me to find out about and wipe my personal data? ",
    "url": "https://www.reddit.com/r/data/comments/1ki2k18/how_to_remove_personal_data_off_the_internet/",
    "score": 9,
    "upvote_ratio": 1.0,
    "num_comments": 5,
    "created_utc": 1746741711.0,
    "author": "EzBriez_",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ki2k18/how_to_remove_personal_data_off_the_internet/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mrbn31w",
        "body": "Depends on where you live and what data you mean. Lots of legal and financial records, no. Social media, maybe. If you’re in the US it’s state by state. If you’re in the EU , yes. Look up Right to Delete laws for your region. Then you get to go company by company asking them to do it. There are also companies that will sell you their services to do so on your behalf. I think one is called deleteMe but do some homework. Not all of them actually do much.",
        "score": 2,
        "created_utc": 1746742443.0,
        "author": "amosmj",
        "is_submitter": false,
        "parent_id": "t3_1ki2k18",
        "depth": 0
      },
      {
        "id": "mrdi4wp",
        "body": "You can't. End. But you can harass orgs to do it, which is also valuable.",
        "score": 2,
        "created_utc": 1746769289.0,
        "author": "Alternative_Top2875",
        "is_submitter": false,
        "parent_id": "t3_1ki2k18",
        "depth": 0
      },
      {
        "id": "mrliuqp",
        "body": "[https://redact.dev/](https://redact.dev/) to wipe out social media data, anything other data you have to submit a request with the website you found info on.",
        "score": 1,
        "created_utc": 1746889049.0,
        "author": "LordNikon2600",
        "is_submitter": false,
        "parent_id": "t3_1ki2k18",
        "depth": 0
      },
      {
        "id": "mrmka92",
        "body": "If you live in the US, and not California, you’re more or less boned. \n\nNo data protections here.",
        "score": 1,
        "created_utc": 1746901266.0,
        "author": "ComprehensiveCat7515",
        "is_submitter": false,
        "parent_id": "t3_1ki2k18",
        "depth": 0
      },
      {
        "id": "ms1j0j4",
        "body": "A lot of individual sites have opt out forms if you find your data on them. The goal is to go through each one by one. It sounds like a lot but if you do one a day, it isn't bad.\nPlus there are some hoops you can go through with Google to get search results removed if your info is in them. \nRemember to set your accounts to private before you have search results removed though because web crawlers index the information for search results regularly.\nTurn off cookie preferences when websites ask. Idk how much this helps but it makes me feel better",
        "score": 1,
        "created_utc": 1747112747.0,
        "author": "thaliasnow",
        "is_submitter": false,
        "parent_id": "t3_1ki2k18",
        "depth": 0
      }
    ],
    "comments_extracted": 5
  },
  {
    "id": "1kico1u",
    "title": "Updating companies database based on M&A",
    "selftext": "Hi Folks,\n\nMy friend's company has a database of around \\~100,000 companies across globe and those companies have their associate ultimate owners. e.g. Apple UK, Apple India, Apple Brazil would have their ultimate owner has Apple. He wants to update the database on a monthly basis based on the M&A happening. He has not updated the data for the last 2-3 years thus all the previous mergers and acquisitions have not updated yet.\n\nWhat would be the way to update the onwership of the company? e.g. one year ago Apple Brazil was bought by Samsung thus it's onwer should be updated to Samsung from Apple.\n\nCould you please recommend the solution and way he can work?",
    "url": "https://www.reddit.com/r/data/comments/1kico1u/updating_companies_database_based_on_ma/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 0,
    "created_utc": 1746775557.0,
    "author": "tata_bye_bye_",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kico1u/updating_companies_database_based_on_ma/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1khbs9l",
    "title": "Final interview with 2 Managers after interview with... 2 MANAGERS (yeah, it's right)",
    "selftext": "Guys, i'm doing a selection process for a position of intern e i arrived too far. it's a big multinational and after HR, 2 managers (Still data sector) interview, technical test, here it comes the final interview with... 2 MANAGERS (Still on the data sector) on the same company. I have some guesses about what could be this final interview but i'm not sure yet. Can you guys advice me, please?",
    "url": "https://www.reddit.com/r/data/comments/1khbs9l/final_interview_with_2_managers_after_interview/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1746660398.0,
    "author": "Character-Tax6241",
    "subreddit": "data",
    "permalink": "/r/data/comments/1khbs9l/final_interview_with_2_managers_after_interview/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1kg0lnh",
    "title": "MCP Servers",
    "selftext": "",
    "url": "https://mcp.so/server/dingo-mcp-server/DataEval?tab=comments",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1746525274.0,
    "author": "chupei0",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kg0lnh/mcp_servers/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1keyod3",
    "title": "Do folks face the issues in finding the right metadata? What are some existing solutions used in your workplace for the same?",
    "selftext": "Hey Data community!\n\nI have been working in the data analytics space for the past 8+ years and one thing that I have struggled with consistently across the various teams and companies I have worked in is, the ability to find the data definitions, metric definitions when I need them. I have to reach out to several people or look through various sets of documentation to find the relevant information. I was curious if other people in this community have faced this challenge as well. If yes, then how do you solve this currently? Are there any tools you use in your current company to solve for this?\n\nThanks all!",
    "url": "https://www.reddit.com/r/data/comments/1keyod3/do_folks_face_the_issues_in_finding_the_right/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1746404724.0,
    "author": "Frequent_Movie_4170",
    "subreddit": "data",
    "permalink": "/r/data/comments/1keyod3/do_folks_face_the_issues_in_finding_the_right/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mqmtatn",
        "body": "Having a metadata team that uses centralized storage and tracking is a big deal. The initial investment can be a challenge depending on the current state of things, but you don’t necessarily need big fancy systems solutions. It is probably most valuable to convince your leadership that centralized ownership and management of metadata will save development time and reduce errors/rework (and therefore money), and then put a team together to create a strategy.",
        "score": 2,
        "created_utc": 1746407572.0,
        "author": "Dataphiliac",
        "is_submitter": false,
        "parent_id": "t3_1keyod3",
        "depth": 0
      },
      {
        "id": "mqn90dh",
        "body": "Agreed. In my current organization, there is a team that owns the portal for a centralized data catalog. However, that catalog still needs to be updated by the data engineers with metric/column description documentation which is a manual process and thus the catalog is'nt regularly updated.",
        "score": 1,
        "created_utc": 1746413425.0,
        "author": "Frequent_Movie_4170",
        "is_submitter": true,
        "parent_id": "t1_mqmtatn",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1key3u4",
    "title": "Monetizing data generation on digital networks",
    "selftext": "Information is reproducible and non-rival. So digital networks naturally permit many-to-many connections (i.e. follows, friends, subscribes...). Every connection is economic. Today we do not measure >90% of the economic activity that occurs on high-connectivity networks. Most of what is monetized is aggregated consumer data at the enterprise level. \n\nThe consumer is left out of the financial value they contribute to networks. \n\nSo I created a CSX Protocol that allocates 100 CSX credits across the accounts you follow each week. Follow 20 accounts? Great, then each will receive 5 CSX credits from you on Sunday night. This occurs every week. Authorized data drives USD income that is then used to buy back CSX credits from users in the system. \n\nI believe this is the future way to create 10X and more value of data. What do you think?",
    "url": "https://www.reddit.com/r/data/comments/1key3u4/monetizing_data_generation_on_digital_networks/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1746403000.0,
    "author": "Jbassiri",
    "subreddit": "data",
    "permalink": "/r/data/comments/1key3u4/monetizing_data_generation_on_digital_networks/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1keifm1",
    "title": "hello i have a problem",
    "selftext": "i have a 172gb folder that i want to extract to my ssd (z has 229gb) my other ssd has (c 112gb)\n\nand (d 39gb where the folder is) how do i extract that file.",
    "url": "https://www.reddit.com/r/data/comments/1keifm1/hello_i_have_a_problem/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1746360943.0,
    "author": "supatop4eta",
    "subreddit": "data",
    "permalink": "/r/data/comments/1keifm1/hello_i_have_a_problem/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1kem4z9",
    "title": "DA/DE/DS - How important is a degree/cert? (BKG - Non CSE)",
    "selftext": "Hi all! I am a working professional in automotive manufacturing with 3 years of experience who wants to transit his career into data related roles. I have a few questions. It would be really helpful if you can enlighten me with your experience in the field.\n\n1. How much are the chances of a person like me to get into this field who is from a totally different industry? Ik it's all about skills but iykwm like even the screening process for example\n2. How important does it get to have a degree/certificate (in CSE or Data Science)?\n3. Any tips on how to show my experience as a manufacturing engineer for a data analyst job role?\n\nPardon me if my queries sound annoying. I am confused and need guidance.",
    "url": "https://www.reddit.com/r/data/comments/1kem4z9/dadeds_how_important_is_a_degreecert_bkg_non_cse/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1746371621.0,
    "author": "userishighaf",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kem4z9/dadeds_how_important_is_a_degreecert_bkg_non_cse/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1kdpmo2",
    "title": "How to get in to data field after completing Masters in Data Science as an international student in Australia?",
    "selftext": "",
    "url": "https://www.reddit.com/r/data/comments/1kdpmo2/how_to_get_in_to_data_field_after_completing/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1746267199.0,
    "author": "Certain_Board7865",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kdpmo2/how_to_get_in_to_data_field_after_completing/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1kcmoxk",
    "title": "Supercharge your R workflows with DuckDB",
    "selftext": "",
    "url": "https://borkar.substack.com/p/r-workflows-with-duckdb?r=2qg9ny",
    "score": 2,
    "upvote_ratio": 0.75,
    "num_comments": 0,
    "created_utc": 1746141714.0,
    "author": "Capable-Mall-2067",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kcmoxk/supercharge_your_r_workflows_with_duckdb/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1kbmr1k",
    "title": "Indeed jobs data?",
    "selftext": "Hi - Anyone work with jobs data from indeed or linkedin? I am currently working with indeed data, and using O\\*NET classifcation to parse job titles into O\\*NET categories, and then into O\\*NET job zones - which is basically a proxy for seniority level, with higher zones being more senior jobs. However, when I aggregate the data and plot on a monthly basis, there are weird peaks in the data. I expect some seasonality in hiring, but this seems weird.\n\nI want to know if others who work with this kind of data have encountered this or what could be causing this?\n\nhttps://preview.redd.it/639je2xif0ye1.jpg?width=1388&format=pjpg&auto=webp&s=036266cd78376b7b76598d3b7ef5d788272b9b20",
    "url": "https://www.reddit.com/r/data/comments/1kbmr1k/indeed_jobs_data/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1746035429.0,
    "author": "StarBaker9",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kbmr1k/indeed_jobs_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1kartei",
    "title": "Need help building a dashboard",
    "selftext": "https://preview.redd.it/e8glqchatsxe1.png?width=1390&format=png&auto=webp&s=e75eb2850ba582c353378a53577dabee5972973c\n\nI want to build a dashboard similar to this. How can I do it?",
    "url": "https://www.reddit.com/r/data/comments/1kartei/need_help_building_a_dashboard/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1745943147.0,
    "author": "Electrical_Sir_9434",
    "subreddit": "data",
    "permalink": "/r/data/comments/1kartei/need_help_building_a_dashboard/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ka75up",
    "title": "Aspiring Data Analyst",
    "selftext": "Hello, I am International Relations student, MA, security policy. I love what I study and I would like to strengthen my portfolio with quantitative skills, which are not really taught intensely by Social Sciences degrees. I am interested in Data Analytics. I dont have tech/comp science background. Is it possible to learn it by myself? I would like to be on good level in 1,5 years or so , by the time i graduate. What can i do? what to focus on? which skills are most relevant to my degree? i really appreciate your help along with my first steps in data world ",
    "url": "https://www.reddit.com/r/data/comments/1ka75up/aspiring_data_analyst/",
    "score": 2,
    "upvote_ratio": 0.67,
    "num_comments": 7,
    "created_utc": 1745875785.0,
    "author": "Warm_Bridge6806",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ka75up/aspiring_data_analyst/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mplhbv7",
        "body": "Simplest would be to pick courses on Coursera and try platforms like Upwork to get real projects. Try to get a reference into an internship to enter the field somehow (even at low pay kind of a thing). ChatGPT should help a lot along the way but one main idea is grit!",
        "score": 1,
        "created_utc": 1745895272.0,
        "author": "MrBarret63",
        "is_submitter": false,
        "parent_id": "t3_1ka75up",
        "depth": 0
      },
      {
        "id": "mpp1du1",
        "body": "Data Camp.",
        "score": 1,
        "created_utc": 1745948274.0,
        "author": "Extension_Dog_7867",
        "is_submitter": false,
        "parent_id": "t3_1ka75up",
        "depth": 0
      },
      {
        "id": "mptugt4",
        "body": "I regularly post about this on my LinkedIn. Don't wanna spam here. So if interested let me know I'll share the links",
        "score": 1,
        "created_utc": 1746016030.0,
        "author": "UpbeatSeaweed1305",
        "is_submitter": false,
        "parent_id": "t3_1ka75up",
        "depth": 0
      },
      {
        "id": "mrgjyvz",
        "body": "Yes, it's absolutely possible to learn data analytics on your own, even without a tech background. I'm on a similar path, and I’ve found that the most important shift is learning how to think in data not just learning tools, but learning to see patterns, ask better questions, and break down problems logically.\n\nOne thing that helped me early on was using a Notion template to build that kind of mindset practicing small projects, tracking questions I wanted to answer, and getting used to working through ideas like an analyst. If it’s helpful, here’s the one I used: [https://www.notion.so/marketplace/creators/malavica](https://www.notion.so/marketplace/creators/malavica)\n\nIt’s totally possible to build a strong skill set within 1.5 years. Just staying consistent and tying what you learn to your interests in security or policy makes all the difference. Wishing you the best in this journey you’re asking all the right questions already!",
        "score": 1,
        "created_utc": 1746814558.0,
        "author": "mademoiselle_made",
        "is_submitter": false,
        "parent_id": "t3_1ka75up",
        "depth": 0
      },
      {
        "id": "mq13hwf",
        "body": "please do",
        "score": 1,
        "created_utc": 1746112061.0,
        "author": "Warm_Bridge6806",
        "is_submitter": true,
        "parent_id": "t1_mptugt4",
        "depth": 1
      },
      {
        "id": "mudy96d",
        "body": "thank u so much for your anwer, i truly appreciate it",
        "score": 1,
        "created_utc": 1748285293.0,
        "author": "Warm_Bridge6806",
        "is_submitter": true,
        "parent_id": "t1_mrgjyvz",
        "depth": 1
      }
    ],
    "comments_extracted": 6
  },
  {
    "id": "1k9nwmt",
    "title": "Need help understanding what tests to use",
    "selftext": "\nI am really lost at understanding which tests to use when looking at my data sample for a university practice report. I know roughly how to perform tests in R but knowing what ones to use in this instance really confuses me.\n\nThey have given use 2 sets of before and after for a test something like this:\nTest values are given on a scale of 1-7\n\nTest 1\nID 1-30 | Before | After |\n\nTest 2\nID 31-60 | Before | After |\n\n(not going to input all the values)\n\nMy thinking is that I should run 2 different paired tests as the factors are dependent but then I am lost at comparing Test 1 and 2 to each other.\n\nShould I perhaps calculate the differences between before and after for each ID and then run nonpaired t-test to compare Test 1 to Test 2? My end goal is to see which test has the higher result (closer to 7).\n\nBecause there are only 2 groups my understanding is that I shouldnt use ANOVA?\n\nThank you,",
    "url": "https://www.reddit.com/r/data/comments/1k9nwmt/need_help_understanding_what_tests_to_use/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 6,
    "created_utc": 1745817406.0,
    "author": "ThreeDogsInAJar",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k9nwmt/need_help_understanding_what_tests_to_use/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mpg61d0",
        "body": "I'd check a flow chart by typing \"which statistical test to use\" on Google images (I tried sharing a link but it was removed by mods...). Lmk if you are still unsure after seeing that (perhaps you are confused about the experimental set-up?). I'll do my best to help although I'm still learning.",
        "score": 1,
        "created_utc": 1745826521.0,
        "author": "berryboi23",
        "is_submitter": false,
        "parent_id": "t3_1k9nwmt",
        "depth": 0
      },
      {
        "id": "mpgd6ou",
        "body": "I’ve looked at a few of these flow charts and sometimes come about different answers. \n\nI really appreciate your response, I’m thinking that I have to do both a paired t test to compare the first test (paired before and after for test 1) and again another paired for the second test… but the lecturer has told others that did it this way that we are then not comparing the two tests to each other. Which is where I am lost… \n\nMaybe I should adjust the data so I can make it an unpaired t test (calculate the differences between before and after for both Test 1 and test 2) and then do a T test on that ? \n\nI’m sorry if none of this makes sense, I wish I could share images in these comments but I can’t figure out how 😢",
        "score": 1,
        "created_utc": 1745831238.0,
        "author": "ThreeDogsInAJar",
        "is_submitter": true,
        "parent_id": "t1_mpg61d0",
        "depth": 1
      },
      {
        "id": "mpic8yx",
        "body": "i think you're mostly on track, you just need to clarify a couple things. \n\nAre you comparing which group improved more (after - before), or just which group had higher post-test scores? If it's improvement, calculate the difference for each person and compare those using an independent t-test (or Mann-Whitney if non-normal). For within-group changes, a paired t-test makes sense since it's repeated measures.\n\nAlso, have you checked if your difference scores are normally distributed? T-tests assume normality—if that doesn’t hold, switch to non-parametric tests (like a Mann-Whitney). \n\nAlso, you're working with a 1-7 Likert scale, are you confident treating it as interval data rather than ordinal? If not, better to stick with non-parametric methods.",
        "score": 1,
        "created_utc": 1745858732.0,
        "author": "berryboi23",
        "is_submitter": false,
        "parent_id": "t1_mpgd6ou",
        "depth": 2
      },
      {
        "id": "mplm11p",
        "body": "So… I’m actually doing both… \n\nI have the same 1-30 people (before and after taking test 1) and 31-60 group (before and after taking test 2) and a total of 3 datasets \n\nThe report is on whether test 1 or test 2 makes a difference on 3 factors\n\nThe first 2 factors use a likert scale and the 3rd factor is a test percentage result out of 100. \n\nSo it’s like \nFactor 1 \nID | Before Test | After Test \n\nFactor 2\nID | before | after \n\nEtc \n\n\nThe professor said we can assume equal variance and has mainly taught us t tests with 1 example of an ANOVA. I’ve been teaching myself extensions of these, and trying to learn how to know when to use Mann-Whitney/ Wilcox/ etc) \n\ndo you mind if I message you to send you photos of the data / the tests I ran to see if they make sense?",
        "score": 1,
        "created_utc": 1745897024.0,
        "author": "ThreeDogsInAJar",
        "is_submitter": true,
        "parent_id": "t1_mpic8yx",
        "depth": 3
      },
      {
        "id": "mplr1pj",
        "body": "Can I use a one way anova test on all 3 sets of data despite two of them using a likert scale and one of them using a quiz percentage?",
        "score": 1,
        "created_utc": 1745899080.0,
        "author": "ThreeDogsInAJar",
        "is_submitter": true,
        "parent_id": "t1_mpic8yx",
        "depth": 3
      }
    ],
    "comments_extracted": 5
  },
  {
    "id": "1k9gg6q",
    "title": "Question regarding OECD datasets",
    "selftext": "How do you guys find data before the 2000's in the oecd database? [OECD tax database](https://data-explorer.oecd.org/vis?fs[0]=Topic%2C1%7CTaxation%23TAX%23%7CPersonal%20and%20property%20tax%23TAX_PPT%23&pg=0&fc=Topic&bp=true&snb=12&df[ds]=dsDisseminateFinalDMZ&df[id]=DSD_TAX_PIT%40DF_PIT_TOP_EARN_THRESH&df[ag]=OECD.CTP.TPS&df[vs]=1.0&dq=.A....S13......&lom=LASTNPERIODS&lo=1&to[TIME_PERIOD]=false) only has 2000 and onwards. Thanks! \n\n",
    "url": "https://www.reddit.com/r/data/comments/1k9gg6q/question_regarding_oecd_datasets/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1745793198.0,
    "author": "LudvigN",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k9gg6q/question_regarding_oecd_datasets/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1k933m3",
    "title": "Science & Engineering publication, by selected region, country, or country and rest of word: 2003 - 2022. Total worldwide Science & Engineering publication output reached 3.3 million articles in 2022, based on entries in the Scopus database.",
    "selftext": "\\*The figure shows total number of publications per year.\n\nI find it quite interesting how the pace of growing number of publications increased from 2018. ",
    "url": "https://i.redd.it/e8d1ag70idxe1.png",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1745757836.0,
    "author": "Fit_Ad3058",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k933m3/science_engineering_publication_by_selected/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1k8jgh2",
    "title": "Can you please provide the source for movie database.",
    "selftext": "The database should include title, release year, run time, gener, overview, imdb rating, and poster link or image source for every movie. \nI need both m movies and tv series.",
    "url": "https://www.reddit.com/r/data/comments/1k8jgh2/can_you_please_provide_the_source_for_movie/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 1,
    "created_utc": 1745691452.0,
    "author": "Vegetable_Salt_6399",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k8jgh2/can_you_please_provide_the_source_for_movie/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mrglgzx",
        "body": "try here you will good datasets for a good practice. [https://www.kaggle.com/search?q=movie+in%3Adatasets](https://www.kaggle.com/search?q=movie+in%3Adatasets) try these template to think in data to solve the problems with data completely beginner friendly and no prior knowledge required [https://www.notion.so/marketplace/creators/malavica](https://www.notion.so/marketplace/creators/malavica)",
        "score": 1,
        "created_utc": 1746815015.0,
        "author": "mademoiselle_made",
        "is_submitter": false,
        "parent_id": "t3_1k8jgh2",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1k7z1jc",
    "title": "Error bars do not align with values from table (unless I don't understand how error bars work)",
    "selftext": "For an assessment, I have error bars where the first and second points do not overlap, and the second and third points do. No big deal. However, when I go to talk about error bars using specific values from the table, it does not add up.\n\nFor example, for datapoints one and do, with error bars that do not overlap the maximum value of the first datapoint is 73.6, and the minimum value of the second datapoint is 73.264 and 73.264<73.6 so should they not overlap?\n\nThe same issue occurs with the second and third datapoints, on the graph the error bars were overlapping, but the maximum value of datapoint 2 was 78.299 and the minimum value of datapoint 3 was 78.61 and 78.61>78.299 so why are they overlapping?\n\nUncertainty was calculated using (max-min)/2\n\nAm I misunderstanding what the error bars show? If so what am I supposed to talk about?\n\nI will attach the data but it won't let me attach 2 images so you'll just have to trust me about the overlap.\n\nhttps://preview.redd.it/antlvawpf2xe1.png?width=446&format=png&auto=webp&s=770540713f688df562926fd59cb2024edcf7379b\n\n  \n\n\nPoints that are highlighted and that have an astrix indicates an outlier was detected or used in a calculation. You do not need to worry about these as the graph does not use these values.\n\n",
    "url": "https://www.reddit.com/r/data/comments/1k7z1jc/error_bars_do_not_align_with_values_from_table/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1745623794.0,
    "author": "Neat_Historian2393",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k7z1jc/error_bars_do_not_align_with_values_from_table/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1k7fo73",
    "title": "Decompose function in R",
    "selftext": "Hello, \n\nSorry I am a new member in reddit and i dont know so much about it but because chatgpt told me that i finished my free trial until 13.56 i need to ask you about smth. Now I am doing a homework about data analysis and finance , and the thing is while looking decomposed time series plot in R teacher asked us about is its stationary or not. And i am not very sure to look , if im not wrong stationarity basically means that time series evolves almost same in the given time and if we dont have stationarity then we cant exactly predicy what will going to happen in the future, so we cant perform forecast. And to have stationarity we need to have constant mean,variance and covarience over time. So in R decomposed plot, where should I look? I think it should be \"random\" but i am not very sure about that. Thank you.",
    "url": "https://www.reddit.com/r/data/comments/1k7fo73/decompose_function_in_r/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1745568702.0,
    "author": "artvin_sevdam08",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k7fo73/decompose_function_in_r/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1k67qdl",
    "title": "Textbooks for multivariate data analysis",
    "selftext": "I would like to get a few recommendations on good multivariate analysis books. In particular, I would be interested in both mathematical and non-mathematical heavy ones so I can gradually deepen my knowledge.   \nWhat would be your suggestions? ",
    "url": "https://www.reddit.com/r/data/comments/1k67qdl/textbooks_for_multivariate_data_analysis/",
    "score": 3,
    "upvote_ratio": 0.81,
    "num_comments": 2,
    "created_utc": 1745435806.0,
    "author": "the_lost_interleukin",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k67qdl/textbooks_for_multivariate_data_analysis/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mph0wu1",
        "body": "Applied Multivariate Statistical Analysis by Härdle & Simar, first chapter provides a brief overview of matrix algebra you might find helpful (check out 3Blue1Brown on YouTube as well)",
        "score": 2,
        "created_utc": 1745843537.0,
        "author": "stuart_pickles",
        "is_submitter": false,
        "parent_id": "t3_1k67qdl",
        "depth": 0
      },
      {
        "id": "mov2sm5",
        "body": "*An Introduction to Multivariate Statistical Analysis* by Anderson, is a rigorous book.",
        "score": 1,
        "created_utc": 1745530606.0,
        "author": "axiom_tutor",
        "is_submitter": false,
        "parent_id": "t3_1k67qdl",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1k6bko3",
    "title": "Vehicle sale data",
    "selftext": "I had an interesting idea for a chart for the r/dataisbeautiful subreddit, but I need sales numbers for all (or at least most) vehicles sold in the US broken down by year and model (and ideally trim but that's not really necessary)\n\nI've had a really hard time finding anything other than like a top 25 list. Any help would be appreciated ",
    "url": "https://www.reddit.com/r/data/comments/1k6bko3/vehicle_sale_data/",
    "score": 2,
    "upvote_ratio": 0.75,
    "num_comments": 3,
    "created_utc": 1745445282.0,
    "author": "jack_mohat",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k6bko3/vehicle_sale_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "moovkhw",
        "body": "Some of this might be available in auto manufacturer's earnings reports but I wouldn't really expect it to all live in one place.",
        "score": 1,
        "created_utc": 1745447923.0,
        "author": "murdercat42069",
        "is_submitter": false,
        "parent_id": "t3_1k6bko3",
        "depth": 0
      },
      {
        "id": "mpu400z",
        "body": "I run a data company, I can definitely help with sourcing and getting this data for you!",
        "score": 1,
        "created_utc": 1746019386.0,
        "author": "Emotional_Corner_467",
        "is_submitter": false,
        "parent_id": "t3_1k6bko3",
        "depth": 0
      },
      {
        "id": "mt4jhso",
        "body": "Have you had a look at the open data marketplaces online? I am also looking for different types of datasets for a university project. I found some useful datasets on Opendatabay (https://www.opendatabay.com/)",
        "score": 1,
        "created_utc": 1747664431.0,
        "author": "Fun_Plum_1526",
        "is_submitter": false,
        "parent_id": "t3_1k6bko3",
        "depth": 0
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1k63cdl",
    "title": "We added keyword intent segmentation to our Looker Studio SEO dashboard. Would love your feedback before we release it",
    "selftext": "Hi everyone! 👋\n\nLast week we shared a Google Search Console dashboard here, and someone asked if we could segment keywords by **intent:** Commercial, Transactional, Informational, and Navigational.\n\nWe thought that was a great idea. So we built it.\n\nTo make it work, we manually categorized over **450 keywords and root patterns across the four intent** types. This gives the dashboard the ability to classify queries based on the language users are actually using.\n\n[Search Intent Dashboard](https://lookerstudio.google.com/u/0/reporting/50caf700-b242-47d6-95ba-daff69dacc13/page/p_2lbzu3dtrd)\n\nThe result: a new version of the dashboard with an intent breakdown built into the Keyword Analysis page.\n\n\n\n🟠 You can also connect your own GSC property via the orange dropdown (top-right), so you can test it live with your real data. Not just a demo.\n\nNow here’s where we need your help:\n\n* Does the segmentation feel accurate to you?\n* Would you change the way it’s visualized?\n* Is anything important missing?\n\nThis isn’t powered by AI. It’s rule-based logic with lots of manual refinement, so we’re very open to making it better.\n\nIf enough people find it useful, we’ll clean it up and make it public next week. Happy to answer any questions in the comments!",
    "url": "https://www.reddit.com/gallery/1k63cdl",
    "score": 2,
    "upvote_ratio": 0.75,
    "num_comments": 0,
    "created_utc": 1745425280.0,
    "author": "kodalogic",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k63cdl/we_added_keyword_intent_segmentation_to_our/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1k5r1zb",
    "title": "Canadians water use during four nations final",
    "selftext": "I have been looking for a graph I saw a few months ago. It was of the water use from Canadians during the second US vs Canada, with an overlay of when the periods end. It showed that people all waited to use the toilet until intermission, and I was trying to find it to show my friend but came up empty. If any of you know what I’m talking about, I’d greatly appreciate help!",
    "url": "https://www.reddit.com/r/data/comments/1k5r1zb/canadians_water_use_during_four_nations_final/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1745384248.0,
    "author": "Winter_Job2570",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k5r1zb/canadians_water_use_during_four_nations_final/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1k5dp3o",
    "title": "Are missing the boat?",
    "selftext": "SoShere's the situation.... a company in The Netherlands.\nCurrently using lots of oldfashioned applicaties build in Progress (Dos based), As400, c# applications that don't share anything in common like a database\ndatabase.\nAllso, in the middle of replacing the old applicaties for a more integrated one ( a slow and painfull projec)\nTrying to migrate data that is of poor quallity.\nNow, the management thinks we mis the boat on AI.\nFrom my point of view, as data engineer responsible for all that has to do with data, I think pur company is nowhere naar the use of AI for its business processen. We can use AI for improving data quality and stuff.\n\nThe management thinks otherwise. We neem to look and start working with AI.\n\nCurious ot you point of view in this, dear data brothers and sisters, follow data enthusiasts. \n\n",
    "url": "https://www.reddit.com/r/data/comments/1k5dp3o/are_missing_the_boat/",
    "score": 4,
    "upvote_ratio": 0.84,
    "num_comments": 9,
    "created_utc": 1745346612.0,
    "author": "SecretOfTheMoon",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k5dp3o/are_missing_the_boat/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "moh5lki",
        "body": "\\-garbage in, garbage out. a.i. won't fix that, but might help you to tell you where the garbage's coming from. \n\n  \nalso: what company? fellow dutchie here \\^\\^",
        "score": 5,
        "created_utc": 1745347255.0,
        "author": "henewie",
        "is_submitter": false,
        "parent_id": "t3_1k5dp3o",
        "depth": 0
      },
      {
        "id": "mohbd8o",
        "body": "In my opinion, first, data cleansing, which itself is a project and may be time consuming in your case because lot of decisions needs to be taken. You can use tools like OpenRefine (I personally never used it, but heard a lot). \n\nAnd then before taking AI flight you must need strong pipelines and centralised data warehouse or Datalake whichever suites you.\n\nI am also following the thread and will learn if someone will propose more efficient way to do that.\n\nThanks",
        "score": 3,
        "created_utc": 1745348969.0,
        "author": "hopon-tram",
        "is_submitter": false,
        "parent_id": "t3_1k5dp3o",
        "depth": 0
      },
      {
        "id": "moh4k22",
        "body": "I don't think so! If anything, it has gotten more affordable, faster, and more scalable to use it for business functions. Many of the early adopters were just fluff.",
        "score": 2,
        "created_utc": 1745346944.0,
        "author": "murdercat42069",
        "is_submitter": false,
        "parent_id": "t3_1k5dp3o",
        "depth": 0
      },
      {
        "id": "mouv6br",
        "body": "You’re not too late, many companies are still experimenting and looking for killer use cases. You and your architect team will need to spend time cleansing, standardising data and developing an architecture to support ML and AI models to consume your cleaned up data. Unfortunately all this necessary long pre-work is hard to justify to execs who want results now. You didn’t say whether your management have a clear strategy or just jumping on the AI bandwagon because it’s the latest hot thing, but having defined use cases that drive tangible value will help with getting management to fund the changes needed.",
        "score": 2,
        "created_utc": 1745528246.0,
        "author": "CarpeMentula",
        "is_submitter": false,
        "parent_id": "t3_1k5dp3o",
        "depth": 0
      },
      {
        "id": "mokj49v",
        "body": "Verbrugge International, Vlissingen",
        "score": 1,
        "created_utc": 1745391864.0,
        "author": "SecretOfTheMoon",
        "is_submitter": true,
        "parent_id": "t1_moh5lki",
        "depth": 1
      },
      {
        "id": "mokjajf",
        "body": "Thanks for your respons",
        "score": 2,
        "created_utc": 1745391972.0,
        "author": "SecretOfTheMoon",
        "is_submitter": true,
        "parent_id": "t1_mohbd8o",
        "depth": 1
      },
      {
        "id": "mokj751",
        "body": "Thanks for your respons",
        "score": 1,
        "created_utc": 1745391913.0,
        "author": "SecretOfTheMoon",
        "is_submitter": true,
        "parent_id": "t1_moh4k22",
        "depth": 1
      },
      {
        "id": "moxkpja",
        "body": "Thanks for your respons. There is no clear view on management leven on AI.\nThey hear and read about it, are approched by companies the can solve all out problems like magic. \n\nSo as you menrioned, I have to make a sollid foundation to support future ai initiatives",
        "score": 1,
        "created_utc": 1745565679.0,
        "author": "SecretOfTheMoon",
        "is_submitter": true,
        "parent_id": "t1_mouv6br",
        "depth": 1
      },
      {
        "id": "mp1xnw2",
        "body": "Yep…I think we all get a lot of AI consultancies cold-emailing us, promising a glowing future, how they’ll solve my customer service or customer personalisation issues with AI. The sales pitches conveniently gloss over the cross-departmental data architecture and technical debt that the DEs needs to fix first. If you don’t have one already, get a good senior sponsor who understands the challenges and can support you. In the meantime, you could try some simple proof of concepts that don’t require complete datasets, like chatbots, or basic narrow-focus recommender/propensity models. That will show visible progress while you frantically fix the data infrastructure 🙂",
        "score": 2,
        "created_utc": 1745621744.0,
        "author": "CarpeMentula",
        "is_submitter": false,
        "parent_id": "t1_moxkpja",
        "depth": 2
      }
    ],
    "comments_extracted": 9
  },
  {
    "id": "1k57aq5",
    "title": "Stats and visualizations from your Google Photos library",
    "selftext": "Hey everyone!\n\nJust wanted to share a little project I've been working on that might be interesting to folks here: [insights.photos](https://insights.photos/): a tool that creates stats and visualizations based on your Google Photos library.\n\nIt shows things like:\n\n* How many photos you’ve taken over time\n* Your most-used devices\n* Locations you photograph the most\n* Visual patterns across the years\n* And lots of other fun photo-related insights\n\nEverything is private, it connects securely to your Google account using the official API, processes the data in your browser/device, and nothing is stored on the server.\n\nI’ve been posting about it over on [r/googlephotos](https://www.reddit.com/r/googlephotos/), and the community there seems to really enjoy it, figured some of you here might like it too!\n\nEven though the Google Photos API was supposed to shut down on March 31, the tool is still working (surprisingly!), and I’ve recently increased the processing limit from 30,000 to 150,000 photos/videos.\n\nSo if you want to explore it in a new way, feel free to give it a try!\n\nHappy to answer any questions.",
    "url": "https://i.redd.it/8lwnzp4d9ewe1.png",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1745331098.0,
    "author": "6FG22222-22",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k57aq5/stats_and_visualizations_from_your_google_photos/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1k4fynd",
    "title": "Turning Google Search Console data into human-readable insights — has anyone else tried this approach?",
    "selftext": "I’ve been working with Google Search Console data for a while, mostly in Looker Studio, and one thing I kept noticing was how repetitive the analysis felt — every report came down to questions like:\n\n\n\n* Are we up or down compared to last month?\n* Which keywords are contributing most to change?\n* Is branded search growing or flat?\n* Any big shifts by device or location?\n\n\n\n\n\nTo reduce the cognitive load, I tried building what I call a “Smart Interpretations” layer into my dashboard. It’s basically a summary module with calculated fields and conditional logic that generates simple, human-readable statements like:\n\n\n\n* “Clicks are up 14%, impressions up 19% — good momentum.”\n* “Mobile CTR dropped 11% week-over-week, mostly on non-branded terms.”\n* “No major changes this period — performance is stable.”\n\n\n\n\n\nNo AI involved, just logic blocks that make it easier to scan trends at a glance. I find it helps a lot when monitoring multiple domains or reviewing performance across teams.\n\n\n\nJust curious — has anyone here experimented with similar methods for summarizing web performance data? Whether in Looker, Tableau, Power BI or something else?\n\n[Google Search Console Dashboard](https://lookerstudio.google.com/reporting/08f9db54-c062-4ca2-a92d-3512ccd36c28)",
    "url": "https://www.reddit.com/gallery/1k4fynd",
    "score": 5,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1745248352.0,
    "author": "kodalogic",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k4fynd/turning_google_search_console_data_into/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mof2jv5",
        "body": "Hey, perhaps try r/webdev for some feedback! What I wonder is what your solution is giving me that Google doesn't? Can't I learn about all this by simply going to my Google profile?",
        "score": 1,
        "created_utc": 1745324062.0,
        "author": "graudesch",
        "is_submitter": false,
        "parent_id": "t3_1k4fynd",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1k4bdi1",
    "title": "Virtual Beginner Friendly Data Hackathon is happening this April 26–27",
    "selftext": "[DubsTech UW](https://www.instagram.com/dubstechuw/) (a student org at the University of Washington) is hosting the **6th Annual Datathon** — a *beginner-friendly*, fully virtual data science competition happening **this weekend (April 26–27)**, and **it's open to everyone worldwide**!\n\nWhether you're into **data analytics, visualization, or machine learning**, this is a great opportunity to:\n\n* Work on **real-world datasets**\n* Use tools like **Python, R, Power BI, Tableau, Excel**, or whatever you’re most comfortable with\n* Get feedback from a panel of **11 expert judges**\n* Build a portfolio-worthy project\n* Learn from live workshops and mentorship\n* Meet and team up with data lovers from around the globe 🌎\n\nWe’re proud to say that our very first Datathon back in 2018 had just 50+ students in a classroom. Now it’s grown into a global event that brings together hundreds of participants—from beginners to seasoned pros.  \n  \n**🔗 Learn More and Register:** [https://datathon2025.webflow.io/](https://datathon2025.webflow.io/)  \n**🗓️ Date:** April 26 & 27, 2025  \n**🌐 Location:** Virtual (Zoom + Discord)\n\nHope to see some of you there! Let me know if you have any questions :)",
    "url": "https://www.reddit.com/r/data/comments/1k4bdi1/virtual_beginner_friendly_data_hackathon_is/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1745235519.0,
    "author": "Happy-Dealer-7125",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k4bdi1/virtual_beginner_friendly_data_hackathon_is/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1k3ttl6",
    "title": "How long does Google keep a record of my search history and the websites I've visited, both when I'm signed into my Google account and when I'm not signed in, but the data is still linked to my device or IP address?",
    "selftext": "",
    "url": "https://www.reddit.com/r/data/comments/1k3ttl6/how_long_does_google_keep_a_record_of_my_search/",
    "score": 6,
    "upvote_ratio": 0.99,
    "num_comments": 5,
    "created_utc": 1745175601.0,
    "author": null,
    "subreddit": "data",
    "permalink": "/r/data/comments/1k3ttl6/how_long_does_google_keep_a_record_of_my_search/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mo4zmzo",
        "body": "FOR-EV-AR!",
        "score": 6,
        "created_utc": 1745177182.0,
        "author": "DonkeyDonRulz",
        "is_submitter": false,
        "parent_id": "t3_1k3ttl6",
        "depth": 0
      },
      {
        "id": "mo5ad54",
        "body": "Not a direct answer, but because questions like that are popular, I am switching to kagi now.  \n[https://kagi.com/](https://kagi.com/)",
        "score": 2,
        "created_utc": 1745180780.0,
        "author": "der_gopher",
        "is_submitter": false,
        "parent_id": "t3_1k3ttl6",
        "depth": 0
      },
      {
        "id": "mo75lda",
        "body": "Assume forever.  Check out your Google takeout download and go through everything that they are storing about you and see how far back it goes. \n\nData is the new gold and storage is typically cheap. You should assume that everything that is saved about you is preserved forever. Even if you delete something from a system, it doesn't necessarily mean that it's actually been deleted. It just means your access has been deleted. \n\nTraining data, compliance, QA testing are all reasons why companies will keep all data even after you ask them to delete it unless legally required.",
        "score": 2,
        "created_utc": 1745204782.0,
        "author": "dotben",
        "is_submitter": false,
        "parent_id": "t3_1k3ttl6",
        "depth": 0
      },
      {
        "id": "mo770bt",
        "body": "Foeva",
        "score": 1,
        "created_utc": 1745205362.0,
        "author": "Minimum_Professor113",
        "is_submitter": false,
        "parent_id": "t3_1k3ttl6",
        "depth": 0
      },
      {
        "id": "mobzqnl",
        "body": "Google myactivity",
        "score": 1,
        "created_utc": 1745274140.0,
        "author": "heislertecreator",
        "is_submitter": false,
        "parent_id": "t3_1k3ttl6",
        "depth": 0
      }
    ],
    "comments_extracted": 5
  },
  {
    "id": "1k3nrcc",
    "title": "How to automatically pull information from a website dashboard into a spreadsheet?",
    "selftext": "Hello!\n\n  \nI run a pizza shop and like to export my stores hourly sales into a spreadsheet because our point of sale system does not allow you to view hourly sales unless you view one day at a time. \n\n  \nIs there a way to have this done automatically?  I tried using an API connection to Zapier but I couldn't get it to work.\n\n  \nFor reference, we use Clover as the point of sale system and I use excel to store all this data. \n\n  \nCurrently the way i do this is logging into the Clover business dashboard and  manually exporting each days sales numbers and then open all those spreadsheets and copy/paste the data from each sheet to my main sheet.\n\n  \nIm not sure if this is enough info for anyone to help but thanks in advance!",
    "url": "https://www.reddit.com/r/data/comments/1k3nrcc/how_to_automatically_pull_information_from_a/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1745159311.0,
    "author": "j-bd20",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k3nrcc/how_to_automatically_pull_information_from_a/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "moag9oe",
        "body": "Have you taken a look at their API? \n\nhttps://docs.clover.com/dev/docs/clover-rest-api-index\n\nThis seems to me to be the best approach. That way you could have a script on your computer that runs the requests to the API, formats the response and saves it to your spreadsheet.",
        "score": 1,
        "created_utc": 1745257409.0,
        "author": "markx15",
        "is_submitter": false,
        "parent_id": "t3_1k3nrcc",
        "depth": 0
      },
      {
        "id": "mom778n",
        "body": "I take it that you are tech savvy business owner but not a programmer. You can use a BI tool, create a report or dashboard with a single table connecting to the API. This will not require you writing any code. Then schedule the report to run every hour and send you an export as Excel. You can use open source BI tools such as metabase or stylebi without any cost",
        "score": 1,
        "created_utc": 1745419517.0,
        "author": "schi854",
        "is_submitter": false,
        "parent_id": "t3_1k3nrcc",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1k3j0zt",
    "title": "Any data governance peeps here?",
    "selftext": "Since I couldn’t find any data governance reddit site, I am posting here. \nHow easy is it to learn Collibra if I learn and work with Alation? \nBoth are governance tool, Collibra is more enterprise used ik, but I only got chance for a project in Alation but want to upskill and move to Collibra later on. ",
    "url": "https://www.reddit.com/r/data/comments/1k3j0zt/any_data_governance_peeps_here/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1745142266.0,
    "author": "National-Owl-9987",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k3j0zt/any_data_governance_peeps_here/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1k2ug1n",
    "title": "career switch: Would I be considered for jobs in IT from phd theoretical physics background",
    "selftext": "Is the career switch even realistic,  since currently apart from my math skills and very basic Mathematica skills I don't have anything.  If possible, can you guys please suggest what are skills I should acquire ?",
    "url": "https://www.reddit.com/r/data/comments/1k2ug1n/career_switch_would_i_be_considered_for_jobs_in/",
    "score": 2,
    "upvote_ratio": 0.75,
    "num_comments": 0,
    "created_utc": 1745063268.0,
    "author": "Strange_Purple_7671",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k2ug1n/career_switch_would_i_be_considered_for_jobs_in/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1k2szm1",
    "title": "How these apps connects my activity with my Facebook profile? I didn't connect Facebook with them. I am using different accounts in different apps. In Adobe I am not even using an account?",
    "selftext": "",
    "url": "https://i.redd.it/rzjrypfsmrve1.jpeg",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1745057345.0,
    "author": "xxxxproplayerxxxx",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k2szm1/how_these_apps_connects_my_activity_with_my/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1k2nbhr",
    "title": "Questions for freelance data analysts on here!",
    "selftext": "1. How long have you been freelaancing?\n2. What did you do before that? Did it come in handy when you decided to get into DA?\n3. I have a prior experience in sales and operations in niche manufacturing industry. Right now I'm working in sales and operations in an SAAS startup. If I want to take up data analytics as a freelancer while still working in my current job (to get me started in DA field ), how realistic is it? \n4. How did you start getting gigs as a freelancer?\n5. What are your tips and opinions for me given my situation? \nNote: I have done the IBM Data Analytics certification so have basic knowledge of python, sql and have good proficiency with excel. I haven't really worked on a portfolio yet but am planning to start on it. \n\nThanks for reading and thanks for taking the time to respond!",
    "url": "https://www.reddit.com/r/data/comments/1k2nbhr/questions_for_freelance_data_analysts_on_here/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1745033964.0,
    "author": "willu_readme",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k2nbhr/questions_for_freelance_data_analysts_on_here/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1k24u1w",
    "title": "Can't generate insights. What am I doing wrong?",
    "selftext": "This is my first Data Analyst role and I'm losing confidence.\n\nMy first few months, I was assigned to come up with an analysis of our customer base and I felt like I did poorly at it. Tl:dr, I jumped onto using clustering models and came up with customer segments that my team said were \"not useful\". I was told to revamp and go back to the basics, so I ended up with a simple EDA that just showed things they already know (distribution of gender, age, etc. and trends -- customers aging, married customers increasing, etc). That was when it hit me how this is not intuitive for me. Like, I didn't immediately have ideas on what I should look at, how I should approach the analysis, or that I had to \"weave a story to make it cohesive\", etc.\n\nAnyway, the second part was to look at spending data and come up with more concrete customer segments. I have been looking at the data for weeks now and still have nothing. The first few initial results I got were shot down (constructively). The main point being, what does the result tell us and how does it help? Some comments I got that made me re-do my work were I needed to clean the data better or I needed to pick up accurate features/fields, rethink the metrics I'm using, or that the results don't tell anything.\n\nI've gotten constructive feedback and tips like look at it from different angles, look at relationships, break it down into questions you want answered, etc. Now, I'm just stuck with multiple pivot tables that I don't even want to look at.\n\nSome numbers are so close to each other, I wonder if there are even patterns in the data. I'm not confident in coming up with interpretations and sometimes I wonder if what I'm getting is even valuable enough to conclude something.\n\nI'm so lost now in how to approach this and honestly, it's like I'm not progressing because I feel like I've looked at everything and still have no results.\n\nWhat am I doing wrong? Aside form lacking experience and intuition.\n\n**Pretty sure i was not able to articulate myself properly but TL;DR I suck at analysis work and have been lost for weeks now and don't know how to proceed. Any tips?**",
    "url": "https://www.reddit.com/r/data/comments/1k24u1w/cant_generate_insights_what_am_i_doing_wrong/",
    "score": 6,
    "upvote_ratio": 0.88,
    "num_comments": 10,
    "created_utc": 1744982606.0,
    "author": "SatisfactionWide8340",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k24u1w/cant_generate_insights_what_am_i_doing_wrong/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mns3b3d",
        "body": "One framework that I found effective is always asking the requester for questions that they want to answer. Questions in plain simple English. \n\nThis also helps you understand the end objective and the requesters line of thinking. \n\nAnd with a little bit of experience you can always ask the second question which decisions would answer to the first question enable them to take. \n\nThen you can work backwards to get the data or preliminary insights. As long as you answer those questions, will be considered a successful deliverable.\n\nA good exercise to help you discover your areas of opportunities whether it be business acumen or technical expertise.",
        "score": 4,
        "created_utc": 1744992644.0,
        "author": "okay-caterpillar",
        "is_submitter": false,
        "parent_id": "t3_1k24u1w",
        "depth": 0
      },
      {
        "id": "mo2ndgc",
        "body": "Sounds to me like you did a cracking job. A lot of what you are experiencing is sort of normal. Don’t be disheartened. I am head of data and I sometimes struggle to see the story. My technical skills are getting rusty so I rely on my team to do lot of the technical work. It’s kind of my role to see the Insight and turn into a story and an action. I sometimes miss! \n\nIt seems you have the technical skills, but you need  to get some experience turning those into actions and insights the business needs. Honestly, that is a huge and unfair ask for a first role. It takes time, understanding the business and the soft skills. You will get there. The challenge is learning all that whilst staying on top of technical skills - which I struggled with - so you l’d be an asset to a right thinking head of data / insight. \n\nThe thread will give you lots of tips and tricks, but you should not feel at all disheartened.",
        "score": 2,
        "created_utc": 1745147219.0,
        "author": "Extension_Dog_7867",
        "is_submitter": false,
        "parent_id": "t3_1k24u1w",
        "depth": 0
      },
      {
        "id": "mnrboh0",
        "body": "This is so silly. First DA job and they give you a bunch of data and ask for insights. No DA can give insights, unless they already know the business domain. Insights come from collaboration with people who are knowledgeable about the business. All data analysis should start from questions. Something like, \"How can we fine tune our targeting to specific customer segements?\" \"How can we improve conversions\", etc.\n\nIf I were you, I'd look for one person you get along with who understands the process. Map out the customer journey (A literal flow chart or funnel that includes the tools used). This will help you understand what is happening. Collaborate with the business to understand the biggest problems in that process. \n\nNow, you can start analyzing the data. You have a question to answer. If it's a sales funnel, just google \"analyzing sales funnels\". See what others have done. If it's some other process, search for that. Come up with ideas and try them out. Your story will be something like, Here is our process, here is where it is breaking down, this data shows a specific problem. Let's figure out how to address it.  \n\nOver time, you'll learn more and more about DA work and the business. You'll start giving recommendations instead of charts.",
        "score": 2,
        "created_utc": 1744984191.0,
        "author": "leogodin217",
        "is_submitter": false,
        "parent_id": "t3_1k24u1w",
        "depth": 0
      },
      {
        "id": "mnr8mt5",
        "body": "Without knowing anything about your employer/data, are you looking in relation to products and purchasing behaviours? Your employers only care about customer insights that have sales implications. Analysis of customer base is a good introduction, but what can your company sell to them?",
        "score": 1,
        "created_utc": 1744983160.0,
        "author": "Cyraga",
        "is_submitter": false,
        "parent_id": "t3_1k24u1w",
        "depth": 0
      },
      {
        "id": "mnwojda",
        "body": "If there are no patterns you need to zoom in or out. If you are looking at transactions by day, like by hour or minute. (Zoom in) There are always patterns unless it is purely random like lotto. In which case the pattern is random for a given sample greater than a threshold . What you need to do is hypothesise . Find out what change when something changes . Also actually get of your arse and do soft testing of concepts , as a newbie you need to learn how the business work, you might find out your data is half the picture, ie doesn’t include web analytics, buy flows , free trials, internal account etc. looking at data can be a bit like looking at shadows on the wall . don’t try and figure out the shape , since you can’t, because you limit yourself to what you know and as a newb that is very little  . But identify characteristics of it and then you can gradually define it, ideally against an hypothesis. Happy for DM if you need help\n\nFrom a learning pov you might checkout the difference between analytics and mining as we called it in the old days. You can also read up on innovation, as there are multiple types of. This opens up your mind a bit so you get confidence to play around. That is where the insights are",
        "score": 1,
        "created_utc": 1745059680.0,
        "author": "mullerjannie",
        "is_submitter": false,
        "parent_id": "t3_1k24u1w",
        "depth": 0
      },
      {
        "id": "mofupli",
        "body": "u/SatisfactionWide8340 as a few others have suggested, this is about learning to ask the right questions, and not so much about your experience or intuition. There are several frameworks for thinking through problems and how to ask write questions of your data.\n\nJobs to be done framework - grouping customers according to what problem they are trying to solve. \n\nTime-based cohorts - segmenting customers based on the time they joined or performed an activity. \n\nSentiment Analysis - segmenting customers by  mood & frustration signals\n\nIt's a good idea to ask your manage or the team who is requesting the analysis for the end goal or the problems they are trying to solve. Customer analysis is a very broad term and its worth digging deep to understand the specifics of what the team wants to learn about their customers.\n\nIf you are struggling, enlist help of some AI tools like ChatGPT, Claude,  Querri, etc. that will also suggest to you what questions can be asked of your data.",
        "score": 1,
        "created_utc": 1745333636.0,
        "author": "neelsc",
        "is_submitter": false,
        "parent_id": "t3_1k24u1w",
        "depth": 0
      },
      {
        "id": "mnrw56v",
        "body": "I wasn't able to explain in detail, but the domain is simple. The product we're selling is a bit straightforward and the analysis is more of understand who our good customers are and define these customer segments.\n\nSo I'm analyzing demographic data alongside their bank transaction info (which they submit to us). I just can't seem to surface any insights from what i'vee been exploring so far. \n\nI've aggregated their spending by category (health, travel, food) and then checkes if there are differences across age groups, genders etc but i guess i'm just having a hard time interpreting or seeing the patterns, if that make sense. like example, a spider chart gives me roughly the same pattern across diff age groups. So i'm like, am i looking at thewrong thing or are there no patterns? or should I forcible extract results and come up with my own intepretation?",
        "score": 1,
        "created_utc": 1744990490.0,
        "author": "SatisfactionWide8340",
        "is_submitter": true,
        "parent_id": "t1_mnrboh0",
        "depth": 1
      },
      {
        "id": "mnrwhih",
        "body": "It's more of obtaining soendin behavior and generating customer segments from there. And then hopefully be able to gove useful insigths to teams like marketing etc by giving them an idea of who our customers are.\n\nBut my main concern is I'm not really seeing much patterns?And any insight that I do find is something that they already know (e.g. avg age of our customers are xx, xx% are male, etc.). Looking at relationships between variables also don't tell much, since they're already a given (e.g. the highest spenders in groceries are those in their 30s to 50s, but if you look at the data, almost all age groups prioritize spending in groceries ans the 30s to 50a group make up majority of our customers.) \n\nSo it's things like these that mke me confuses. Because some of the results are just obvious, the rest are what we already know. And then the remaining show no evident pattern that can make me confidently say \"these are are segments and their characteristics\"",
        "score": 1,
        "created_utc": 1744990593.0,
        "author": "SatisfactionWide8340",
        "is_submitter": true,
        "parent_id": "t1_mnr8mt5",
        "depth": 1
      },
      {
        "id": "mntaa0w",
        "body": "I still want to know what problem are they trying to solve. How do they plan to use your analysis. I assume it must be related to increased sales or higher prices. There needs to be a goal. The why of the analysis. \n\nIf they just want demographics, do the clustering you did and look into what trates each cluster has. Give each one a name. Then dig into the pivots like you did. The story is that our customers fall into these categories, here's why. \n\nIf that fails, I don't know what to say. They may already have a defined presentation they want and just aren't telling you. That's where finding an ally will help.",
        "score": 1,
        "created_utc": 1745005730.0,
        "author": "leogodin217",
        "is_submitter": false,
        "parent_id": "t1_mnrw56v",
        "depth": 2
      },
      {
        "id": "mnyxcwc",
        "body": "Do the customers cross over into other segments? So you're saying the you know that highest grocery buyers are in 30s to 50s but if you have say travel data in there too, do they cross into that? How many can you cross sell to? Is there a pattern there? Or is there a cross section of those else where? Cross selling is important so finding segments of people where you can say this type of person is a cross selling dream so you want to target them with X may be the kind of insights that they are after. Just an idea?!",
        "score": 1,
        "created_utc": 1745088655.0,
        "author": "RevolutionaryTerm577",
        "is_submitter": false,
        "parent_id": "t1_mnrwhih",
        "depth": 2
      }
    ],
    "comments_extracted": 10
  },
  {
    "id": "1k20ue8",
    "title": "How to Visualize Customer Purchases vs. Sales Impact?",
    "selftext": "Hi everyone,\nI hope this is the right place to ask. I have a spreadsheet with all the sales invoices for 2024, and I need to analyze the sales trend of a specific customer. What I’m trying to show is that when this customer ordered my products and had them on display, the products sold consistently and often outperformed competitor products—even without any promotional effort.\n\nI want to visualize:\n\t•\tWhen the customer ordered my products,\n\t•\tThe sales performance that followed,\n\t•\tAnd how this compares to sales of competitor products in the same timeframe.\n\nThe goal is to create a compelling graphic or dashboard that clearly illustrates this trend and correlation.\n\nI’m looking for advice on:\n\t•\tWhat software or tools are best suited for this (Excel, Power BI, Google Sheets, Tableau, etc.)?\n\t•\tHow to structure the data and what kind of chart would best demonstrate the point?\n\t•\tIf there’s anyone experienced who would be open to helping me build this or guide me through it.\n\nThanks in advance for any tips, templates, or pointers!\n",
    "url": "https://www.reddit.com/r/data/comments/1k20ue8/how_to_visualize_customer_purchases_vs_sales/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1744968723.0,
    "author": "SinneMann19",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k20ue8/how_to_visualize_customer_purchases_vs_sales/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1k1vrk1",
    "title": "Help!",
    "selftext": "I need the emails and personal phone numbers of dentists from US and Canada. I need a good database. Can anyone of you help me?",
    "url": "https://www.reddit.com/r/data/comments/1k1vrk1/help/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1744948168.0,
    "author": "Warisay",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k1vrk1/help/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mnspvc1",
        "body": "Dm.",
        "score": 1,
        "created_utc": 1744999343.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_1k1vrk1",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1k1ajzk",
    "title": "Recent graduate struggling to land a data analyst job – what am I doing wrong?",
    "selftext": "Hi everyone,\nI'm a recent graduate from Tunisia actively looking for a data analyst role. Since graduation, I’ve been applying daily on LinkedIn and Indeed to positions all over Europe, but I always get rejected—most of the time without even reaching the interview stage.\n\nI’ve worked on several interesting projects in data analysis, and I’m proficient in Power BI and Tableau. I genuinely enjoy this field and am constantly trying to improve my skills, but I feel stuck.\n\nHas anyone here been in a similar situation? What could I be doing wrong? Any advice or feedback would be really appreciated.\n\nThanks in advance!",
    "url": "https://www.reddit.com/r/data/comments/1k1ajzk/recent_graduate_struggling_to_land_a_data_analyst/",
    "score": 4,
    "upvote_ratio": 0.84,
    "num_comments": 8,
    "created_utc": 1744889724.0,
    "author": "No-Psychology-7771",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k1ajzk/recent_graduate_struggling_to_land_a_data_analyst/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mnkh0o7",
        "body": "First job is always the hardest, I would try finding it through connections in your existing network from your school (rather than just trying for generic positions on LinkedIn). Without prior experience the employer highly relies on your personal skills, which your network is probably more familiar with.\n(Based on my experience as a hiring manager / employer)",
        "score": 3,
        "created_utc": 1744890091.0,
        "author": "notwerks",
        "is_submitter": false,
        "parent_id": "t3_1k1ajzk",
        "depth": 0
      },
      {
        "id": "mnkpkfk",
        "body": "First jobs are always the hardest as well as the fact that you are from Tunisia and will require visa support.\n\nThere are a lot of Europeans in the Data Sciences so they are more likely to get a job since they require less support than you\n\nI would look into the Data gig job and see if you can get a job via making connections that way.",
        "score": 2,
        "created_utc": 1744893477.0,
        "author": "CakeisaDie",
        "is_submitter": false,
        "parent_id": "t3_1k1ajzk",
        "depth": 0
      },
      {
        "id": "mnm1b41",
        "body": "I hate to be a downer, but as someone with no experience and needing a visa, your chances of getting the kind of job you want are close to 0%. People need to justify to the government why they are hiring you if you require a visa, and with no experience, it's never going to work. \n\n\nYour options are:\n- get more experience. Find a local job and do that for 5-10 years first so you have a stronger profile.\n- find a different way to come to Europe (e.g., via education) and then search for a job when you have existing work permissions.",
        "score": 2,
        "created_utc": 1744908174.0,
        "author": "goatsnboots",
        "is_submitter": false,
        "parent_id": "t3_1k1ajzk",
        "depth": 0
      },
      {
        "id": "mnr14jw",
        "body": "Just keep applying, change up the cv trial and error it! for me after my masters it took around 2 years. Don’t blame the market, I had a ton of interviews but they come in waves,  months of interviews then months of silence and waiting and rejections but just push through, someone of my friends took days after graduations and some like me took years everyone is different. Build yourself like a notion board of applications and keep going. Once interviews come , prepare well , Connect and keep optimistic. You can do it :)",
        "score": 2,
        "created_utc": 1744980442.0,
        "author": "trymenick",
        "is_submitter": false,
        "parent_id": "t3_1k1ajzk",
        "depth": 0
      },
      {
        "id": "mnpgato",
        "body": "try starting a company buddy",
        "score": 1,
        "created_utc": 1744949353.0,
        "author": "casual12938",
        "is_submitter": false,
        "parent_id": "t3_1k1ajzk",
        "depth": 0
      },
      {
        "id": "mo96k6h",
        "body": "new graduate job market is probably one of the worst because employers are trying to use AI to replace junior staff. I suggest try to use the gap to know more about AI",
        "score": 1,
        "created_utc": 1745242711.0,
        "author": "schi854",
        "is_submitter": false,
        "parent_id": "t3_1k1ajzk",
        "depth": 0
      },
      {
        "id": "mnkpry6",
        "body": "What is Data gig economy?",
        "score": 1,
        "created_utc": 1744893554.0,
        "author": "No-Psychology-7771",
        "is_submitter": true,
        "parent_id": "t1_mnkpkfk",
        "depth": 1
      },
      {
        "id": "mnkqcmm",
        "body": "usually a contractor position of some sort for a short term project that has clear objectives.\n\nWe tried to hire one of our foreign contractors based on their performance last month although we had to tailor the position significantly \n\nAfter this kid did 3-4 projects for us over 12 months Nigerian.\n\nUnfortunately they lost the H1-B lottery so we get to try again next year.)",
        "score": 2,
        "created_utc": 1744893764.0,
        "author": "CakeisaDie",
        "is_submitter": false,
        "parent_id": "t1_mnkpry6",
        "depth": 2
      }
    ],
    "comments_extracted": 8
  },
  {
    "id": "1k179z1",
    "title": "I need Datasets for Diagnostics & lab items . Where can I find it. Any pointers",
    "selftext": "",
    "url": "https://www.reddit.com/r/data/comments/1k179z1/i_need_datasets_for_diagnostics_lab_items_where/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1744876028.0,
    "author": "AdminMember",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k179z1/i_need_datasets_for_diagnostics_lab_items_where/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1k0g3vi",
    "title": "Interview",
    "selftext": "I had got interviewed in Target by a Lead data analyst , and she was asking me multiple SQL questions. I could solve all questions. At the end she tried to correct me by asking to reverse the join condition that is a.id = b.id instead of b.id = a.id, and she tried to convince me that first condition defines left join and 2nd decides right join. I am sure that she rejected me just because I disagreed to her understanding.\n\nJust wondering about the horrible situation of analysts working with her 😆😆",
    "url": "https://www.reddit.com/r/data/comments/1k0g3vi/interview/",
    "score": 5,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1744795136.0,
    "author": "Vegetable-Apple-4692",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k0g3vi/interview/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mnfgkaa",
        "body": "[removed]",
        "score": 3,
        "created_utc": 1744819353.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_1k0g3vi",
        "depth": 0
      },
      {
        "id": "mnfhyvm",
        "body": "Exactly its painful when you loose a interview from a good product based organisation just because of interviewer stupidity . Isnt it ? \n\nHowever I have experienced this only once . I believe good companies should do a mock interview to test their interviewer so that they don’t loose their standards.",
        "score": 1,
        "created_utc": 1744819775.0,
        "author": "Vegetable-Apple-4692",
        "is_submitter": true,
        "parent_id": "t1_mnfgkaa",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1k0f4cl",
    "title": "Are we ad-hoc task completers or value creators ?",
    "selftext": "The data function needs a paradigm shift. ",
    "url": "https://v.redd.it/uzd12z19m5ve1",
    "score": 1,
    "upvote_ratio": 0.6,
    "num_comments": 0,
    "created_utc": 1744790695.0,
    "author": "Substantial_Rub_3922",
    "subreddit": "data",
    "permalink": "/r/data/comments/1k0f4cl/are_we_adhoc_task_completers_or_value_creators/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jzph6b",
    "title": "Is a pure math degree good for getting into data and finance?",
    "selftext": "Hello! I am potentially doing a math degree as I love math to pieces. We are currently doing series in calculus 2 and it’s my favorite part of the class by a mile due to the regimented rules that make sense! The rules involved make perfect sense and that is why I love them! \n\nI am most likely doing a data science minor to compliment my math degree. I want to get into data and I was wanting to know if a pure math degree can be great for getting into this field. \n\nAny advice is appreciated, \n\nThanks! ",
    "url": "https://www.reddit.com/r/data/comments/1jzph6b/is_a_pure_math_degree_good_for_getting_into_data/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 7,
    "created_utc": 1744716522.0,
    "author": "JakeMealey",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jzph6b/is_a_pure_math_degree_good_for_getting_into_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mn7vc2v",
        "body": "Better to do applied maths, which if you like calculus is going to be something you enjoy anyway. \n\nReally good is if you can do some numerical analysis (essentially calculus solved by machines)\n\nbut it doesn’t matter really, just make sure you learn to code",
        "score": 1,
        "created_utc": 1744717809.0,
        "author": "IntelligenzMachine",
        "is_submitter": false,
        "parent_id": "t3_1jzph6b",
        "depth": 0
      },
      {
        "id": "mn8a9rg",
        "body": "As a person who has a math degree and is currently a data analyst, I would say that even though having the math background is nice, there are degrees that could fit the work I do better. I have a master's degree in statistics and a minor from undergrad in computer science, and I find that I use both of those more in my work than my math degree. Now, I will acknowledge that if you're looking to become a data analyst that it is a very broad job title and depending on the organization, it means different things, but I think statistics is easier to apply in the workplace (at least for me and the organization I work for) than the things I learned in earning my mathematics degree. I'm sure it's different for a lot of people, just my two cents.",
        "score": 1,
        "created_utc": 1744723529.0,
        "author": "ejex13",
        "is_submitter": false,
        "parent_id": "t3_1jzph6b",
        "depth": 0
      },
      {
        "id": "mnabczd",
        "body": "You will absolutely need to compliment your math degree with some sort of minor or certifications in SQL and data analytics or data science.  \n\nYou can get by with solely a math degree but you might be dropped for those with experience with SQL or analytics.",
        "score": 1,
        "created_utc": 1744745644.0,
        "author": "Lost_Philosophy_",
        "is_submitter": false,
        "parent_id": "t3_1jzph6b",
        "depth": 0
      },
      {
        "id": "mncd9xd",
        "body": "Interesting. Everything you study in pure math as an undergrad has an application. Group theory can describe particles, number theory is used for communications, etc. even better, there are almost no closed for integrals in the real world so you end up using numerical methods anyway which is algebra. I also recommend focusing your efforts instead of trying to tangentially get into finance through pure math.",
        "score": 1,
        "created_utc": 1744770159.0,
        "author": "shwilliams4",
        "is_submitter": false,
        "parent_id": "t3_1jzph6b",
        "depth": 0
      },
      {
        "id": "mng93rj",
        "body": "I know a very successful data scientist and entrepreneur with an applied mathematics degree. Its all about what you do with it IMO.",
        "score": 1,
        "created_utc": 1744827627.0,
        "author": "DistanceOk1255",
        "is_submitter": false,
        "parent_id": "t3_1jzph6b",
        "depth": 0
      }
    ],
    "comments_extracted": 5
  },
  {
    "id": "1jzwhom",
    "title": "Building a doctor database — what data sources would you recommend?",
    "selftext": "Hey everyone — I’m working on building a structured database of U.S. doctors with names, specialties, locations, and ideally some contact info or enrichment like affiliations or social profiles.\n\nI figured I'd start with NPI data as the base, then try to enrich from there. I'm still early in the process though, and I’m wondering if anyone has advice on other useful data sources or approaches you've used before?\n\nWould really appreciate any ideas or pointers 🙏",
    "url": "https://www.reddit.com/r/data/comments/1jzwhom/building_a_doctor_database_what_data_sources/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1744735073.0,
    "author": "Imaginary-Bench-3175",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jzwhom/building_a_doctor_database_what_data_sources/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jyunvg",
    "title": "How to gather data from the internet",
    "selftext": "Hello, I am completely new to data collection (and Reddit too), and I am trying to collect information about every German defense company (name, address, revenue). I was wondering if there are any ways to make the collection process faster and smoother (than googling every single one individually). \n\nI take any tips, not just for this particular case, but to facilitate data collection in general. You never know when it might come in handy.\n\nThank you in advance",
    "url": "https://www.reddit.com/r/data/comments/1jyunvg/how_to_gather_data_from_the_internet/",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 6,
    "created_utc": 1744622688.0,
    "author": "Para-link",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jyunvg/how_to_gather_data_from_the_internet/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mn1mret",
        "body": "It's called web scraping. Either learn to do it, or pay someone else to.",
        "score": 4,
        "created_utc": 1744630417.0,
        "author": "ItsSignalsJerry_",
        "is_submitter": false,
        "parent_id": "t3_1jyunvg",
        "depth": 0
      },
      {
        "id": "mn71s1g",
        "body": "You can use Google maps - just search for germany defence companies and you will get a nice list. It will be a manual work but you can do it. Another way to do this is to use a webscraper like Apify's Google Maps Scraper. It will depend on the number of results. If we're talking about less than 50 results, you might be better off doing it manually, but for more results use web scraper.",
        "score": 3,
        "created_utc": 1744700329.0,
        "author": "No_Employer_5855",
        "is_submitter": false,
        "parent_id": "t3_1jyunvg",
        "depth": 0
      },
      {
        "id": "mn2lgtr",
        "body": "Hey\nI am a web scraper freelancer \nThe best way to find a directory or data source where you can find these information then either by self or hire a web scraper for it",
        "score": 2,
        "created_utc": 1744643030.0,
        "author": "Ritik_Jha",
        "is_submitter": false,
        "parent_id": "t3_1jyunvg",
        "depth": 0
      },
      {
        "id": "mn7ar3c",
        "body": "There's lots of tools for web scraping, this is a relatively new one that is becoming quite popular https://github.com/mendableai/firecrawl",
        "score": 2,
        "created_utc": 1744706192.0,
        "author": "sdairs_ch",
        "is_submitter": false,
        "parent_id": "t3_1jyunvg",
        "depth": 0
      },
      {
        "id": "mn27fur",
        "body": "Thanks",
        "score": 2,
        "created_utc": 1744638574.0,
        "author": "Para-link",
        "is_submitter": true,
        "parent_id": "t1_mn1mret",
        "depth": 1
      },
      {
        "id": "mn8dxgd",
        "body": "Thanks a lot, I'll check it out. It can always be useful for future reference",
        "score": 1,
        "created_utc": 1744724776.0,
        "author": "Para-link",
        "is_submitter": true,
        "parent_id": "t1_mn71s1g",
        "depth": 1
      }
    ],
    "comments_extracted": 6
  },
  {
    "id": "1jx5i02",
    "title": "ChatLLM: A Game-Changer in Accessing Multiple LLMs Efficiently",
    "selftext": "",
    "url": "https://frontbackgeek.com/chatllm-a-game-changer-in-accessing-multiple-llms-efficiently/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1744419634.0,
    "author": "codeagencyblog",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jx5i02/chatllm_a_gamechanger_in_accessing_multiple_llms/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jvtcqt",
    "title": "I built a system that creates Google Ads dashboards in Looker Studio—fully automated, no human interaction needed",
    "selftext": "Hey folks,\n\n\n\nI’ve been working with agencies and noticed how much time gets wasted building Looker Studio dashboards manually—especially for Google Ads.\n\n\n\nThe idea hit me: *what if this entire workflow could run itself?*\n\n\n\nSo I built a system that does exactly that:\n\n• Connects to your Google Ads account\n\n• Auto-detects campaigns, KPIs (like ROAS, CTR, etc.)\n\n• Builds two dashboard versions (internal deep dive + client-ready)\n\n• And all of this happens with no dragging charts, no edits—just click and go\n\n\n\nThis was originally meant to help our own team scale faster without hiring more analysts. But honestly, it’s been surprisingly helpful for smaller teams too.\n\n\n\nWe even added logic to adjust layout based on campaign volume, clean styling, and simplified filters—so even less technical clients get it right away.\n\n\n\nI’d love to hear how others here are tackling reporting automation. Anyone else building something to cut down on weekly report building? Or trying to remove repetitive steps?\n\n\n\nHappy to swap ideas and lessons learned 🙌",
    "url": "https://www.reddit.com/gallery/1jvtcqt",
    "score": 4,
    "upvote_ratio": 0.83,
    "num_comments": 2,
    "created_utc": 1744275563.0,
    "author": "kodalogic",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jvtcqt/i_built_a_system_that_creates_google_ads/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mmdhw5x",
        "body": "Cool! Did you use python?",
        "score": 2,
        "created_utc": 1744286730.0,
        "author": "Gcbs_jiraiya",
        "is_submitter": false,
        "parent_id": "t3_1jvtcqt",
        "depth": 0
      },
      {
        "id": "mmj7aw8",
        "body": "Hey! Nope, no Python here — the whole thing runs directly in Looker Studio using native connectors and calculated fields.\n\nOur goal was to avoid custom code or external pipelines, so it’s something anyone can duplicate and use without technical setup. Just connect your Google Ads account, and it’s ready to go.\n\nIf you’re curious how we structured it, happy to walk you through it!",
        "score": 1,
        "created_utc": 1744361978.0,
        "author": "kodalogic",
        "is_submitter": true,
        "parent_id": "t1_mmdhw5x",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1jv8fsy",
    "title": "Data Science Course",
    "selftext": "https://preview.redd.it/hkazxke4utte1.png?width=510&format=png&auto=webp&s=833ea74caf60ffc994d63400c274e32cec5970fb\n\nYour thoughts? ( paid Course )",
    "url": "https://www.reddit.com/r/data/comments/1jv8fsy/data_science_course/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1744212185.0,
    "author": "Weekly_Fig_9626",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jv8fsy/data_science_course/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jujqib",
    "title": "Designing cross-platform dashboards to unify marketing + SEO data into a single story",
    "selftext": "In my work consolidating data from GA4, Google Ads, and Search Console, one of the challenges has been telling a coherent story across platforms. Different metrics, different formats—hard to make something that feels unified.\n\n\n\nSo I started experimenting with modular layouts that break down the funnel into layers:\n\n1. **Traffic acquisition**\n\n2. **On-site engagement**\n\n3. **Conversion**\n\n4. **Post-conversion behavior** (e.g., retention, repeat visits)\n\n\n\nI used this structure to design a dashboard that prioritizes user flow rather than siloed KPIs. The result looks more like a visual narrative than a traditional report.\n\n\n\nHere’s a PNG of the layout (color-coded by platform and interaction stage). Curious what others think in terms of data-to-visual mapping, flow, and design clarity.",
    "url": "https://www.reddit.com/gallery/1jujqib",
    "score": 6,
    "upvote_ratio": 0.81,
    "num_comments": 0,
    "created_utc": 1744134420.0,
    "author": "kodalogic",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jujqib/designing_crossplatform_dashboards_to_unify/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ju4pv7",
    "title": "Previewing parquet directly from the OS",
    "selftext": "I've worked with Parquet for years at this point and it's my favorite format by far for data work.\n\nNothing beats it. It compresses super well, fast as hell, maintains a schema, and doesn't corrupt data (I'm looking at you Excel & CSV). but...\n\nIt's impossible to view without some code / CLI. Super annoying, especially if you need to peek at what you're doing before starting some analyse. Or frankly just debugging an output dataset.\n\nThis has been my biggest pet peeve for the last 6 years of my life. So I've fixed it haha.\n\nThe image below shows you how you can quick view a parquet file from directly within the operating system. Works across different apps that support previewing, etc. Also, no size limit (because it's a preview obviously)\n\nI believe strongly that the data space has been neglected on the UI & continuity front. Something that video, for example, doesn't face.\n\nI'm planning on adding other formats commonly used in Data Science / Engineering.\n\nLike:\n\n\\- Partitioned Directories *( this is pretty tricky )*\n\n\\- HDF5\n\n\\- Avro\n\n\\- ORC\n\n\\- Feather\n\n\\- JSON Lines\n\n\\- DuckDB (.db)\n\n\\- SQLLite (.db)\n\n\\- Formats above, but directly from S3 / GCS without going to the console.\n\nAny other format I should add?\n\nLet me know what you think!\n\nhttps://i.redd.it/ryn09je8bjte1.gif\n\n",
    "url": "https://www.reddit.com/r/data/comments/1ju4pv7/previewing_parquet_directly_from_the_os/",
    "score": 2,
    "upvote_ratio": 0.67,
    "num_comments": 0,
    "created_utc": 1744084696.0,
    "author": "Impressive_Run8512",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ju4pv7/previewing_parquet_directly_from_the_os/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jtqwfa",
    "title": "Data Processor or AI",
    "selftext": "It seems data processors are going to be replaced by AI. This can lead to AI creating data processing pipeline in the background and appear that as API or Websocket.\n\nI think there is a huge opportunity here we need to address.",
    "url": "https://www.reddit.com/r/data/comments/1jtqwfa/data_processor_or_ai/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1744046754.0,
    "author": "kush_ptl",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jtqwfa/data_processor_or_ai/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mmnr9se",
        "body": "ooc have you tried using AI to write pipelines?",
        "score": 1,
        "created_utc": 1744419216.0,
        "author": "thinkingatoms",
        "is_submitter": false,
        "parent_id": "t3_1jtqwfa",
        "depth": 0
      },
      {
        "id": "mmnsnup",
        "body": "Wrote sqls but capturing the exact context of the question was hard.\n\nDo you have any suggestions or products to look at ? GitHub repos are also fine",
        "score": 1,
        "created_utc": 1744419745.0,
        "author": "kush_ptl",
        "is_submitter": true,
        "parent_id": "t1_mmnr9se",
        "depth": 1
      },
      {
        "id": "mmnwbty",
        "body": "i don't think AI is close to writing fully correct autonomous pipelines yet, one can prompt engineer the crap out of it but you'd still need the knowledge to write the pipeline in order to know ai did it right, and often pure prompt engineering is slower than pure coding.  atm imho best use of ai is assisted coding",
        "score": 1,
        "created_utc": 1744421154.0,
        "author": "thinkingatoms",
        "is_submitter": false,
        "parent_id": "t1_mmnsnup",
        "depth": 2
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1jtu01t",
    "title": "The safe zone in which there was a 0% chance that a major stock market crash would happen has already ended. It was between October 14, 2024 and April 2, 2025.",
    "selftext": "\n\n[https://academia.edu/123877619/Dow\\_Jones\\_percentage\\_changes\\_between\\_1896\\_and\\_2023\\_in\\_correlation\\_with\\_the\\_orbital\\_phase\\_of\\_Mars/](https://academia.edu/123877619/Dow_Jones_percentage_changes_between_1896_and_2023_in_correlation_with_the_orbital_phase_of_Mars/)\n\nThis theory that a stock market crash will never happen when Mars is in front of the sun is confirmed in real time. Based on the information provided, Redditors in this thread calculated when Mars would go behind the sun again and saw the theory play out in real time\n\n[https://www.reddit.com/r/AnomalousEvidence/comments/1i2dxej/massive\\_bombshell\\_a\\_100\\_statistical\\_correlation/](https://www.reddit.com/r/AnomalousEvidence/comments/1i2dxej/massive_bombshell_a_100_statistical_correlation/)\n\nhttps://preview.redd.it/0uh615ljsgte1.jpg?width=734&format=pjpg&auto=webp&s=b0346ea2dee23f5a62cca544c8eca2033b8e0d11\n\n",
    "url": "https://www.reddit.com/r/data/comments/1jtu01t/the_safe_zone_in_which_there_was_a_0_chance_that/",
    "score": 0,
    "upvote_ratio": 0.33,
    "num_comments": 0,
    "created_utc": 1744054243.0,
    "author": "AnthonyofBoston",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jtu01t/the_safe_zone_in_which_there_was_a_0_chance_that/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jtatet",
    "title": "Learn data science",
    "selftext": "i wanna go into data science/machine learning for my job, im a sophomore hs rn, what should i do to get into a good college/uni. What should i be doing",
    "url": "https://www.reddit.com/r/data/comments/1jtatet/learn_data_science/",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1743992698.0,
    "author": "Hyperruxor",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jtatet/learn_data_science/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mm2dz2s",
        "body": "The most important thing to focus on are projects. Not only are they the best way to retain what you learn and network with other programmers, they also can help you craft a clear story on why you want to learn data science in the first -- which shows the initiative many universities are looking for. Is there a particular problem you'd like to solve with your work? Make sure the projects you select all ladder up to the story you're trying to tell in your portfolio, admissions essay, etc. Best of luck!",
        "score": 2,
        "created_utc": 1744132681.0,
        "author": "udacity",
        "is_submitter": false,
        "parent_id": "t3_1jtatet",
        "depth": 0
      },
      {
        "id": "mm2j7bz",
        "body": "Do i start with learning to code into machine learning?",
        "score": 1,
        "created_utc": 1744134148.0,
        "author": "Hyperruxor",
        "is_submitter": true,
        "parent_id": "t1_mm2dz2s",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1jtc6vo",
    "title": "Have a question about an insecure site and my data",
    "selftext": "I'm not sure where to post this to be honest but I have a question... Could somebody let me have access to \"storageaccess\" which is a sitw you can get movies and tv shows but it's not a secure site, could the person who gave me the access to it have access to my data and the stuff on my phone?",
    "url": "https://www.reddit.com/r/data/comments/1jtc6vo/have_a_question_about_an_insecure_site_and_my_data/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1743997255.0,
    "author": "Barbie_2495",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jtc6vo/have_a_question_about_an_insecure_site_and_my_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jsg1lz",
    "title": "Do these dice seem fair? [OC]",
    "selftext": "I bought this pair of handmade D6 dice on vacation, and you can tell they are not perfectly made just holding them. I wanted to see how fair they actually are, so I test rolled them by hand into a dice tray, and these are the results, rolled separately and together.\n\nI know what a fair set of data from dice should look like (equal individually and bell curve together), but these dice almost seem to be fair in a different sense, just having higher rolls in the extremes and kind of a funky curve when rolled together. Do you guys think these seem fair? Is there a better place for me to ask this?",
    "url": "https://www.reddit.com/gallery/1jsg1lz",
    "score": 21,
    "upvote_ratio": 0.92,
    "num_comments": 15,
    "created_utc": 1743893875.0,
    "author": "probablynotpolice",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jsg1lz/do_these_dice_seem_fair_oc/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mlo7mfp",
        "body": "How/why is there a 1 in your results table?",
        "score": 13,
        "created_utc": 1743929160.0,
        "author": "roastedpotato20",
        "is_submitter": false,
        "parent_id": "t3_1jsg1lz",
        "depth": 0
      },
      {
        "id": "mlm6o8o",
        "body": "For each dice separately, run a Chi square test. Enter the tally for each of the 6 numbers. The expected outcome will be the total number of rolls divided by 6\n\nIf the p is < 0.05 then it suggests the dice is not fair\n\nYou can do this in excel. I think the comand is chi.test",
        "score": 10,
        "created_utc": 1743894918.0,
        "author": "ybetaepsilon",
        "is_submitter": false,
        "parent_id": "t3_1jsg1lz",
        "depth": 0
      },
      {
        "id": "mlptsc6",
        "body": "I dit the chi-square test with alpha = 5% and seems that the dices are fair",
        "score": 6,
        "created_utc": 1743955360.0,
        "author": "Spirited_Copy_403",
        "is_submitter": false,
        "parent_id": "t3_1jsg1lz",
        "depth": 0
      },
      {
        "id": "mlq858w",
        "body": "I didnt want it to feel left out",
        "score": 8,
        "created_utc": 1743959996.0,
        "author": "probablynotpolice",
        "is_submitter": true,
        "parent_id": "t1_mlo7mfp",
        "depth": 1
      },
      {
        "id": "mlpf6sx",
        "body": "Bro got 0 and 1 lmao",
        "score": 3,
        "created_utc": 1743950585.0,
        "author": "Spirited_Copy_403",
        "is_submitter": false,
        "parent_id": "t1_mlo7mfp",
        "depth": 1
      },
      {
        "id": "mmak487",
        "body": "This guy stats",
        "score": 1,
        "created_utc": 1744238108.0,
        "author": "No_Design958",
        "is_submitter": false,
        "parent_id": "t1_mlm6o8o",
        "depth": 1
      },
      {
        "id": "mlq8eu9",
        "body": "Oh, really? Im surprised, it seems like theres enough variation in the number of rolls that they would definitely be unfair. Thanks btw",
        "score": 2,
        "created_utc": 1743960079.0,
        "author": "probablynotpolice",
        "is_submitter": true,
        "parent_id": "t1_mlptsc6",
        "depth": 1
      },
      {
        "id": "mlpi65j",
        "body": "One fell off the table and bro said fuck it, it's 1",
        "score": 5,
        "created_utc": 1743951562.0,
        "author": "roastedpotato20",
        "is_submitter": false,
        "parent_id": "t1_mlpf6sx",
        "depth": 2
      },
      {
        "id": "mlssqpv",
        "body": "If you in increase the sample size (roll more) you can run the test again with greater certainty. \n\nPer the law of large numbers, the more you roll, the closer to the expected outcome you’ll be. If you roll alot more and the curve still looks lopsided, thats and indication the dice isnt fair",
        "score": 1,
        "created_utc": 1743992040.0,
        "author": "PlayLikeNewbs",
        "is_submitter": false,
        "parent_id": "t1_mlq8eu9",
        "depth": 2
      },
      {
        "id": "mlq7wr9",
        "body": "I wish that was it. My first pair of snake eyes I saw 2 ones and thought \"oh, one\"",
        "score": 5,
        "created_utc": 1743959923.0,
        "author": "probablynotpolice",
        "is_submitter": true,
        "parent_id": "t1_mlpi65j",
        "depth": 3
      },
      {
        "id": "mm3bde2",
        "body": "Rolling the dice more will wear them down thus making them less fair. Large numbers doesn’t work when repeated trials changes the input.",
        "score": 1,
        "created_utc": 1744142337.0,
        "author": "c126",
        "is_submitter": false,
        "parent_id": "t1_mlssqpv",
        "depth": 3
      },
      {
        "id": "mm4osjz",
        "body": "You are technically right. How much wear would the dice get from doubling the sample tho?",
        "score": 1,
        "created_utc": 1744158062.0,
        "author": "PlayLikeNewbs",
        "is_submitter": false,
        "parent_id": "t1_mm3bde2",
        "depth": 4
      },
      {
        "id": "mm4sr06",
        "body": "I read somewhere casinos change them out every 8 hours.",
        "score": 1,
        "created_utc": 1744159457.0,
        "author": "c126",
        "is_submitter": false,
        "parent_id": "t1_mm4osjz",
        "depth": 5
      },
      {
        "id": "mm51e8w",
        "body": "True! But thats also for security/checking for trick dice. Anyways, bless up. Good convo",
        "score": 1,
        "created_utc": 1744162521.0,
        "author": "PlayLikeNewbs",
        "is_submitter": false,
        "parent_id": "t1_mm4sr06",
        "depth": 6
      }
    ],
    "comments_extracted": 14
  },
  {
    "id": "1jrl8ju",
    "title": "Open data Netherlands",
    "selftext": "I am trying to find open datasets that are relatively up to date on social media usage and mental health. But beyond some commercial usage I can't find much. There are some studies that seem to be from the same national surveys but are not open data. \n\nIt's somewhat frustrating that sensitive data like crime among youth is readily available but social media usage (without specifics) is somehow too sensitive? But it can be used for marketing. Ther is a lot of fake posturing and selective moralism it seems. As it's too sensitive to be open data but it somehow can be used by commercial and financial interests? Very frustrating.\n\nDoes anyone know if there are datasets after 2023 about social media usage in the Netherlands that someone that is just a data-nerd without any substantial financial backing can use?",
    "url": "https://www.reddit.com/r/data/comments/1jrl8ju/open_data_netherlands/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1743797018.0,
    "author": "aemilius89",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jrl8ju/open_data_netherlands/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mlgwjuf",
        "body": "I would try looking for studies released in that time frame that indicate the use of data similar to what youre looking for. Here's an example I found\n\nhttps://link.springer.com/article/10.1007/s12187-023-10080-8\n\nIf you reach out to the paper authors asking for a copy of their initial data collection along with the results of their semantic/sentiment analysis or whatever kind of profiling they performed, there should be a good chance they will gladly provide it, assuming they are free to do so.",
        "score": 3,
        "created_utc": 1743814293.0,
        "author": "PaperMoonsOSINT",
        "is_submitter": false,
        "parent_id": "t3_1jrl8ju",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1jqmygs",
    "title": "Hundreds of millions more dollars recouped by governments after ICIJ investigations",
    "selftext": "",
    "url": "https://www.icij.org/investigations/panama-papers/hundreds-of-millions-more-dollars-recouped-by-governments-after-icij-investigations/?utm_campaign=news&utm_medium=social&utm_source=reddit",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1743697646.0,
    "author": "ICIJ",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jqmygs/hundreds_of_millions_more_dollars_recouped_by/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jqkg0y",
    "title": "Managing data shouldn’t feel like herding cats",
    "selftext": "Hey folks! Ever feel like your data is all over the place—different systems, messy spreadsheets, and dashboards that make no sense? It’s like trying to herd cats, right? We totally get it. \n\nA while back, we worked with a team that was drowning in data chaos. They had customer info in one system, sales figures in another, and no way to connect the dots. It wasn’t just frustrating—it was holding them back from making smart decisions. \n\nSo, here’s what we did: we helped them clean up their data, centralize it, and set up automated processes to keep things organized. The best part? We built dashboards that gave them real-time insights without needing a PhD in analytics. Suddenly, their data wasn’t just \\*numbers\\* anymore—it was actionable insights that actually made their work easier.\n\nNow they’re making decisions faster, spotting trends before they become problems, and saving hours every week. Honestly, seeing the transformation is the best part of what we do.\n\nIf you’re dealing with data headaches too, we’d love to chat about how you can turn it around with our [enterprise data management services](https://datafortune.com/services/enterprise-data-management/). Or just drop a comment—what’s been your biggest challenge with managing data? Let’s swap ideas! \n\n",
    "url": "https://www.reddit.com/r/data/comments/1jqkg0y/managing_data_shouldnt_feel_like_herding_cats/",
    "score": 0,
    "upvote_ratio": 0.29,
    "num_comments": 1,
    "created_utc": 1743691799.0,
    "author": "DataMaster2025",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jqkg0y/managing_data_shouldnt_feel_like_herding_cats/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mlq0zjr",
        "body": "Naw",
        "score": 1,
        "created_utc": 1743957723.0,
        "author": "notstoppinguntil30",
        "is_submitter": false,
        "parent_id": "t3_1jqkg0y",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1jq83vr",
    "title": "Normalizing temperature data",
    "selftext": "I have one off temperature readings for in situ rocks at different times of day over multiple days. \n\nTypically, you would just use a data logger to do this - but that wasn't feasible for this project.\n\nI thought I had a way to normalize those data for comparisons, but it didn't work.\n\nSo here is an example of what I have:\n\nRock 001 - 23 degrees, 9:13am, 8/12/24\nRock 002 - 29 degrees, 1:00pm, 8/12/24\nRock 001 - 27 degrees, 11:45 am, 8/24/24\nRock 002 - 30 degrees, 10:15,am, 8/24/24\n\nI also have air temp from the nearest weather station for each date and time.\n\nThe real data is 40 rocks with 5 observations at different dates and times.\n\nI've been looking for papers that have this same issue, but I don't think I'm using the right keywords.\n\nAny ideas for normalizing these temps so I can compare them?  \n\nI figure anyone monitoring temperatures over seasons must have a similar problem to correct for.",
    "url": "https://www.reddit.com/r/data/comments/1jq83vr/normalizing_temperature_data/",
    "score": 1,
    "upvote_ratio": 0.99,
    "num_comments": 1,
    "created_utc": 1743650740.0,
    "author": "spacecowgirl87",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jq83vr/normalizing_temperature_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mla7msx",
        "body": "You have some trouble there. \n\nFirstly, what is the desired output of this measurement? For example, if you wish to compare the effect of the location on your samples for the given time (I believe thats the goal, but let us know), then you might need to decide a sensitivity. You might try to average the temps of the each rock for the given timeframe, but the deviation will be through the roof. Can this be compensated/tolerated in the final ? If you can find a good correlation (R\\^2>0.9) between the rock temps and air temps, and assuming you have access to the complete weather station data for your timeframe, then you could try to interpolate the missing measurements as well, but I don't know if you want to do this. \n\n> I figure anyone monitoring temperatures over seasons must have a similar problem to correct for\n\nI am not working in the field so I can't confirm anything, but I would assume the output of that measurements (often) would be at an hourly rate of resolution, and locations (measurement sources) are binned. \n\n  \nWith the information you have given, I would try to bin the data for clusters. How frequent are the measurements? Can you cluster the measurement times? Can you bin the locations? I think only independent variable here is the rocks. What if you split the timeframe itself? \n\n  \nGood luck. Following the post for other ideas.",
        "score": 1,
        "created_utc": 1743720987.0,
        "author": "New_Alarm3749",
        "is_submitter": false,
        "parent_id": "t3_1jq83vr",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1jplkvw",
    "title": "Memory card",
    "selftext": "I erased all on camera. Attempting to recover photos now. Search using disk drive and currently comparing deleted files to files previously transfer to external hard drive. No point recovering files I already have. \n\nIssue\n\nI can find most but not the files with _SCF at the start. E.g. _SCF1499.JPG\nI'm assuming the file name has changed on transfer? Any other ideas? ",
    "url": "https://www.reddit.com/r/data/comments/1jplkvw/memory_card/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 5,
    "created_utc": 1743590800.0,
    "author": "cbe29",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jplkvw/memory_card/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "ml1a3vl",
        "body": "dont touch it, give it to a professional.  if you slow formatted the card you are out of luck.  gl!",
        "score": 1,
        "created_utc": 1743606822.0,
        "author": "thinkingatoms",
        "is_submitter": false,
        "parent_id": "t3_1jplkvw",
        "depth": 0
      },
      {
        "id": "ml1ndhk",
        "body": "How did i slow format?",
        "score": 1,
        "created_utc": 1743610808.0,
        "author": "cbe29",
        "is_submitter": true,
        "parent_id": "t1_ml1a3vl",
        "depth": 1
      },
      {
        "id": "ml1nthc",
        "body": "like formatted the drive but chose not to do a quick format.  how did you delete the files",
        "score": 1,
        "created_utc": 1743610943.0,
        "author": "thinkingatoms",
        "is_submitter": false,
        "parent_id": "t1_ml1ndhk",
        "depth": 2
      },
      {
        "id": "ml2omkw",
        "body": "Pressed delete all on the camera",
        "score": 1,
        "created_utc": 1743621558.0,
        "author": "cbe29",
        "is_submitter": true,
        "parent_id": "t1_ml1nthc",
        "depth": 3
      },
      {
        "id": "ml4kyyj",
        "body": "ya id maybe not touch it and give it to a restoration professional.  or if you have access to linux there might be some things you can run yourself.  but if you are asking here then you are better off checking with a professional",
        "score": 1,
        "created_utc": 1743643114.0,
        "author": "thinkingatoms",
        "is_submitter": false,
        "parent_id": "t1_ml2omkw",
        "depth": 4
      }
    ],
    "comments_extracted": 5
  },
  {
    "id": "1jowuna",
    "title": "Does anyone require a paper on Data science or AI ML topic to be proofread or something. Happy to help since I need to author a paper for my applications.",
    "selftext": "I want to publish a paper for my Master's application. For the same if someone is pursuing research on the lines of Data science and or AI ML, I would love to help out in some capacity. Please reach out if you think we can work something out.",
    "url": "https://www.reddit.com/r/data/comments/1jowuna/does_anyone_require_a_paper_on_data_science_or_ai/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1743517449.0,
    "author": "Ok-Ingenuity-1396",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jowuna/does_anyone_require_a_paper_on_data_science_or_ai/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jorkoh",
    "title": "Data, what is it, why is it so accessible?",
    "selftext": "At my company we recently changed platforms on this we communicate to each other and photos get sent through. Now they HAVE incorporated chatGPT into it all. I wondered why the interface was different suddenly. This interface has videos of me doing speeches and now this has been given to AI. When I raised the issue with my company, I was told to get with the times and to stop being precious.\n\nWho benefits here? I feel everywhere is data hungry, so many policies say they share data with META and Google. But why? and some even state, they don't sell data, but share it with third parties, but why?\n\nI'm single, I go to work, I have a son, there isn't anything interesting. I valued my privacy which is now gone.\n\nHow can companies be allowed to just give out this data? Why is this data wanted? Surely it isn't advertising.",
    "url": "https://www.reddit.com/r/data/comments/1jorkoh/data_what_is_it_why_is_it_so_accessible/",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 1,
    "created_utc": 1743500250.0,
    "author": "BadAccomplished165",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jorkoh/data_what_is_it_why_is_it_so_accessible/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "ml1lgmm",
        "body": "If it's not for advertising, it's for business intelligence.",
        "score": 1,
        "created_utc": 1743610229.0,
        "author": "PaperMoonsOSINT",
        "is_submitter": false,
        "parent_id": "t3_1jorkoh",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1joclh3",
    "title": "what is the difference between content analysis and categorization of themes in responses?",
    "selftext": "For a class I am taking, we are working on a group project that involves us each interviewing some people (we have done 8 interviews). In the write up portion of this project, it says to \"Describe your approach to analyze your primary data (e.g., content analysis and categorization of themes in responses)\". What does that mean, how do they differ and how would I apply them? I have looked it up but I keep getting answers that do not apply to my situation. ",
    "url": "https://www.reddit.com/r/data/comments/1joclh3/what_is_the_difference_between_content_analysis/",
    "score": 29,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1743451297.0,
    "author": "ajknightly",
    "subreddit": "data",
    "permalink": "/r/data/comments/1joclh3/what_is_the_difference_between_content_analysis/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mkyg0wy",
        "body": "Basically, content analysis is when you go through your interview data and look for specific stuff. Like how many times people mention “stress” or whether they talk positively or negatively about something. Categorizing themes is the next step, where you group related responses into broader ideas. Like turning mentions of “burnout”, long hours”, and “no support” into a theme like “workplace pressure”. It’s less about counting and more about understanding what people mean. If that sounds kind of overwhelming (which it totally can be, especially with multiple interviews), there are dedicated tools like AILYZE that help guide you through it. They flag common/ unique viewpoints, suggest themes, and make the whole process less painful. You can consider it if you’re new to qualitative analysis and not sure where to start.",
        "score": 1,
        "created_utc": 1743558089.0,
        "author": "Prettyme_17",
        "is_submitter": false,
        "parent_id": "t3_1joclh3",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1jniac4",
    "title": "What is the best way to collect like >10 years old news articles from the mainstream media and newspapers?",
    "selftext": "",
    "url": "https://www.reddit.com/r/data/comments/1jniac4/what_is_the_best_way_to_collect_like_10_years_old/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1743357504.0,
    "author": "SaintPellegrino4You",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jniac4/what_is_the_best_way_to_collect_like_10_years_old/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jnlo6l",
    "title": "Got an interview for Data Trainee position",
    "selftext": "What are some questions I can expect? ",
    "url": "https://www.reddit.com/r/data/comments/1jnlo6l/got_an_interview_for_data_trainee_position/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 0,
    "created_utc": 1743366241.0,
    "author": "Secret_Resource_9807",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jnlo6l/got_an_interview_for_data_trainee_position/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jnb0hq",
    "title": "Converting hevc files into normal mp4 files",
    "selftext": "Hello there :D\n\nI need help woth converting my datas. I made some Videos on my phone and as i got them onto my pc, the programs on my pc aren't able to open the videos. They're from a concert and I dont really want to lose them.\n\nDoes anyone knows a solution for my problem?\n\nBest regards!",
    "url": "https://www.reddit.com/r/data/comments/1jnb0hq/converting_hevc_files_into_normal_mp4_files/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1743336442.0,
    "author": "UseMeHardDaddy69",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jnb0hq/converting_hevc_files_into_normal_mp4_files/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jmy6b0",
    "title": "I need a solution to search through tens of thousands of PDFs that I 100% know are backed up to Google Drive, pCloud, and OneDrive. Any specific prompts I can use with Gemini Advanced, Copilot Pro, or another AI? A federal agency is requesting documents from 4 to 6 years ago.",
    "selftext": "",
    "url": "https://www.reddit.com/r/data/comments/1jmy6b0/i_need_a_solution_to_search_through_tens_of/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1743287822.0,
    "author": "kaiser1025",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jmy6b0/i_need_a_solution_to_search_through_tens_of/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jn4bn7",
    "title": "How one can monetize customer data from old companies ?",
    "selftext": "Old data",
    "url": "https://www.reddit.com/r/data/comments/1jn4bn7/how_one_can_monetize_customer_data_from_old/",
    "score": 0,
    "upvote_ratio": 0.33,
    "num_comments": 0,
    "created_utc": 1743307320.0,
    "author": "PersonalityCapital19",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jn4bn7/how_one_can_monetize_customer_data_from_old/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jmzuwa",
    "title": "What is the most valuable company data ?",
    "selftext": "Employee salary and contacts\nCosting and pricing \nPatents and intellectual property \n\n",
    "url": "https://www.reddit.com/r/data/comments/1jmzuwa/what_is_the_most_valuable_company_data/",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 1,
    "created_utc": 1743292687.0,
    "author": "PersonalityCapital19",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jmzuwa/what_is_the_most_valuable_company_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mkguim4",
        "body": "Depends on the industry or business model but universally I believe Customer Data is the most valuable. In the concept of business customer data drives personalization and segmentation that leads to high customer retention rates.",
        "score": 1,
        "created_utc": 1743306859.0,
        "author": "nerdnedy",
        "is_submitter": false,
        "parent_id": "t3_1jmzuwa",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1jm8m6c",
    "title": "We created an AI data analysis platform : Supboard!",
    "selftext": "Hello guys , apologies if it's not the right space for this .\nMe and my team have created together https://supaboard.ai/  , it is basically an AI powered data analysis platform where you don't have to know anything about SQL , python or other data analysis platform and get insights of your data by giving simple prompts\n\nNow we will be launching it on product hunt also \nSo if you guys like Supaboard, then kindly tap that notify me button on product hunt so that it can garner some good support and momentum \nhttps://www.producthunt.com/products/supaboard-ai\n\nAnd if you guys have any feedback, feel free to write it down \nThanks :) ",
    "url": "https://www.reddit.com/r/data/comments/1jm8m6c/we_created_an_ai_data_analysis_platform_supboard/",
    "score": 2,
    "upvote_ratio": 0.67,
    "num_comments": 5,
    "created_utc": 1743203962.0,
    "author": "Harshit-24",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jm8m6c/we_created_an_ai_data_analysis_platform_supboard/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mk9u42k",
        "body": "It says \"coming soon\"",
        "score": 2,
        "created_utc": 1743204779.0,
        "author": "justin_reborn",
        "is_submitter": false,
        "parent_id": "t3_1jm8m6c",
        "depth": 0
      },
      {
        "id": "mklafl0",
        "body": "Wow very cool and interesting.\nWill be sure to check it out when it launches.\nI created a AI data analysis tool specifically for google sheets called [Octo](https://spriglaunch.com/r/1jm8m6c-1743372757292?url=https%3A%2F%2Fchromewebstore.google.com%2Fdetail%2Focto-analyze-google-sheet%2Fobloifdoiohdfcnimbahlphceecmjoci&amp;userId=48oibhr6&amp;postId=1jm8m6c&amp;subreddit=data).\nIt can create charts, do analysis and send automated insights on schedule. Pls check it out.",
        "score": 1,
        "created_utc": 1743372794.0,
        "author": "Both-Blueberry2510",
        "is_submitter": false,
        "parent_id": "t3_1jm8m6c",
        "depth": 0
      },
      {
        "id": "mknawhz",
        "body": "I've seen it on PH, very cool tool. Our product has the similar function as yours, [powerdrill.ai](http://powerdrill.ai) . You are welcome to check it out and give us any of your feedback!",
        "score": 1,
        "created_utc": 1743403481.0,
        "author": "Powerdrill_AI",
        "is_submitter": false,
        "parent_id": "t3_1jm8m6c",
        "depth": 0
      },
      {
        "id": "mk9uctf",
        "body": "Here is the product link https://supaboard.ai/\nThat link was of product hunt , basically we are launching there also",
        "score": 1,
        "created_utc": 1743204861.0,
        "author": "Harshit-24",
        "is_submitter": true,
        "parent_id": "t1_mk9u42k",
        "depth": 1
      },
      {
        "id": "mklagsd",
        "body": "It looks like you shared an AMP link. These should load faster, but AMP is controversial because of [concerns over privacy and the Open Web](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot).\n\nMaybe check out **the canonical page** instead: **[https://chromewebstore.google.com/detail/octo-analyze-google-sheet/obloifdoiohdfcnimbahlphceecmjoci](https://chromewebstore.google.com/detail/octo-analyze-google-sheet/obloifdoiohdfcnimbahlphceecmjoci)**\n\n*****\n\n ^(I'm a bot | )[^(Why & About)](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot)^( | )[^(Summon: u/AmputatorBot)](https://www.reddit.com/r/AmputatorBot/comments/cchly3/you_can_now_summon_amputatorbot/)",
        "score": 1,
        "created_utc": 1743372806.0,
        "author": "AmputatorBot",
        "is_submitter": false,
        "parent_id": "t1_mklafl0",
        "depth": 1
      }
    ],
    "comments_extracted": 5
  },
  {
    "id": "1jlvdm3",
    "title": "How Data Helped an Indie Band Turn Their Struggles into Success!",
    "selftext": "Hey Mates!\n\nI just wanted to share a little something that happened recently with our team at the BI firm I work for. It’s not your typical promo, but I think it’s pretty cool and might resonate with some of you.\n\nSo, we got this indie band as a client who was really struggling to get their music out there. They were posting on social media like crazy but felt like no one was listening. You know that feeling when you’re just shouting into the void? Yeah, that was them.\n\nWe decided to step in and take a look at their data. We used our [**business intelligence**](https://datafortune.com/services/enterprise-data-management/business-intelligence/) tools to dig into their social media stats, and honestly, we found some surprising stuff:\n\n* Their most engaged followers weren’t actually buying their music or tickets.\n* Some posts that they thought were great were actually turning people off.\n* There were whole groups of potential fans they hadn’t even tapped into yet.\n\nAfter sharing these insights with the band, we helped them switch up their strategy. Instead of just posting random updates, they started creating content that really spoke to their audience. They even tried some targeted ads based on the data we provided.\n\nFast forward a few months, and guess what? Their Spotify streams shot up by 60% and they even snagged a local sponsorship deal!\n\nIt just goes to show that with the right data, you can really make a difference. So if you’re in a similar boat—whether you’re an artist or in any other field—don’t just throw stuff at the wall and hope it sticks. Use your data!",
    "url": "https://www.reddit.com/r/data/comments/1jlvdm3/how_data_helped_an_indie_band_turn_their/",
    "score": 6,
    "upvote_ratio": 0.87,
    "num_comments": 0,
    "created_utc": 1743169388.0,
    "author": "DataMaster2025",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jlvdm3/how_data_helped_an_indie_band_turn_their/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jm5txw",
    "title": "How Data Analytics is Transforming Supplier Performance Evaluation",
    "selftext": "",
    "url": "https://qcd.digital/how-data-analytics-is-transforming-supplier-performance-evaluation/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1743196404.0,
    "author": "Kuczerenko",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jm5txw/how_data_analytics_is_transforming_supplier/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jl8835",
    "title": "The Confused Analytics Engineer",
    "selftext": "",
    "url": "https://daft-data.medium.com/the-confused-analytics-engineer-773dc0068c69?sk=6f828120bb6dca0974524ca2232bbc0e",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1743093436.0,
    "author": null,
    "subreddit": "data",
    "permalink": "/r/data/comments/1jl8835/the_confused_analytics_engineer/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jl07hm",
    "title": "[Advice] Building a benchmarking tool to compare utility usage with competitors. Looking for feedback on visualization",
    "selftext": "Hi everyone!  \nI’m working on a benchmarking report for a project that helps compare utility usage (like energy or water) against a group of similar competitors. The goal is to make inefficiencies easy to spot at a glance.  \nI have a decent grasp of stats, but I’m not very confident when it comes to data visualization and layout. I’d really appreciate any feedback or suggestions on how to improve the clarity, structure, or overall look of the report.  \nIf you also think there’s a better way to present the data altogether, I’m open to that too!  \nThanks in advance for your help 🙏",
    "url": "https://i.redd.it/qkajhzrhe7re1.png",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1743068821.0,
    "author": "Upper-Hand-8682",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jl07hm/advice_building_a_benchmarking_tool_to_compare/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jkzphd",
    "title": "How would you present this data in a presentation slide? (For job interview)",
    "selftext": "I am looking to compare the sales of frozen, refrigerated, cupboard food over the past 3 months. I have all the data and know how to work with it. \n\nMy question is- how would you present this analysis back to stakeholders (this is my task). \n\nI was thinking a pie chart for each month with some explanation, however not sure it looks visually appealing. I’m using excel and PowerPoint. ",
    "url": "https://www.reddit.com/r/data/comments/1jkzphd/how_would_you_present_this_data_in_a_presentation/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 8,
    "created_utc": 1743066457.0,
    "author": "Flat-Park6164",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jkzphd/how_would_you_present_this_data_in_a_presentation/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mjzhxwy",
        "body": "[removed]",
        "score": 2,
        "created_utc": 1743069370.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_1jkzphd",
        "depth": 0
      },
      {
        "id": "mk1vmif",
        "body": "Thank you so much!!",
        "score": 2,
        "created_utc": 1743098774.0,
        "author": "Flat-Park6164",
        "is_submitter": true,
        "parent_id": "t1_mjzhxwy",
        "depth": 1
      },
      {
        "id": "mk6j69c",
        "body": "[removed]",
        "score": 1,
        "created_utc": 1743168584.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t1_mk1vmif",
        "depth": 2
      },
      {
        "id": "mk81vyq",
        "body": "They really liked my presentation thank you!! 🤞",
        "score": 2,
        "created_utc": 1743184840.0,
        "author": "Flat-Park6164",
        "is_submitter": true,
        "parent_id": "t1_mk6j69c",
        "depth": 3
      },
      {
        "id": "mkbviax",
        "body": "[removed]",
        "score": 2,
        "created_utc": 1743238957.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t1_mk81vyq",
        "depth": 4
      },
      {
        "id": "mkoqmqk",
        "body": "I got the job 🥹🥳🥳",
        "score": 2,
        "created_utc": 1743429872.0,
        "author": "Flat-Park6164",
        "is_submitter": true,
        "parent_id": "t1_mkbviax",
        "depth": 5
      },
      {
        "id": "mkoqolt",
        "body": "Thank you so much for your advice!!!",
        "score": 2,
        "created_utc": 1743429890.0,
        "author": "Flat-Park6164",
        "is_submitter": true,
        "parent_id": "t1_mkbviax",
        "depth": 5
      }
    ],
    "comments_extracted": 7
  },
  {
    "id": "1jkd0gz",
    "title": "23and me data deletion?",
    "selftext": "Forgive me if this is totally the wrong spot for this (and let me know if there is a better subreddit), but I've been wanting to delete my 23andme data for a while, and now seems to be the time -the bankruptcy, etc.\n\nI was thinking to download my raw data, but the site says that will take a few days (in order for them to process it..or something). Is it smarter to say F it, and delete all data immediately - or will a few days of waiting not really matter? \n\nAgain, sorry if this is the wrong place - this is a field I have no experience with.\n\nThank youuuuu.",
    "url": "https://www.reddit.com/r/data/comments/1jkd0gz/23and_me_data_deletion/",
    "score": 6,
    "upvote_ratio": 0.88,
    "num_comments": 2,
    "created_utc": 1742999136.0,
    "author": "someresearch",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jkd0gz/23and_me_data_deletion/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mjubnet",
        "body": "Consumer reports posted a video about it. Check it out.",
        "score": 2,
        "created_utc": 1743000723.0,
        "author": "New_Alarm3749",
        "is_submitter": false,
        "parent_id": "t3_1jkd0gz",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1jkbpen",
    "title": "How to display this survey data in a neat graph?",
    "selftext": "",
    "url": "https://i.redd.it/hnl7m7oic1re1.png",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1742995511.0,
    "author": "namless_boi",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jkbpen/how_to_display_this_survey_data_in_a_neat_graph/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mkb8yac",
        "body": "\n\n\n\nBased on your questionnaire design, the survey is grouped by age. Since all responses in the questionnaire fall within the 3-5 point range, displaying individual questionnaire details might overwhelm viewers like navigating a maze. Instead, consider presenting statistical results - for example, showing what percentage of 18-20 year olds gave 5 points, 4 points, etc. on specific metrics.\n\n\n\nOverall, I recommend adopting coarser data granularity to create a visual dashboard. Additionally, incorporating a variety of chart types could help reduce viewers' visual fatigue.\n\n\n\nHope this helps.",
        "score": 1,
        "created_utc": 1743224589.0,
        "author": "Shoddy-Moose4330",
        "is_submitter": false,
        "parent_id": "t3_1jkbpen",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1jk8m5f",
    "title": "Trying to find large datasets on Alzheimer's and dementia",
    "selftext": "**A bit of backstory: My father passed away from Alzheimer's in 2023. I am a software developer studying LLMs, and I’m looking to see if there are any large datasets on Alzheimer's or any projects that possibly have an API for accessing relevant data. I am based in the UK. Thanks!**\n\n",
    "url": "https://www.reddit.com/r/data/comments/1jk8m5f/trying_to_find_large_datasets_on_alzheimers_and/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 2,
    "created_utc": 1742984969.0,
    "author": "Reasonable_Edge2411",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jk8m5f/trying_to_find_large_datasets_on_alzheimers_and/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mjw0hxb",
        "body": "What kind of data are you looking for? Genetic, or proteomic data, or survival, or maybe patient age? Do you have something specific in mind? Would you consider curating multiple sources for parameters ? And lastly, would you consider collaborating for a scientific paper?",
        "score": 1,
        "created_utc": 1743018445.0,
        "author": "New_Alarm3749",
        "is_submitter": false,
        "parent_id": "t3_1jk8m5f",
        "depth": 0
      },
      {
        "id": "mjw4zuv",
        "body": "It’s just more for learning how large languages models work just would like data that would give me a bit of passion I guess.  And yes stats like u mention.",
        "score": 1,
        "created_utc": 1743019734.0,
        "author": "Reasonable_Edge2411",
        "is_submitter": true,
        "parent_id": "t1_mjw0hxb",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1jk1ouc",
    "title": "Need some clarity on the below course",
    "selftext": "Hi data engineers,\nI was surfing the internet regarding the data engineering courses and i found one paid course in the below link https://educationellipse.graphy.com/courses/End-to-End-Data-Engineering--Azure-Databricks-and-Spark-66c646b1bb94c415a9c33899\n\nHave anyone of you taken this course, please provide your  suggestions whether to take it or not, it would be really helpful.\n\nThanks in advance",
    "url": "https://www.reddit.com/r/data/comments/1jk1ouc/need_some_clarity_on_the_below_course/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1742956707.0,
    "author": "I-am-a-new-realm",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jk1ouc/need_some_clarity_on_the_below_course/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jjvbmv",
    "title": "Data Council conference",
    "selftext": "Anyone going next month in Oakland? Anyone ever been",
    "url": "https://www.reddit.com/r/data/comments/1jjvbmv/data_council_conference/",
    "score": 4,
    "upvote_ratio": 0.76,
    "num_comments": 0,
    "created_utc": 1742939156.0,
    "author": "chicanatifa",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jjvbmv/data_council_conference/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jise0b",
    "title": "Data",
    "selftext": "Guys , how do you perform data analytics and anything that can help me learn data analytics as a complete beginner?",
    "url": "https://www.reddit.com/r/data/comments/1jise0b/data/",
    "score": 5,
    "upvote_ratio": 0.86,
    "num_comments": 7,
    "created_utc": 1742827635.0,
    "author": "Harshit-24",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jise0b/data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mjly8hf",
        "body": "[removed]",
        "score": 4,
        "created_utc": 1742880061.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_1jise0b",
        "depth": 0
      },
      {
        "id": "mknbfk5",
        "body": "If you just want to do data analysis for a project, there are many cool tools out there can help you and you don't need to have a knowledge background. You are more than welcome to check out our product [powerdrill.ai](http://powerdrill.ai) and see whether it fits you. And if you want to learn da from beginning, there are always some cool tutorials on yt for you to study. For example, if you want to study SQL, I love the class taught by Mosh on yt! Hope these info can help you. Good luck with your work!",
        "score": 1,
        "created_utc": 1743403808.0,
        "author": "Powerdrill_AI",
        "is_submitter": false,
        "parent_id": "t3_1jise0b",
        "depth": 0
      },
      {
        "id": "mlw6v5e",
        "body": "You don't need to stress out when you can use a data analysis tool. OWOX BI  does analysis, visualisation and reporting . Compared to other tools, OWOX BI’s differentiation is its Semantic Layer which doesn't take days to setup. My team really loves that it skips the ‘drag-and-drop’ step, just chat, and trusted data appears in their google sheets. It’s lighter than other tools but excels at ad-hoc needs… (just because of spreadsheets lighter than BIs)Ideal for orgs where ‘I need this now’ outweighs ‘build a perfect dashboard’.",
        "score": 1,
        "created_utc": 1744046333.0,
        "author": "EasternAggie",
        "is_submitter": false,
        "parent_id": "t3_1jise0b",
        "depth": 0
      },
      {
        "id": "mjtqia0",
        "body": "This seems interesting \nI will start working on it \nBy any chance, can you provide me a bit guidance through the data analysis journey or something \nI mean I can DM you if you are down for it",
        "score": 2,
        "created_utc": 1742993799.0,
        "author": "Harshit-24",
        "is_submitter": true,
        "parent_id": "t1_mjly8hf",
        "depth": 1
      },
      {
        "id": "mjzfhbz",
        "body": "[removed]",
        "score": 3,
        "created_utc": 1743067862.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t1_mjtqia0",
        "depth": 2
      },
      {
        "id": "mjzqr1y",
        "body": "This surely helps a lot \nYour fundamental points are really something providing me guidance in this field \nThanks man \nI will drop a DM if I get stuck somewhere \nThanks again for the support:)",
        "score": 2,
        "created_utc": 1743074083.0,
        "author": "Harshit-24",
        "is_submitter": true,
        "parent_id": "t1_mjzfhbz",
        "depth": 3
      }
    ],
    "comments_extracted": 6
  },
  {
    "id": "1jiej6n",
    "title": "Getting statistics for a movie list",
    "selftext": "Sorry if this is not right for this sub, I wasn't sure where to put it. \n\nA couple days ago I decided to make a list of all of the movies I've ever seen, so far this has come out to about 623. I was originally going to use an AI tool to pull statistics and crap from it and \"Scientifically find my favorite movie\" but none of the ones I know of are able to process the full list, although they have given me some cool results. I have no idea how all that stuff works and I'm very bad at math, this was just a little passion project I've been working on. If anybody has any sites that would work or tips or anything please let me know.",
    "url": "https://www.reddit.com/r/data/comments/1jiej6n/getting_statistics_for_a_movie_list/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1742777063.0,
    "author": "CarelessRestaurant88",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jiej6n/getting_statistics_for_a_movie_list/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mjlg2h9",
        "body": "Best way would be to download VS Studio and code some basic python with the help of AI",
        "score": 1,
        "created_utc": 1742871574.0,
        "author": "No_Strawberry8083",
        "is_submitter": false,
        "parent_id": "t3_1jiej6n",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1jibeah",
    "title": "How to use multiple languages in a datapipeline",
    "selftext": "Was wondering if any other people here are part of teams that work with multiple different languages in a data pipeline.\nEg. at my company we use some modules that are only available on R, and then run some scripts on those outputs in python. \nI wanted to know how teams that have this problem streamline data across multiple languages maintaining data in memory. \n\nAre there tools that let you setup scripts in different languages to process data in a pipeline with different languages. \n\nMainly to be able to scale this process with tools available on the cloud. ",
    "url": "https://www.reddit.com/r/data/comments/1jibeah/how_to_use_multiple_languages_in_a_datapipeline/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1742768261.0,
    "author": "pirana04",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jibeah/how_to_use_multiple_languages_in_a_datapipeline/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jib7dp",
    "title": "Multiple languages in a datapipeline",
    "selftext": "Was wondering if any other people here are part of teams that work with multiple different languages in a data pipeline.\nEg. at my company we use some modules that are only available on R, and then run some scripts on those outputs in python. \nI wanted to know how teams that have this problem streamline data across multiple languages maintaining data in memory. \n\nAre there tools that let you setup scripts in different languages to process data in a pipeline with different languages. \n\nMainly to be able to scale this process with tools available on the cloud. ",
    "url": "https://www.reddit.com/r/data/comments/1jib7dp/multiple_languages_in_a_datapipeline/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 0,
    "created_utc": 1742767757.0,
    "author": "pirana04",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jib7dp/multiple_languages_in_a_datapipeline/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ji7iy7",
    "title": "[ Removed by Reddit ]",
    "selftext": "[ Removed by Reddit on account of violating the [content policy](/help/contentpolicy). ]",
    "url": "https://www.reddit.com/r/data/comments/1ji7iy7/removed_by_reddit/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 2,
    "created_utc": 1742758256.0,
    "author": null,
    "subreddit": "data",
    "permalink": "/r/data/comments/1ji7iy7/removed_by_reddit/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mjczoqb",
        "body": "Nice try FBI",
        "score": 2,
        "created_utc": 1742758804.0,
        "author": "noodlesallaround",
        "is_submitter": false,
        "parent_id": "t3_1ji7iy7",
        "depth": 0
      },
      {
        "id": "mjdiw07",
        "body": "Thats all u got ?",
        "score": 1,
        "created_utc": 1742764755.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t1_mjczoqb",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1jhb3vj",
    "title": "How to evaluate/research the total amount of lifetime unemployment rate of germans?",
    "selftext": "For a school project i am researching the lifetime unemployment rate of germans (how many germans, who are able to work, become, on average, unemployed in their worklife?) and am struggling to cohesively ask this question search engines or ai tools.\nIt seems like there is hardly any available data, so i am asking myself if there is a, easy, way to compute these rate myself and am more than welcome to any possible input.",
    "url": "https://www.reddit.com/r/data/comments/1jhb3vj/how_to_evaluateresearch_the_total_amount_of/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1742657628.0,
    "author": "alessandrux",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jhb3vj/how_to_evaluateresearch_the_total_amount_of/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jf8q60",
    "title": "Data Analyst vs Data Engineer",
    "selftext": "I currently work as a Data Analyst, however my actual job duties fit the description for a Data Engineer exactly. Would there be any benefit to asking my supervisor to change my title from analyst to engineer? Is this worth a conversation? ",
    "url": "https://www.reddit.com/r/data/comments/1jf8q60/data_analyst_vs_data_engineer/",
    "score": 12,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1742420680.0,
    "author": "Putrid-Individual616",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jf8q60/data_analyst_vs_data_engineer/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mip1a5l",
        "body": "Yes, it’s worth it. Market salaries for data engineers are higher than they are for data analysts. You will set yourself up for better comp in the future.",
        "score": 3,
        "created_utc": 1742421753.0,
        "author": "CheeseDog_",
        "is_submitter": false,
        "parent_id": "t3_1jf8q60",
        "depth": 0
      },
      {
        "id": "miqcrdy",
        "body": "Second that. Even if you don't get a change in salary in your current place, the title can help negotiate a better pay when moving to another company. And data engineer path is worthwhile, given you put the effort to be competent.",
        "score": 1,
        "created_utc": 1742437613.0,
        "author": "ContributionFuzzy443",
        "is_submitter": false,
        "parent_id": "t1_mip1a5l",
        "depth": 1
      },
      {
        "id": "mjhi78u",
        "body": "Agreed, data engineer and data scientists are valued and paid more than data analysts",
        "score": 1,
        "created_utc": 1742827709.0,
        "author": "Harshit-24",
        "is_submitter": false,
        "parent_id": "t1_miqcrdy",
        "depth": 2
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1jfinxb",
    "title": "Looking for relative cost of modern military equipment",
    "selftext": "Hello I'm looking for a list with relative, approximate costs for various pieces of military equipment. I don't really care about units as long as they are consistent. With modern I mean 1970 or newer. Mainly looking at ground forces, with shorter-range weapons (sub 50km, so no ICBMs or similar). Don't really care about which country/company makes/buys the stuff, again assuming I can get consistent units.\n\nAnyone has some good places to start looking?",
    "url": "https://www.reddit.com/r/data/comments/1jfinxb/looking_for_relative_cost_of_modern_military/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1742452154.0,
    "author": "Organic-Major-9541",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jfinxb/looking_for_relative_cost_of_modern_military/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1je3br4",
    "title": "🚀 Data Cheat Sheets ( Python, Pandas, pyspark, sql, DAX PBI)– Looking for Feedback!",
    "selftext": "\nHey everyone! I’ve created a set of Data Analyst Cheat Sheets  covering Python, SQL, Pandas, PySpark, Power BI, and DAX  (single page for each) to help learners and professionals.\n\n📂 You can download them for $1.99 (or pay whatever you feel is fair). Would love to hear your thoughts or suggestions for improvements! 😊\n\n\n🔗 [Download here](https://surl.li/ncvtjc)\n\nWould love your feedback! ",
    "url": "https://www.reddit.com/r/data/comments/1je3br4/data_cheat_sheets_python_pandas_pyspark_sql_dax/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1742299411.0,
    "author": "bittersalt1",
    "subreddit": "data",
    "permalink": "/r/data/comments/1je3br4/data_cheat_sheets_python_pandas_pyspark_sql_dax/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1jd3cww",
    "title": "Everything You Need to Know About Pipelines",
    "selftext": "*In the fast-paced world of software development, data processing, and technology, pipelines are the unsung heroes that keep everything running smoothly. Whether you’re a coder, a data scientist, or just someone curious about how things work behind the scenes, understanding pipelines can transform the way you approach tasks. This article will take you on a journey through the world of pipelines*  \n[https://medium.com/@ahmedgy79/everything-you-need-to-know-about-pipelines-3660b2216d97](https://medium.com/@ahmedgy79/everything-you-need-to-know-about-pipelines-3660b2216d97)",
    "url": "https://www.reddit.com/r/data/comments/1jd3cww/everything_you_need_to_know_about_pipelines/",
    "score": 3,
    "upvote_ratio": 0.8,
    "num_comments": 2,
    "created_utc": 1742181464.0,
    "author": "ahmed4929",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jd3cww/everything_you_need_to_know_about_pipelines/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mi8fyd5",
        "body": "Thanks for the share. I'll check it out",
        "score": 2,
        "created_utc": 1742205150.0,
        "author": "Soren_Professor",
        "is_submitter": false,
        "parent_id": "t3_1jd3cww",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1jcsw49",
    "title": "Struggling to understand SQLite fundamentals….",
    "selftext": "Hey everyone, I’m a bit confused about how SQLite works in a Git-based project. Hoping someone can clear this up!\n\nSo, I get that a SQLite database is just a file (.sqlite or .db). And if I modify it—say, adding new rows or changing schema—those changes are saved to the file on disk. But if I don’t git add and git commit the modified file, then those changes aren’t tracked in Git, right?\n\nThat means if someone else uses the same repo on the  server, they won’t see my database updates because they only have the last committed version of the database file. So in that case, what’s the “correct” way to handle SQLite in a repo?\n\nI feel like committing the DB file is a bad idea , but if I don’t, how does everyone else keep the file in sync?\n\nWould love to hear how vyou all handle this in your projects! Thanks in advance!",
    "url": "https://www.reddit.com/r/data/comments/1jcsw49/struggling_to_understand_sqlite_fundamentals/",
    "score": 3,
    "upvote_ratio": 0.63,
    "num_comments": 1,
    "created_utc": 1742151716.0,
    "author": null,
    "subreddit": "data",
    "permalink": "/r/data/comments/1jcsw49/struggling_to_understand_sqlite_fundamentals/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mi8hdqy",
        "body": "Typically you add \"migration files\" to the git history instead -- these include the SQL code for the DDL changes you make (such as adding tables, columns, etc) so that whoever clones your repo can spin up their own DB and then run your migration files on it to get the correct state of the DB\n\nThere are loads of resources online about migration files, so you should be able to find a guide that works for you",
        "score": 1,
        "created_utc": 1742206008.0,
        "author": "Bilbottom",
        "is_submitter": false,
        "parent_id": "t3_1jcsw49",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1jbbk5i",
    "title": "Dataset for US Electricity Rates",
    "selftext": "Does anyone know of a public or private dataset that tracks the cost of electricity across the US? Or even across the world by Country?",
    "url": "https://www.reddit.com/r/data/comments/1jbbk5i/dataset_for_us_electricity_rates/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1741979231.0,
    "author": "PeaPutrid3463",
    "subreddit": "data",
    "permalink": "/r/data/comments/1jbbk5i/dataset_for_us_electricity_rates/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1j9vaoa",
    "title": "Thesis data got large....",
    "selftext": "hi y'all\n\nI'm not a data analyst by any stretch of the imagination, but in an attempt to spite one of my faculty I have accidentally generated a rather long spreadsheet of information that hasn't stopped growing. \n\nTo the people who know more than me, what is your favorite software to generate charts, summaries etc? I'm trying to avoid spending days building a thousand charts and having to add data from all over the spreadsheet.\n\n It's all in a Google sheet currently, so I can export to other formats kinda? any advice is appreciated!\n\n  \n\\*\\*Admin I don't think this counts as low effort but happy to take down at your request!",
    "url": "https://www.reddit.com/r/data/comments/1j9vaoa/thesis_data_got_large/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1741814797.0,
    "author": "nwrafter",
    "subreddit": "data",
    "permalink": "/r/data/comments/1j9vaoa/thesis_data_got_large/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mhgljlt",
        "body": "Tableu or power bi are powrful visualization programs, you could also use python. all you have to do is convert to the sheet to a .csv and it’s usable in most things.",
        "score": 1,
        "created_utc": 1741816436.0,
        "author": "Jaho03",
        "is_submitter": false,
        "parent_id": "t3_1j9vaoa",
        "depth": 0
      },
      {
        "id": "mhiyutv",
        "body": "Data:\n\nThe maximum size of an excel spreadsheet is 1M rows. If you have 100K+ rows it's possibly time to start considering other options.\n\nCSV is good because it's minimal and plain text, and the only file size restrictions are imposed by your file system (4GB each is usually the standard limit, but you probably want to stay under a gigabyte for usability and portability reasons).\n\nThe optimal solution for storing and interfacing with very large amounts of tabular data is going to be SQL, most of the time.\n\nI would recommend possibly converting your data to csv and then into a file format called \"SQLite\" which is like a local file you can interact with as if it were a sql database. This will give you a feel for how interacting with the data in SQL would work.\n\nIf the data gets too large, you'll want to host it on a dedicated server (or even your own computer) with a real SQL database. \n\nOne bonus of sql-ifying your data is that it can now be brought into almost any licensed visualization software.\n\nVisualizations:\n\nThere are almost too many options for visualization tooling and hosting. Find something that makes sense to you and stick to it for as long as you can.\n\nI am an experienced Tableau user. If you can get a license, it's good for quick visualizations. If you can't get a license, don't bother. Tableau public will force you to store your data on the open Internet just to save your vis. And Idk if you want that. Tableau also hates hates hates making tables from your data and will fight you every step of the way.\n\nPowerBI is much better at tables.\n\nIf you're willing to code, JavaScript has D3, Python has matplotlib and plotly, R has ggplot, etc. You're probably going to want to start with a jupyter notebook and then eventually migrate over to some kind of dashboard code.",
        "score": 1,
        "created_utc": 1741848859.0,
        "author": "mathbbR",
        "is_submitter": false,
        "parent_id": "t3_1j9vaoa",
        "depth": 0
      },
      {
        "id": "mi6kq8a",
        "body": "CSV is better. \nImport the CSV into a PowerBi and build on the charts and graphs.",
        "score": 1,
        "created_utc": 1742171648.0,
        "author": "Amazing-Cupcake-3597",
        "is_submitter": false,
        "parent_id": "t3_1j9vaoa",
        "depth": 0
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1j8q5zs",
    "title": "Seeking Career Advice – Data Analyst to $100K+ Path",
    "selftext": "Hi everyone,\n\nI’m looking for some career advice and hoping Reddit can provide some insights—or at least spark a conversation that leads to something even better.\n\nFor context, I completed my Master’s at NYU and have been working as a Data Analyst in a marketing agency in the U.S. for the past three years. My current salary is $80K.\n\nI have extensive experience with:\n\n- SQL (on google cloud), Python, Excel\n-Google Ads, Meta Ads, CM360 (and many other advertisers’ reporting tools)\n\nI’ve become the go-to person on my team for data and coding-related solutions, and I frequently assist the Data Engineering team as well. \n\nNow, I’m aiming to increase my salary to $100K. Given my experience, is this a realistic goal? Would it be more feasible in my current role, or should I pivot toward Data Engineering or another higher-paying path? Should I focus on learning specific skills or tools to make this jump?\n\nAdditionally, am I aiming too high for my level of experience, or is this a reasonable expectation?\n\nAny advice would be greatly appreciated! Thanks in advance.\n\n ",
    "url": "https://www.reddit.com/r/data/comments/1j8q5zs/seeking_career_advice_data_analyst_to_100k_path/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1741698023.0,
    "author": "Front_Magazine2724",
    "subreddit": "data",
    "permalink": "/r/data/comments/1j8q5zs/seeking_career_advice_data_analyst_to_100k_path/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mhxmepd",
        "body": "What geographic area are you in? \n\n100k with 3 years of experience is absolutely doable unless you're in a small rural area. In my opinion, it'd be low in markets like NYC. \n\nAs for other skills, it really depends on the position/company. You have all the core skills and others which is great because a lot of companies are starting to have data analyst require business analyst or data science/engineering skills as well as the market for them is saturated at the moment. \n\nMy recommend would be to expand your search beyond just data analyst roles. Search for similar roles or keywords like data driven insights. A lot of industries are looking for DA skills but not calling the role effort explicitly that. \n\nYou'll be able to get a role with your skills, so while you could always learn more, I'd focus on people able to explain how your current skills would be useful for the role you're applying for",
        "score": 2,
        "created_utc": 1742052519.0,
        "author": "Po_Biotic",
        "is_submitter": false,
        "parent_id": "t3_1j8q5zs",
        "depth": 0
      },
      {
        "id": "mk6s9ar",
        "body": "Oh hi. Sorry for replying a little late. \n\nThanks a ton for the advice! After researching and asking around in network events I got the same idea. Realized that I can even get to data engineering roles with the skills. \n\nI'll get to the applying now. Hoping for the best 🤞",
        "score": 1,
        "created_utc": 1743171530.0,
        "author": "Front_Magazine2724",
        "is_submitter": true,
        "parent_id": "t1_mhxmepd",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1j8dqmj",
    "title": "Interactive IoT time series data",
    "selftext": "I have time series data I would like to display on my web site.\n\nI would like to create dynamic graphs that can be zoomed, panned or compared.\n\nThe amount od measurement points to be displayed is at max 10k, but the whole dataset could be millions.\n\nDoes anyone have an recommendations on what to use?\n\n",
    "url": "https://www.reddit.com/r/data/comments/1j8dqmj/interactive_iot_time_series_data/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1741651941.0,
    "author": "Soft-Conclusion-2004",
    "subreddit": "data",
    "permalink": "/r/data/comments/1j8dqmj/interactive_iot_time_series_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1j87kft",
    "title": "Struggling to Extract Meaningful Data from Spotify—API? Hosting Platforms? GOING CRAZY HERE",
    "selftext": "I know this isnt the ideal place to ask about this but i dont have enough carma yet on other subreddits that would be more fitting, and we're really getting pressed here. ANY HELP IS WELCOME  \n  \nMy team is working on a project with Spotify, and to make it happen, we need to extract listener data from our clients' podcast accounts. Some of the podcasts are hosted through Spotify for Podcasters, and others on Podbean.\n\nThe issue is that both platforms provide almost no raw data—it’s basically just episode names, dates, listeners, and clicks. There are a few other columns, but they’re mostly empty because Spotify constantly changes its data structure and lacks consistency (sorry for the frustration, but it’s been challenging). The same goes for the Spotify API—it’s almost useless beyond basic tracking. I’m at a loss for what other hosting platforms offer solid, raw, and consistent data. We’re looking for metrics like retention rates, breakdowns by quartile, completion rates, growth rates—but honestly, we’d take any form of structured data. Direct access to the server would be a game-changer in terms of automation, too. Right now, one team member spends nearly an entire week manually extracting and feeding data for 26 podcasts, which is incredibly time-consuming.\n\nThe client wants results, but we simply don’t have enough data to provide anything statistically significant or even remotely preditive (the intention is to do predictive analysis which we need really complete and robust data for). We explained this to them, and they asked us to recommend a hosting platform that fits our needs. But we can’t even do that, since there’s no information online beyond vague claims like \"we provide data visualizations,\" which isn’t helpful. We need the raw data.\n\nSo my question is—how do people generally extract meaningful data from Spotify? How does anyone run advanced analysis with such limited data? Do podcasters just not analyze their data? Is there some hidden API or hosting platform we’re missing? It’s honestly really confusing, and we’re desperate for any tips, methods, or hosting platforms that are actually data centered.",
    "url": "https://www.reddit.com/r/data/comments/1j87kft/struggling_to_extract_meaningful_data_from/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1741636117.0,
    "author": null,
    "subreddit": "data",
    "permalink": "/r/data/comments/1j87kft/struggling_to_extract_meaningful_data_from/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1j846vw",
    "title": "new way for data analysis",
    "selftext": "SimuGen AI is an intelligent business strategy assistant that helps entrepreneurs and companies **test, optimize, and predict** the impact of their decisions before executing them. By combining **historical data, real-time market trends, and AI-driven forecasting**, it allows users to simulate different business strategies—pricing changes, expansion plans, marketing shifts—and instantly see potential outcomes.\n\nWith **dynamic scenario modeling**, businesses can explore \"what-if\" situations, compare strategies, and receive **AI-generated recommendations** to maximize success. Unlike static reports, SimuGen AI continuously adapts to industry trends, offering real-time insights through **interactive dashboards and predictive analytics**.\n\nInstead of relying on gut feelings, decision-makers get **data-backed simulations** to navigate risks, seize opportunities, and make smarter choices—turning uncertainty into strategy.",
    "url": "https://www.reddit.com/r/data/comments/1j846vw/new_way_for_data_analysis/",
    "score": 0,
    "upvote_ratio": 0.43,
    "num_comments": 1,
    "created_utc": 1741627762.0,
    "author": "Murky_Comfort709",
    "subreddit": "data",
    "permalink": "/r/data/comments/1j846vw/new_way_for_data_analysis/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mh60xeg",
        "body": "Thank you for sharing! You are also welcomed to check out our product powerdrill ai. Good luck!",
        "score": 0,
        "created_utc": 1741678224.0,
        "author": "Powerdrill_AI",
        "is_submitter": false,
        "parent_id": "t3_1j846vw",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1j7vef3",
    "title": "Where can I find roleplay-related textual data?",
    "selftext": "Hello, \n\n  \nI'm currently developing LLM assisstant for dungeons and dragons. However I struggle with finding data. Where should I look for them? \n\n  \nBest Regards guys",
    "url": "https://www.reddit.com/r/data/comments/1j7vef3/where_can_i_find_roleplayrelated_textual_data/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1741601868.0,
    "author": "ButterscotchCheap304",
    "subreddit": "data",
    "permalink": "/r/data/comments/1j7vef3/where_can_i_find_roleplayrelated_textual_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mhka8fi",
        "body": "you can start with reddit, and similarly principled sources of d&d discourse. alternatively, books (and similar corpus)",
        "score": 1,
        "created_utc": 1741873029.0,
        "author": "saintmichel",
        "is_submitter": false,
        "parent_id": "t3_1j7vef3",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1j7s8rg",
    "title": "Displaying data from CSV",
    "selftext": "Hello everyone. I am quite new to data processing and would like to request some help. The data I am working on are CSV files. The files itself are old files that nobody else in my office knows how to use/read.\n\nThe format is usually something like this.  \nThe left column is is the timestamp while the right one is the value of the data itself.\n\nFor this example, while the file itself is named with the date of the data, it is unclear what specific time of day each data is logged on.\n\n|1514822400000,5.88| \n\n|1514822401000,5.63 |\n\nOr\n\n|202501010000.00,4| \n\n|202501010100.00,4 |\n\nWith the second example the timestamp is marked with year, month and date, while the former is written differently and I'm not sure how I'm supposed to read it.\n\nWith these CSV files I can make a graph such as these, using Flow CSV Viewer.\n\nhttps://preview.redd.it/k86h1j6xzsne1.png?width=476&format=png&auto=webp&s=7181f97a8ce4a1516243dc7357749fb718c15135\n\nAs it is now, I can display the entirety of a dataset or partially, but it is not clear what time the data is recorded on.\n\nMy question is, is there an application or some other way that can display the date and time of the timestamp instead of the number the timestamp itself has? If anyone knows about this or if there's a more general guide, please tell me, thank you.\n\nEdit: Upon further research I see the  common method is using python to visualize the data, is there a method that uses more application interface like CSV Viewer instead?",
    "url": "https://www.reddit.com/r/data/comments/1j7s8rg/displaying_data_from_csv/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 11,
    "created_utc": 1741587482.0,
    "author": "Xignu",
    "subreddit": "data",
    "permalink": "/r/data/comments/1j7s8rg/displaying_data_from_csv/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mh0ex5l",
        "body": "First timestamps might be epoch time if you're expecting data from 2018?",
        "score": 1,
        "created_utc": 1741609492.0,
        "author": "k00_x",
        "is_submitter": false,
        "parent_id": "t3_1j7s8rg",
        "depth": 0
      },
      {
        "id": "mh1qx82",
        "body": "I use [pandas.to_datetime](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html)",
        "score": 1,
        "created_utc": 1741625166.0,
        "author": "impatient_facility",
        "is_submitter": false,
        "parent_id": "t3_1j7s8rg",
        "depth": 0
      },
      {
        "id": "miapnlx",
        "body": "Flow CSV Viewer actually has a built-in function str\\_datetime to convert Unix timestamps into a readable string. So you can add a formula like time = str\\_datetime(column1), and then drag the time variable to the X axis in top of the plots. Hope this helps!",
        "score": 1,
        "created_utc": 1742234279.0,
        "author": "jerha202",
        "is_submitter": false,
        "parent_id": "t3_1j7s8rg",
        "depth": 0
      },
      {
        "id": "mh0f7c8",
        "body": "Stick it in a python data frame and try to convert it into dates.",
        "score": 1,
        "created_utc": 1741609613.0,
        "author": "k00_x",
        "is_submitter": false,
        "parent_id": "t1_mh0ex5l",
        "depth": 1
      },
      {
        "id": "mjlgivw",
        "body": "Sorry for the very late reply, but is there a manual on how to do t hat?\n\nEDIT: I think I managed to do so and it does indeed show the time. Is it also possible for the function to just display the dates?",
        "score": 1,
        "created_utc": 1742871753.0,
        "author": "Xignu",
        "is_submitter": true,
        "parent_id": "t1_miapnlx",
        "depth": 1
      },
      {
        "id": "mhbbhgm",
        "body": "I've been learning how to use python using notebooks such as Jupyter, is that what you mean?",
        "score": 1,
        "created_utc": 1741743669.0,
        "author": "Xignu",
        "is_submitter": true,
        "parent_id": "t1_mh0f7c8",
        "depth": 2
      },
      {
        "id": "mjwa01r",
        "body": "Great! Yes, just use str\\_date instead of str\\_datetime.",
        "score": 1,
        "created_utc": 1743021098.0,
        "author": "jerha202",
        "is_submitter": false,
        "parent_id": "t1_mjlgivw",
        "depth": 2
      },
      {
        "id": "mjz3fp0",
        "body": "So I tried that but it seems it doesn't work. It just shows a blank line.",
        "score": 1,
        "created_utc": 1743059876.0,
        "author": "Xignu",
        "is_submitter": true,
        "parent_id": "t1_mjwa01r",
        "depth": 3
      },
      {
        "id": "mkf0z85",
        "body": "Sorry, I didn't see at first that your timestamps are milliseconds and not seconds. Please try to divide column1 by 1000, i.e. `datetime = str_datetime(column1 / 1000)`, and `date = str_date(column1 / 1000)`. Hope this helps!",
        "score": 1,
        "created_utc": 1743282636.0,
        "author": "jerha202",
        "is_submitter": false,
        "parent_id": "t1_mjz3fp0",
        "depth": 4
      },
      {
        "id": "mkf1dtg",
        "body": "(BTW, there is also a user support forum that you can access through the Help menu - I respond much quicker there. Happy to help here too - I'm just not here very often.)",
        "score": 1,
        "created_utc": 1743282774.0,
        "author": "jerha202",
        "is_submitter": false,
        "parent_id": "t1_mkf0z85",
        "depth": 5
      },
      {
        "id": "mkne1yv",
        "body": "It totally works, thanks a lot!",
        "score": 1,
        "created_utc": 1743405481.0,
        "author": "Xignu",
        "is_submitter": true,
        "parent_id": "t1_mkf0z85",
        "depth": 5
      }
    ],
    "comments_extracted": 11
  },
  {
    "id": "1j7dzt9",
    "title": "Help me taper my expectations",
    "selftext": "Ive applied to hundreds of jobs that are WFH and have gotten a few interviews but no offers (yet atleast) but im considering switching gears and branching out into a hybrid role \n\nSo help me taper my expectations, what has your experience been with interviewing for hybrid data roles? Are you getting more interviews for hybrid jobs or WFH jobs? Or is the job market just bad everywhere we look right now lol",
    "url": "https://www.reddit.com/r/data/comments/1j7dzt9/help_me_taper_my_expectations/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 0,
    "created_utc": 1741544742.0,
    "author": "ExcellentLog5789",
    "subreddit": "data",
    "permalink": "/r/data/comments/1j7dzt9/help_me_taper_my_expectations/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1j6mfy6",
    "title": "TimeSeries forcasting with Prophet",
    "selftext": "Hi, I am using as my predictable (y) sum of three numbers that define usage of some app (audio time, chat messages and some other) is that a good practice in this situation? Also have data for 6 months (day by day) is that enough to train prophet model or should I start looking for other models? Other advices would be appreciated to, since this is project for my master thesis. :)",
    "url": "https://www.reddit.com/r/data/comments/1j6mfy6/timeseries_forcasting_with_prophet/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1741455768.0,
    "author": "djoule53",
    "subreddit": "data",
    "permalink": "/r/data/comments/1j6mfy6/timeseries_forcasting_with_prophet/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mgptq5n",
        "body": "Prophet builds a seasonal index so it's best if you have at least one sample of each season, or one year.  Are you able to get more data?  Did you aggregate all your data and fill any gaps?",
        "score": 2,
        "created_utc": 1741456408.0,
        "author": "Confident-Ant-8972",
        "is_submitter": false,
        "parent_id": "t3_1j6mfy6",
        "depth": 0
      },
      {
        "id": "mgq71pi",
        "body": "Actually this is for school app, so i have ending of summer break to ending of break of first semester and starting of new semestar (second one), so there are some seasonality. I am not able unfortunatly, maybe I can make some fake. I didn't understood the last question, but maybe the answer is that I don't have null values for this period of 6 mknths or 180 rows of daily data.",
        "score": 1,
        "created_utc": 1741460495.0,
        "author": "djoule53",
        "is_submitter": true,
        "parent_id": "t1_mgptq5n",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1j6idxx",
    "title": "Loading and merging csv",
    "selftext": "So I'm currently doing final year project for that my mentor shared me 11gb of data which contains 150 CSV files ,how should I merge them and perform task further . I guess performing task on 150csv files at once will require some heavy computing system but I only 12gb ram .what I'm thinking that after merging I can split them into 30 datasets or maybe before merging I can work first 30 the other 30s ? . Thank you :)",
    "url": "https://www.reddit.com/r/data/comments/1j6idxx/loading_and_merging_csv/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1741444385.0,
    "author": "Far-Palpitation4482",
    "subreddit": "data",
    "permalink": "/r/data/comments/1j6idxx/loading_and_merging_csv/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mgotpu3",
        "body": "Try DUCK DB , and see if you convert those CSV in parquet files which would reduce size and then you can process them at one go",
        "score": 2,
        "created_utc": 1741445069.0,
        "author": "MiddleSale7577",
        "is_submitter": false,
        "parent_id": "t3_1j6idxx",
        "depth": 0
      },
      {
        "id": "mgrqdsk",
        "body": "Without knowing anything about your project my question would be whether you need to merge them. If you are performing some function on every line, sorting them only to get specific lines, or summarizing them then I would merge them I’d write a function to do what I want on a per file basis then call it once per file. Depending on what you’re doing it could take an hour or a day. If you’re doing machine learning and can afford a few bucks, buy done cloud space and do it there.",
        "score": 1,
        "created_utc": 1741478803.0,
        "author": "amosmj",
        "is_submitter": false,
        "parent_id": "t3_1j6idxx",
        "depth": 0
      },
      {
        "id": "mgtzcdu",
        "body": "Download the trial version of Alteryx and hoover them up with that !",
        "score": 1,
        "created_utc": 1741515812.0,
        "author": "Mr-Gothika",
        "is_submitter": false,
        "parent_id": "t3_1j6idxx",
        "depth": 0
      },
      {
        "id": "mgovcgm",
        "body": "Will try :)",
        "score": 1,
        "created_utc": 1741445635.0,
        "author": "Far-Palpitation4482",
        "is_submitter": true,
        "parent_id": "t1_mgotpu3",
        "depth": 1
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "1j5snlh",
    "title": "Looking for mods",
    "selftext": "Anyone interested in modding - mainly your job would be to remove the spam posts masquerading as “content”",
    "url": "https://www.reddit.com/r/data/comments/1j5snlh/looking_for_mods/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 5,
    "created_utc": 1741364989.0,
    "author": "heresacorrection",
    "subreddit": "data",
    "permalink": "/r/data/comments/1j5snlh/looking_for_mods/",
    "is_self": true,
    "distinguished": "moderator",
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mgoam8q",
        "body": "I'm interested and have experience modding both large and small subs.",
        "score": 2,
        "created_utc": 1741437440.0,
        "author": "double_dose_larry",
        "is_submitter": false,
        "parent_id": "t3_1j5snlh",
        "depth": 0
      },
      {
        "id": "mhd0bvr",
        "body": "I'm interested, I'd like to help make some good community guides/pages too!",
        "score": 1,
        "created_utc": 1741774529.0,
        "author": "PaperMoonsOSINT",
        "is_submitter": false,
        "parent_id": "t3_1j5snlh",
        "depth": 0
      },
      {
        "id": "mivnkah",
        "body": "I'm interested! This is my new account, but I had a lot of mod experience on my previous one.",
        "score": 1,
        "created_utc": 1742510718.0,
        "author": "VDCArchitect",
        "is_submitter": false,
        "parent_id": "t3_1j5snlh",
        "depth": 0
      },
      {
        "id": "mgktpq9",
        "body": "I'm interested!",
        "score": 0,
        "created_utc": 1741382256.0,
        "author": "bigshirtjonny",
        "is_submitter": false,
        "parent_id": "t3_1j5snlh",
        "depth": 0
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "1j51xnf",
    "title": "Best Courses/Resources for Becoming a Data Analyst (Have BSc in CS & Programming Knowledge)",
    "selftext": "Hey everyone,\n\nI have a BSc in Computer Science and a decent programming background (Python, SQL, etc.). I'm looking to transition into a Data Analyst role and want to make sure I'm learning the right skills.\n\nWhat are the best courses (free or paid) or learning paths for someone in my position? I want to focus on real-world data analysis, visualization, and business intelligence.\n\nWould love any recommendations on platforms like Coursera, Udemy, DataCamp, etc., or general advice on what skills to prioritize.\n\nThanks in advance!",
    "url": "https://www.reddit.com/r/data/comments/1j51xnf/best_coursesresources_for_becoming_a_data_analyst/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1741284721.0,
    "author": "Flimsy_Bee3707",
    "subreddit": "data",
    "permalink": "/r/data/comments/1j51xnf/best_coursesresources_for_becoming_a_data_analyst/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mh6fzgx",
        "body": "Following are some high-rated resources for get started in data analysis.\n\n\\-[Top-Rated Data Udemy Courses](https://www.jaffainc.com/Entrepreneur-courses.html#SQLbootcamp) : (SQL, Power B, Excel, Python, R, Data Analysis)\n\n\\-[SQL Certificate Courses \"with an Instructor\"](http://www.iwanttolearnsql.com)\n\nGood luck!",
        "score": 1,
        "created_utc": 1741687820.0,
        "author": "Sea-Concept1733",
        "is_submitter": false,
        "parent_id": "t3_1j51xnf",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1j4ykws",
    "title": "Looking for the easiest way to create a list and then pivot from a decent size data set. Combining my love of MtG and excel.",
    "selftext": "MtG Nerds - I'm working on my sliver edh deck and trying to optimize my manabase. I've decided to include 3 fetchable trilands and I'm wondering which combination of the 3 allows me to cast the greatest number of dual-colored slivers in the deck. For example I could cast Dormant sliver off of Raffine's Tower and Jetmir's Garden, but not Raffine's Tower and Xander's Lounge. I'm looking to put each of the trilands into a spreadsheet that spits out all the color combinations that each combination of 3 trilands can produce. Then put that list into a pivot to filter for the ones that match the dual-color slivers I'm running.  Is this vital to deckbuilding, no. But my excel brain has now taken over control of the project just to see if it can be done.\n\nExcel Nerds - I have 10 cards that each produce 3 different colors. There are 5 total colors in the game, and none of the 10 cards repeat colors, each card is unique and I'm only using 1 of each of the unique cards. I'm looking to create a sheet where I can input each of the 3 colors that each card produces, and figure out what combinations of 2 colors are produced by combinations of 3 cards. Each card can only contribute once for a given color pair.\n\nThere are 10c3 = 120 card combinations, and each combination of cards can produce 3x3x3 = 27 different color pairs. So that's 3240 different 2-color combinations to start.\n\nFor example if card 1 produces colors A,B,C, card 2 produces colors A,D,E and Card 3 produces colors B,C,D then the combination of all 3 cards can produce 27 different combinations of color pairs (including duplicates) - AA, AD, AE, BA, BD, BE, CA, CD, CE, AB, AC, AD, BB, BC, BD, CB, CC, CD, AB, AC, AD, DB, DC, DD, EB, EC, ED.\n\nOn top of the above, I'd also like to filter out repeats where 2 cards share 2 colors. For example with the cards above, cards 1 and 3 can produce BC and CB. I'd prefer to only count that once, as it is the same 2 cards producing the same color combination.\n\nTIA for any suggestions, and hello to all with overlapping hobbies!\n\nEdit: Forgot to mention, I've gotten as far as creating the list of all 3240 combinations, and I'm manually reviewing each of the 120 3-card combos to weed out the repeats. Hoping for a faster/easier way.",
    "url": "https://www.reddit.com/r/data/comments/1j4ykws/looking_for_the_easiest_way_to_create_a_list_and/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1741276425.0,
    "author": "shuflww",
    "subreddit": "data",
    "permalink": "/r/data/comments/1j4ykws/looking_for_the_easiest_way_to_create_a_list_and/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1j51a05",
    "title": "I need US death record data",
    "selftext": "Hey I’m a AI agent developer and one of my client tasked me with a automation system that will notify family members if someone from their family has passed away. The system will take their names and other information to check public death records to check for any match. But I could not find any database containing all the latest death record at least not for a third party to check without submitting an application and paying a fee upfront ( which is not the goal for this automation). Now is there any publicly available record that is up-to date and which I can use as a source for this automation? I’m a non USA citizen so I am not fully aware of their public record system. Can any one help me with that ? \n\nWhat I need : \n1. Publicly searchable death records by ( name, location, age or security number)\n2. Up-to date data ( as the automation is aiming for a alert system for the family members) \n\nNote : \nI have checked cdc.gov and this requires application submission and a upfront fee to check. And I have also checked archives.com and truth finder but I’m not so sure that the data  will be as accurate as government data. ",
    "url": "https://www.reddit.com/r/data/comments/1j51a05/i_need_us_death_record_data/",
    "score": 1,
    "upvote_ratio": 0.6,
    "num_comments": 1,
    "created_utc": 1741283124.0,
    "author": "graphite1212",
    "subreddit": "data",
    "permalink": "/r/data/comments/1j51a05/i_need_us_death_record_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mghswqf",
        "body": "same but I need this to buy computers of stay at home 30-40 year olds, if they die, for bitcoin wallets",
        "score": 3,
        "created_utc": 1741347188.0,
        "author": "andreabarbato",
        "is_submitter": false,
        "parent_id": "t3_1j51a05",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1j4d4g7",
    "title": "Best way to track Reddit content performance?",
    "selftext": "Hello!\n\nI am creating content on Reddit and I would like to be able to track the performance of posts based on time of day and the content itself. The tags used, popularity, etc. The post insights are helpful but there is not a way to turn that stuff into data, at least none that I've found. I also know that the API is not really accessible, which is fine! I don't need an automated program, I just would like to be able to put in the data of how popular a post is and have some kind of tagging system to reflect what content is the most popular.\n\nI'm having a hard time finding templates for this and I know Reddit's insights go away after 45 days and it's already been 20 since I started making content. If anyone has any templates, I am willing to try anything. I want to do a really good job with this and I would love to have a dataset that helps me do that.\n\nThanks for any help!\n\nEdit: also I know the insights give me a percentage of upvotes vs downvotes and I can do that math based on that but if there's a way to just see the number of downvotes, that would also be helpful.",
    "url": "https://www.reddit.com/r/data/comments/1j4d4g7/best_way_to_track_reddit_content_performance/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1741206717.0,
    "author": "VictorCrowneAudio",
    "subreddit": "data",
    "permalink": "/r/data/comments/1j4d4g7/best_way_to_track_reddit_content_performance/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1j4ajg5",
    "title": "Best map making tool for disease tracking in orchards?",
    "selftext": "Hey everyone, I’m posting this in a few different subreddits (looking to get as many different ideas on the best way to do this.)\n\nHere’s what I’ve got: \n\nCherry tree orchids (referred to as “blocks”) that we (small-ish family farm) want to be able to have block maps that we can track infected trees/ removed (because of specific disease) trees/ have some kind of color coding so it’s easy to see those patterns of how the disease is moving through the block and any “hotspots” within each block.   \n\n\nCurrently we’re trying to use excel to make a simple grid map with each cell being one tree space. The problem(s) is that it doesn't allow us to put much data (we could have colors/ bold or not font/ font size all have their own meanings (i.e. infected, empty space, etc) but that still seems too clunky/ too much to look at. It would be great to be able to turn layers off (only look at infected trees, or trees we've tested in specific years, etc)\n\nI know there has to be something out there that is less time consuming to set up, easier to manage, and better suited to holding/ displaying all the information we need it to have. \n\nIf this works well enough we’d probably also eventually use it to track insects (both pests and beneficials) and nutrient distribution in the blocks.   \n\n\nIdeally any program we’d use wouldn’t be too expensive, and wouldn’t require too beefy of a computer to run. ",
    "url": "https://www.reddit.com/r/data/comments/1j4ajg5/best_map_making_tool_for_disease_tracking_in/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1741200498.0,
    "author": null,
    "subreddit": "data",
    "permalink": "/r/data/comments/1j4ajg5/best_map_making_tool_for_disease_tracking_in/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mgmzzbz",
        "body": "I feel like your mental model of this problem might be a little off and while the comments on the other versions of this post offer some valid methods (mainly qgis) I don’t feel like that’s exactly what you’re looking for. \nI can take a stab at writing a python script that I think would accomplish what you need but I need some extra details. How many blocks would you be dealing with and how are they arranged? Are trees randomly spread throughout or are they planted in grid patterns. What data  is currently collected about the trees and how? (do you have the date each tree was planted, notes about times when they are inspected/ watered/ etc). \nIf current data collection methods are lacking I could also offer some advice for simple record keeping that would help out with later analysis. Any details, data, images/ layout drawings would be appreciated.",
        "score": 1,
        "created_utc": 1741410547.0,
        "author": "MostlyUnnoticedGhost",
        "is_submitter": false,
        "parent_id": "t3_1j4ajg5",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1j3u68h",
    "title": "Revenue by quarter API",
    "selftext": "Does anyone know where I can get the reported total revenue using an API? I am trying to get Q4 of 2024 on NU bank. The number on Trading View seems to be 2.99 B when you scroll down on Financials > Overview > scroll to estimates, but when I use financial modeling prep I'll get very off numbers like 1.04 billion on the earnings report. I've tried Yahoo Finance which also gives me 1.04 billion, and Alpha Vantage which gives me 1.04 Billion. \n\n  \nEven when I go to quarterly income statements on Trading View the number I get is 2.71B. \n\nI've also gone to the investor page on the website and sure enough, I see 2.99B. \n\nThis should be a straightforward number to get, not sure why this is giving me so much trouble.",
    "url": "https://www.reddit.com/r/data/comments/1j3u68h/revenue_by_quarter_api/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1741146448.0,
    "author": "Jamaicandeathmetal",
    "subreddit": "data",
    "permalink": "/r/data/comments/1j3u68h/revenue_by_quarter_api/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1j2raon",
    "title": "Help with data extraction/acquisition",
    "selftext": "Is this really possible?!\n\nI am a 4th year student and preparing my dissertation proposal. I plan on making a ML model based on parameters from 4 different databases, which are Drug bank, ProtParam, Uniprot and PSORTb. I want to exract protein target information (features) across these databases and get a single file to train my model to be able to detect novel protein targets against 4 bacterial species. There is a python script I have which is supposed to get all this infor for me and neatly pack it into a CSV file but it's not working.\n\nAny help, advice or alternative databases that integrate for getting all this infor would be appreciated. Or even help with the project or some form of supervision, the proposal is needed this Friday and I'm stunned. Help!",
    "url": "https://www.reddit.com/r/data/comments/1j2raon/help_with_data_extractionacquisition/",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 0,
    "created_utc": 1741030625.0,
    "author": "Terrible_Molasses862",
    "subreddit": "data",
    "permalink": "/r/data/comments/1j2raon/help_with_data_extractionacquisition/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1j2dvt2",
    "title": "Should I stay in my current role or start looking for a new job?",
    "selftext": "I currently work as a Junior Performance Analyst within a \"product\" in a large company.  In my department, there is no one else working with data the way I do. This is an advantage because I have the opportunity to become a reference in this area, but it's also a disadvantage since there is no one to guide me in a more precise and specific way. Given my personal career plan—to become a Data Analyst—how long should I keep pursuing this role within this company?\n\nI joined very recently and have just taken on a project to develop an automation and a dashboard for my team, which is currently part of my responsibilities. However, once I finish the automation and dashboards, I will no longer have as many data-focused tasks.",
    "url": "https://www.reddit.com/r/data/comments/1j2dvt2/should_i_stay_in_my_current_role_or_start_looking/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1740989458.0,
    "author": "Upbeat-Minute-4916",
    "subreddit": "data",
    "permalink": "/r/data/comments/1j2dvt2/should_i_stay_in_my_current_role_or_start_looking/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mfrl056",
        "body": "Never any harm in doing a little professional window shopping. It may help clarify what you want and need. I would also look for user groups, centers of excellence, and mentors within your company on the stuff you are working on. Not everyone gets to be on a full team but sometimes we have a support system outside our team.",
        "score": 2,
        "created_utc": 1741002148.0,
        "author": "amosmj",
        "is_submitter": false,
        "parent_id": "t3_1j2dvt2",
        "depth": 0
      },
      {
        "id": "mfsqzj2",
        "body": "You can start looking for better options while staying in your current job. I spent three years in the same area, doing more than my collegues, but after applying for different roles I finally received I really good offer. More salary for less worked hours. They took in count my experience in my previous job.",
        "score": 2,
        "created_utc": 1741018117.0,
        "author": "0alexmyz0",
        "is_submitter": false,
        "parent_id": "t3_1j2dvt2",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1j25e27",
    "title": "Data Science or machine learning engineering?",
    "selftext": "I'm an Information Systems undergraduate with experience in data analysis and a background in a junior enterprise.\n\nI don’t want to continue in data analysis because, in my opinion, AI will eventually replace this profession. However, I have an optimistic outlook on Data Science (DS) and Machine Learning Engineering (MLE).\n\nBetween DS and MLE, which do you think will have greater longevity in the job market and a lower entry barrier?",
    "url": "https://www.reddit.com/r/data/comments/1j25e27/data_science_or_machine_learning_engineering/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1740960014.0,
    "author": "leoporra",
    "subreddit": "data",
    "permalink": "/r/data/comments/1j25e27/data_science_or_machine_learning_engineering/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mt2lx1f",
        "body": "What is the difference between them ? This question should clear your doubt .",
        "score": 1,
        "created_utc": 1747629708.0,
        "author": "Majestic_Ice_1891",
        "is_submitter": false,
        "parent_id": "t3_1j25e27",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1j1npu7",
    "title": "What project should I work on?",
    "selftext": "\nHey everyone,\n\nI’m looking to apply for an AI/ML internship, but I’m stuck on what project to work on next. So far I’ve covered a good amount NLP models like ANN, RNN, LSTM, BiLSTM, Encoder-Decoder, and Transformers, along with architectures like AlexNet, BERT, and a few others.\n\nI want to build something that not only sharpens my skills but also makes my application stand out. If you’ve landed an internship in AI/ML before or have any project ideas that could help, I’d love to hear your thoughts\n",
    "url": "https://www.reddit.com/r/data/comments/1j1npu7/what_project_should_i_work_on/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1740910286.0,
    "author": "Ok_Bluebird_5291",
    "subreddit": "data",
    "permalink": "/r/data/comments/1j1npu7/what_project_should_i_work_on/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1j0o7e0",
    "title": "How to remove personal data online?",
    "selftext": "Guys I am seriously dumb when it comes to these stuff, I really need someone's help. I found a few websites like deleteme and aura that claim to remove data like LLC info, phones and emails online but I have no idea if they are legit or not. Are they worth it? I am not trying to be fully anonymous but I found some data leak in sites like rocketreach and even asking chatGPT, please help, don't laugh",
    "url": "https://www.reddit.com/r/data/comments/1j0o7e0/how_to_remove_personal_data_online/",
    "score": 3,
    "upvote_ratio": 0.8,
    "num_comments": 8,
    "created_utc": 1740793329.0,
    "author": "Anajac",
    "subreddit": "data",
    "permalink": "/r/data/comments/1j0o7e0/how_to_remove_personal_data_online/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mfd25hu",
        "body": "I think you have to give them your data in order to help you. That is off putting enough for me.",
        "score": 3,
        "created_utc": 1740794245.0,
        "author": "BadAccomplished165",
        "is_submitter": false,
        "parent_id": "t3_1j0o7e0",
        "depth": 0
      },
      {
        "id": "mg9i3ni",
        "body": "I paid for a year of DeleteMe and I have been pretty impressed.  They send me quarterly reports showing where they removed information.",
        "score": 2,
        "created_utc": 1741228771.0,
        "author": "moviedodd",
        "is_submitter": false,
        "parent_id": "t3_1j0o7e0",
        "depth": 0
      },
      {
        "id": "mrpu261",
        "body": "It’s completely understandable to feel overwhelmed when dealing with online privacy concerns! Removing personal data can be tricky, especially when you’re not sure which services are reliable.\n\nIf you’re looking for more detailed guidance on how to remove your personal information from the internet, there’s an insightful post that breaks down the best options: [The Best Personal Data Removal Services for 2025 ](https://www.reddit.com/r/Privacy360/comments/1iusp8f/the_best_personal_data_removal_services_for_2025/?spm=a2ty_o01.29997173.0.0.30f7c921yPSF4P). It covers popular tools like DeleteMe, Incogni, and more, along with their pros and cons.\n\nFor those who have used any of these services, what has been your experience? Is it worth the investment?",
        "score": 1,
        "created_utc": 1746950220.0,
        "author": "Patient-Fly9676",
        "is_submitter": false,
        "parent_id": "t3_1j0o7e0",
        "depth": 0
      },
      {
        "id": "n13d68e",
        "body": "No worries at all—this is a really important topic, and it’s great that you’re taking steps to protect your personal data. You’re definitely not alone in feeling overwhelmed by this stuff, so no need to stress!\n\nServices like **DeleteMe** and **Incogni** are two of the most popular options for removing your info from data brokers and people-search sites. They work by opting you out of databases that collect and sell your personal details (like RocketReach or similar platforms). While DeleteMe focuses more on comprehensive removal from major data brokers, Incogni is known for its GDPR-based approach, which can be particularly effective in the EU.\n\nIf you’re unsure which service might work best for you, there’s a detailed comparison of these tools here: [The Best Personal Data Removal Services for 2025 ](https://www.reddit.com/r/Privacy360/comments/1iusp8f/the_best_personal_data_removal_services_for_2025/). It breaks down their features, pricing, and effectiveness to help you decide.\n\nFor those who’ve used DeleteMe or Incogni, what has been your experience? Did they help reduce your exposure online?",
        "score": 1,
        "created_utc": 1751531385.0,
        "author": "Patient-Fly9676",
        "is_submitter": false,
        "parent_id": "t3_1j0o7e0",
        "depth": 0
      },
      {
        "id": "mfd3f1j",
        "body": "Incogni",
        "score": 0,
        "created_utc": 1740794694.0,
        "author": "patrick_schliesing",
        "is_submitter": false,
        "parent_id": "t3_1j0o7e0",
        "depth": 0
      },
      {
        "id": "mfewppt",
        "body": "Full disclosure, I am part of the Optery Team. Optery is a data removal service like DeleteMe and Aura. There are plenty other data removal services, and basically, data removal services help scrub info from people search sites or data brokers like Rocketreach, whitepages, etc. Tho I cannot speak for other data removal services, the reason why Optery asks for data upon using the service is because it is used to run over hundreds of data broker sites. This is a more in-depth explanation: [https://help.optery.com/en/article/why-does-optery-need-so-much-of-my-information-ry0lom/](https://help.optery.com/en/article/why-does-optery-need-so-much-of-my-information-ry0lom/)\n\nIf you don't want to share your personal info, you can actually opt-out from these data brokers yourself: [DIY opt-out guide](https://www.optery.com/opt-out-guides/). The advantage of data removal services would be the simultaneous removals from hundreds of data brokers.",
        "score": 2,
        "created_utc": 1740826659.0,
        "author": "choco_titan-07",
        "is_submitter": false,
        "parent_id": "t1_mfd25hu",
        "depth": 1
      },
      {
        "id": "mgesxsg",
        "body": "Interesting! Where do they remove your data from",
        "score": 1,
        "created_utc": 1741299951.0,
        "author": "Anajac",
        "is_submitter": true,
        "parent_id": "t1_mg9i3ni",
        "depth": 1
      },
      {
        "id": "mgf82n2",
        "body": "Many, many data broker sites, like 411.com.",
        "score": 2,
        "created_utc": 1741305083.0,
        "author": "moviedodd",
        "is_submitter": false,
        "parent_id": "t1_mgesxsg",
        "depth": 2
      }
    ],
    "comments_extracted": 8
  },
  {
    "id": "1j0dcha",
    "title": "Hi Data people, We (Rollstack) are giving away a $2,000 gift card to one lucky data person. Attend a demo to get 5 extra entries. (Obvs void where prohibited. Rules apply. See site for details",
    "selftext": "",
    "url": "https://www.rollstack.com/articles/vive-2025-rollstack-giveaway?utm_source=reddit&utm_medium=social&utm_campaign=giveaway-feb-2025-reddit-data",
    "score": 2,
    "upvote_ratio": 0.67,
    "num_comments": 0,
    "created_utc": 1740764082.0,
    "author": "Rollstack",
    "subreddit": "data",
    "permalink": "/r/data/comments/1j0dcha/hi_data_people_we_rollstack_are_giving_away_a/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1izppfv",
    "title": "Open source marketing campaign or audience insights data?",
    "selftext": "My background is in insights and market research. I'm currently job hunting and I'm seeing a lot of roles in audience insights and **marketing** research, which I don't have direct experience in. I was thinking about trying to do some small projects to include in my applications to show I have transferrable skills, but I'm struggling to find open source data to work with. Does anyone have any suggestions? Thanks so much.",
    "url": "https://www.reddit.com/r/data/comments/1izppfv/open_source_marketing_campaign_or_audience/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1740688346.0,
    "author": "belledamesans-merci",
    "subreddit": "data",
    "permalink": "/r/data/comments/1izppfv/open_source_marketing_campaign_or_audience/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1izdswc",
    "title": "Data literacy escape room",
    "selftext": "Hello, I need some help.\n\nSome colleague and I are building an escape room to help teach colleagues about data literacy.\n\nThe idea is a murder mystery where we have 10 characters all mapped out with varying characteristics.\n\nWhat I want to do is not pick a killer but have the players decide on who they think the killer could be on different points of data. So each play through is different and varied.\n\nI’d love to hear your thoughts and ideas on how we could do this or any other thoughts you may have.",
    "url": "https://www.reddit.com/r/data/comments/1izdswc/data_literacy_escape_room/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1740656448.0,
    "author": "megablocks516",
    "subreddit": "data",
    "permalink": "/r/data/comments/1izdswc/data_literacy_escape_room/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mfe70n0",
        "body": "Neat idea but I'd spend this money on videography instead.",
        "score": 1,
        "created_utc": 1740811266.0,
        "author": "Alternative_Top2875",
        "is_submitter": false,
        "parent_id": "t3_1izdswc",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1izg7gs",
    "title": "Need Help selling data",
    "selftext": "Hey Guys\n\nSo, I have worked as an AI tutor and have a large amount of voice recordings used to train a LLM for voice recognition, and I want to sell that data. I would say, I have around 10000+ voice recordings of humans as users and scripts that are being used. Can anyone help me with how much it might be worth and who or where I can approach to sell them? Also, is it even legal to do it and would it be right to do it?",
    "url": "https://www.reddit.com/r/data/comments/1izg7gs/need_help_selling_data/",
    "score": 0,
    "upvote_ratio": 0.33,
    "num_comments": 0,
    "created_utc": 1740664161.0,
    "author": "Legal-Masterpiece372",
    "subreddit": "data",
    "permalink": "/r/data/comments/1izg7gs/need_help_selling_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1iyzj5f",
    "title": "How can I keep data that I’ve added to cart on PSID from disappearing?",
    "selftext": "Hello.\nSo, I have a preliminary presentation due of some descriptive statistics of the topic I’ve chosen. \nHowever, for the past three days, each day, including today, I’ve been adding data to my cart, then maybe I take a little break (maybe 2-3 hours) or am just logged out automatically from my account, and then the data is not in my cart anymore, even though before, I would check my cart every once in a while while being logged in to make sure everything was there, and it was, but not anymore. \nWhat can I do to avoid this? \nI’ve spent almost the whole day on this for it all to disappear. ",
    "url": "https://www.reddit.com/r/data/comments/1iyzj5f/how_can_i_keep_data_that_ive_added_to_cart_on/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1740607119.0,
    "author": "jenny-0515",
    "subreddit": "data",
    "permalink": "/r/data/comments/1iyzj5f/how_can_i_keep_data_that_ive_added_to_cart_on/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1iyxgmu",
    "title": "Gold options (HELP PLEASE)",
    "selftext": "Does anybody know if I can retrieve data for european call/put gold options? If anyone knows a Bloomberg ticker for it then share please I urgently need it. ",
    "url": "https://www.reddit.com/r/data/comments/1iyxgmu/gold_options_help_please/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1740601908.0,
    "author": "AfraidBlacksmith2483",
    "subreddit": "data",
    "permalink": "/r/data/comments/1iyxgmu/gold_options_help_please/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ixcxxh",
    "title": "finding social media profiles",
    "selftext": "Is there a way to do this by using their email address?\n\nWarmer outreach ",
    "url": "https://www.reddit.com/r/data/comments/1ixcxxh/finding_social_media_profiles/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1740431044.0,
    "author": "lookingforananswer23",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ixcxxh/finding_social_media_profiles/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1iwylbr",
    "title": "Data Ethics",
    "selftext": "We have seen governments take aggressive steps to delete, extract and undermine data and data integrity across US federal institutions.\n\nThough this is not a political but a practical question. What can / should data analysts of sound integrity and principle do to hamper or halt the aggressive and subversive moves by government and non government actors to destroy data and the objective insight derived from it. \n\nFor example if a government or gov sponsored fan club sent squads of inexperienced coders to hack, extract. and splat data tables. \n\nDo us Data folks at the insight end of the spectrum have any power to protect ‘truth’ when systems are overridden, people are coerced and data protection, governance and security etc. fails? \n\n",
    "url": "https://www.reddit.com/r/data/comments/1iwylbr/data_ethics/",
    "score": 0,
    "upvote_ratio": 0.33,
    "num_comments": 0,
    "created_utc": 1740391196.0,
    "author": "Extension_Dog_7867",
    "subreddit": "data",
    "permalink": "/r/data/comments/1iwylbr/data_ethics/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1iws7nm",
    "title": "Help Me !",
    "selftext": "For a personal data analysis project, I want to predict revenue potential for the following medical devices in the next 20 years:\n\n1. Medical AI for FDA Approval\n2. On-Device Medical AI\n3. Remote Medical Equipment\n4. Urodynamic Testing Equipment\n5. Laser Equipment for Prostate Surgery and Ureteral Stone Fragmentation\n6. Handheld Parathyroid Examination Device\n7. Cervical Cancer Screening and Treatment Device\n8. AI-Assisted Knee Joint Surgical Robotic System\n9. Disposable Flexible Endoscopy Equipment\n10. Multi-Wavelength Light Source Device for Internal Surgery\n\nWhat do you think is the best way to do this? I am also having trouble finding specific data for each device. Any recommendations?",
    "url": "https://www.reddit.com/r/data/comments/1iws7nm/help_me/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1740366547.0,
    "author": "BandicootSouth8668",
    "subreddit": "data",
    "permalink": "/r/data/comments/1iws7nm/help_me/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1iwrmt2",
    "title": "Ways to learn data-related technical skills?",
    "selftext": "So a bit of a background on me:\n\nI am a freshman college student at a fairly large D1 university with a major in business analytics. I actually came into university as undecided, but have been considering analytics for a while now.\n\nLast semester I took an entry level programming class that went over basic functions of Python and SQL and found that I actually have a pretty good knack for that stuff. I was wondering what are some ways I can learn data analytics skills outside of the classroom, as I probably won't be starting the courses for my major until next year.\n\nI heard decent stuff about the Google Data Analytics certification but I'm not sure if it's helpful professionally and I would rather pursue a free option that is self paced.\n\nIf I could get some reources on some places to start, I would greatly appreciate it! Anything helps.",
    "url": "https://www.reddit.com/r/data/comments/1iwrmt2/ways_to_learn_datarelated_technical_skills/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 5,
    "created_utc": 1740364740.0,
    "author": "OrangeTallion",
    "subreddit": "data",
    "permalink": "/r/data/comments/1iwrmt2/ways_to_learn_datarelated_technical_skills/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mehs6nw",
        "body": "You can browse [this site](https://www.jaffainc.com/Entrepreneur-courses.html) which contains high rated Udemy courses on data science.\n\nGood luck!",
        "score": 1,
        "created_utc": 1740390425.0,
        "author": "Sea-Concept1733",
        "is_submitter": false,
        "parent_id": "t3_1iwrmt2",
        "depth": 0
      },
      {
        "id": "mehtkvy",
        "body": "Data Camp is pretty good and accessible resource. It’s worth the investment.",
        "score": 1,
        "created_utc": 1740391276.0,
        "author": "Extension_Dog_7867",
        "is_submitter": false,
        "parent_id": "t3_1iwrmt2",
        "depth": 0
      },
      {
        "id": "mf2wg9m",
        "body": "Hey! As you already know some Python and SQL, the best way to improve is through hands-on practice. Practice writing queries and analyzing real datasets. Work on data cleaning, visualization, and basic reporting.\n\nAlso, learn pivot tables and dashboards in Excel. Plus, try free tools for creating visual reports.\n\nRegarding certifications - it’s helpful but not necessary. Free resources and self-paced learning work just as well.\n\t\nThe key is doing! Good luck",
        "score": 1,
        "created_utc": 1740669020.0,
        "author": "Interesting_Pie_2232",
        "is_submitter": false,
        "parent_id": "t3_1iwrmt2",
        "depth": 0
      },
      {
        "id": "meiqf98",
        "body": "By any chance do you know if I get access to it for free using my university email?",
        "score": 1,
        "created_utc": 1740406459.0,
        "author": "OrangeTallion",
        "is_submitter": true,
        "parent_id": "t1_mehtkvy",
        "depth": 1
      },
      {
        "id": "mej80uk",
        "body": "I am not sure - it’s very good value - a couple of hundred dollars a year. Take a look at the website it’s very easy to use and navigate.",
        "score": 1,
        "created_utc": 1740412040.0,
        "author": "Extension_Dog_7867",
        "is_submitter": false,
        "parent_id": "t1_meiqf98",
        "depth": 2
      }
    ],
    "comments_extracted": 5
  },
  {
    "id": "1iw0ud6",
    "title": "COOP apprenticeship",
    "selftext": "Hello everyone, I just started my co-op program for Data Analytics through the co-op apprenticeship. Has anyone here taken it and successfully found a job? What was your experience?",
    "url": "https://www.reddit.com/r/data/comments/1iw0ud6/coop_apprenticeship/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1740281292.0,
    "author": "Acceptable-Hat-8249",
    "subreddit": "data",
    "permalink": "/r/data/comments/1iw0ud6/coop_apprenticeship/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ivyd6e",
    "title": "Data Enthusiasts shirts",
    "selftext": "👋 Hello, Data Enthusiasts!  \n\nWe hope your datasets are clean, your visualizations are stunning, and your coffee is strong! ☕📊 (And if not, don’t worry—your data just has *character*, right?)  \n\nWe’re Code Culture, a small business run by a team of tech-loving nerds who are passionate about creating fun, stylish apparel and accessories for people like YOU—data analysts, coders, and tech pros who make the digital world go ‘round.  \n\nFrom tees that say *“SELECT * FROM weekend WHERE fun = TRUE;”* to hoodies that declare *“I’m not lazy, I’m in energy-saving mode,”* we’ve got something for every data wizard and coding hero out there.  \n\n👉 If you’d like to check out our collection, you can find us here: www.codeculture.store\n\n**To the admins**: We hope it’s okay to share this here! If not, please let us know, and we’ll happily adjust. 🙏  \n\n\nThanks for letting us introduce ourselves, and we’d love to hear from you! Let’s keep the data (and the laughs) flowing. 💻🎉  \n\n#CodeCulture #DataAnalystLife #TechStyle #SmallBusiness  ",
    "url": "https://www.reddit.com/r/data/comments/1ivyd6e/data_enthusiasts_shirts/",
    "score": 0,
    "upvote_ratio": 0.25,
    "num_comments": 0,
    "created_utc": 1740273397.0,
    "author": "North-Cheesecake-350",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ivyd6e/data_enthusiasts_shirts/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ivoo62",
    "title": "Careers in Data",
    "selftext": "Just a quick question seeking some input. I have a BA in economics and a MBA. I work as a Operations Supervisor in the logistics field right now but would like to transition over to something less phsyically demanding and that uses my analytical brain more directly. My current job indirectly uses analytics because I use a lot off reports to seek efficency and improve my operation in order to beat budget objectivs. Anyway, I like to learn and for fun did the Google IT Support program on Coursera and now I am about 1/2 way throught the Google Data Analytics program. Planning to also do the Microsoft program to learn Power BI as well. Today I learned I could go to the University of Arizona Masters of Information System Management program for free through my job due to a substantial discount and a tuition reimbursment program avalible to me at work. I'm just curious what peopls thoughts are about wether I should do this or just do the two Coursera programs get Data+ and a Power BI cert and move on?\n\nJob titles I am intrested in are Data Analyst, Business Analyst, Logistics or Supply Chain Analyst but I also have some intrest in Data Engeneering. I also have a Data Camp subscription and have completed the Data Literacy track and am currently working on the Data Analyst in SQL track. ",
    "url": "https://www.reddit.com/r/data/comments/1ivoo62/careers_in_data/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1740246974.0,
    "author": "bmtrnavsky",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ivoo62/careers_in_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mebw75s",
        "body": "I also did my BA in economics. Picked up a minor in data analytics and took all the econometrics they offered. \n\nFresh out of school I targeted business analyst roles that listed SQL and python as requirements. Data analyst roles was and still are very competitive so I focused on getting professional experience with the right tools, over specific job titles. That’s turned out to absolutely be the right move. I’m about 5 years in and have gone business analyst (company 1) -> business analyst (company 2) -> associate data engineer (company 2) -> senior business data analyst (company 3 acquired company 2)\n\nGiven your MBA and certs your path will probably look different but hopefully this helps give some perspective!",
        "score": 3,
        "created_utc": 1740312983.0,
        "author": "Auggernaut88",
        "is_submitter": false,
        "parent_id": "t3_1ivoo62",
        "depth": 0
      },
      {
        "id": "megn425",
        "body": "Thank you!",
        "score": 1,
        "created_utc": 1740368779.0,
        "author": "bmtrnavsky",
        "is_submitter": true,
        "parent_id": "t1_mebw75s",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1ive8ck",
    "title": "I scraped & analyzed Y Combinator data to understand startup one-liner pitch trends",
    "selftext": "I recently scraped and analyzed data from Y Combinator to understand how start-ups present their business in a single sentence (one-liner). I built an interactive dashboard that highlights:\n\n\n\n\\- The most frequently used words and their evolution over time,\n\n\\- Breakdown by industry and sub-industry,\n\n\\- Major trends that emerge over time.\n\nIf you're looking to gain a better understanding of the start-up ecosystem, refine your own pitch or identify trends that stand out, this analysis could be of real interest to you.\n\nDon't hesitate to let me know if you'd like to know more I'd be delighted to give you a quick demo of the dashboard!  \n(here a preview of the dashboard)  \n\n\nhttps://preview.redd.it/wcjlunpginke1.png?width=1320&format=png&auto=webp&s=9c5f1246efdf7ab64dad399acb7b70cfb5712429\n\n",
    "url": "https://www.reddit.com/r/data/comments/1ive8ck/i_scraped_analyzed_y_combinator_data_to/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1740212939.0,
    "author": "Complete_Tart5651",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ive8ck/i_scraped_analyzed_y_combinator_data_to/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1iv0qo4",
    "title": "Pandas vs SQL for quick data wrangling, where do you stand?",
    "selftext": "I’m a Pandas fan but SQL’s growing on me, I wanna hear your thoughts on both, or if you use other apps let me know!",
    "url": "https://www.reddit.com/r/data/comments/1iv0qo4/pandas_vs_sql_for_quick_data_wrangling_where_do/",
    "score": 7,
    "upvote_ratio": 0.89,
    "num_comments": 9,
    "created_utc": 1740169568.0,
    "author": "timolenain",
    "subreddit": "data",
    "permalink": "/r/data/comments/1iv0qo4/pandas_vs_sql_for_quick_data_wrangling_where_do/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "me1yzil",
        "body": "SQL - indexes, indexes, indexes",
        "score": 5,
        "created_utc": 1740171022.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_1iv0qo4",
        "depth": 0
      },
      {
        "id": "me22h2s",
        "body": "Polars",
        "score": 2,
        "created_utc": 1740172027.0,
        "author": "BuildingViz",
        "is_submitter": false,
        "parent_id": "t3_1iv0qo4",
        "depth": 0
      },
      {
        "id": "me1vh79",
        "body": "Pandas all the way for me, love the flexibility and how it plays nice with Python. SQL’s great for big datasets, though, especially if you’re stuck in a database-heavy gig.",
        "score": 2,
        "created_utc": 1740169918.0,
        "author": "Yung_FLex666",
        "is_submitter": false,
        "parent_id": "t3_1iv0qo4",
        "depth": 0
      },
      {
        "id": "me4cut8",
        "body": "SQL works with a variety of infrastructure over multiple sizes and typically is great for aggregation.  Some databases have inbuilt ML stuff.  There are also extensions for search and geospatial for some databases.  It’s also great if you want to build an app etc.  Alternatively, schedule jobs or have a central spot for data.\n\nPandas has some nice integrations with other packages: charting, sklearn, statsmodels etc.  You can convert easily to other computational frameworks such as numpy, PyTorch tensors etc.  Skills apply well to GeoPandas.  I like to use these types of frameworks for EDA or when building ML models etc.  They often have lots of great model diagnostic and training tools.",
        "score": 1,
        "created_utc": 1740203938.0,
        "author": "410onVacation",
        "is_submitter": false,
        "parent_id": "t3_1iv0qo4",
        "depth": 0
      },
      {
        "id": "me2olg2",
        "body": "SAS, but it’s too expensive now",
        "score": -1,
        "created_utc": 1740178485.0,
        "author": "LaughingZ",
        "is_submitter": false,
        "parent_id": "t3_1iv0qo4",
        "depth": 0
      },
      {
        "id": "me2rpcb",
        "body": "who hurt you? lol",
        "score": 3,
        "created_utc": 1740179468.0,
        "author": "Dependent_Ad_9109",
        "is_submitter": false,
        "parent_id": "t1_me2olg2",
        "depth": 1
      },
      {
        "id": "me39lzu",
        "body": "I’m confused by your comment. Is it about SAS or the expensive part?",
        "score": 1,
        "created_utc": 1740188251.0,
        "author": "LaughingZ",
        "is_submitter": false,
        "parent_id": "t1_me2rpcb",
        "depth": 2
      },
      {
        "id": "me3aahb",
        "body": "A bit of both 😁 Learning SAS for school and plan to never touch it again, coming from a programming background i found it annoying as hell",
        "score": 3,
        "created_utc": 1740188506.0,
        "author": "Dependent_Ad_9109",
        "is_submitter": false,
        "parent_id": "t1_me39lzu",
        "depth": 3
      },
      {
        "id": "me9wko0",
        "body": "LOL. I think we are talking about the same SAS but just wanna clarify this is the language from the company in NC (pronounced ‘Sass’, I think there is another software out there pronounced S-A-S). I went to NC State where the founder has his roots so I learned it there. I also did matlab and R in college and found both of those easier at the time, but then I was in a job that primarily used SAS for 6 years. Now that I’m learning pandas and R again I’m like, SAS has its usefulness for sure. Maybe once I get more skill in pandas I’ll think differently.",
        "score": 1,
        "created_utc": 1740277325.0,
        "author": "LaughingZ",
        "is_submitter": false,
        "parent_id": "t1_me3aahb",
        "depth": 4
      }
    ],
    "comments_extracted": 9
  },
  {
    "id": "1iugpb5",
    "title": "Analysis of subreddit reading/writing comprehension levels",
    "selftext": "Would someone be able to analyze data between right and left leaning subreddits, and see what reading/writing comprehension level they’re at? I’m curious to see what school grade on average each one would be at\n\nI asked AI to do it but apparently chatGPT doesn’t have access to Reddit API :( ",
    "url": "https://www.reddit.com/r/data/comments/1iugpb5/analysis_of_subreddit_readingwriting/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1740106513.0,
    "author": "earthnarb",
    "subreddit": "data",
    "permalink": "/r/data/comments/1iugpb5/analysis_of_subreddit_readingwriting/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1itmyn8",
    "title": "New Data PM Looking to Upskill in AI, Cloud Computing & Beyond",
    "selftext": "I’m a Data Project Manager at a small startup, managing a team of 5 data quality analysts who primarily work in Excel. With 6 months of experience in my first job, I’m eager to upskill as the company explores AI to automate quality tasks and cloud computing for scalable data storage as our data grows over the next 1-2 years.\n\nI have basic programming knowledge in R and Python from college courses, and my company has allocated 150 hours for training. I’d love advice on which skills to focus on to align with these developments and advance my career. Any suggestions from professionals in the field would be greatly appreciated!",
    "url": "https://www.reddit.com/r/data/comments/1itmyn8/new_data_pm_looking_to_upskill_in_ai_cloud/",
    "score": 3,
    "upvote_ratio": 0.8,
    "num_comments": 0,
    "created_utc": 1740015995.0,
    "author": "rehanali_007",
    "subreddit": "data",
    "permalink": "/r/data/comments/1itmyn8/new_data_pm_looking_to_upskill_in_ai_cloud/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1itcrl5",
    "title": "Take a look at my project and let me know if its good please.",
    "selftext": "This is my second project ever and I don’t know if I’m on the right track. Does it look good? Is this what a project should look like? What can I improve on?",
    "url": "https://www.kaggle.com/code/saraiya0/u-s-national-parks-project",
    "score": 2,
    "upvote_ratio": 0.67,
    "num_comments": 1,
    "created_utc": 1739989967.0,
    "author": "Annual_Patient6742",
    "subreddit": "data",
    "permalink": "/r/data/comments/1itcrl5/take_a_look_at_my_project_and_let_me_know_if_its/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mdtehmm",
        "body": "Looks like a well planned little analysis. Haven't gone in depth yet, but what I'd look for is what conclusions or further analysis might come out of the objective analyses. Is the outcome what you would expect, if not why?",
        "score": 2,
        "created_utc": 1740065685.0,
        "author": "ItsSignalsJerry_",
        "is_submitter": false,
        "parent_id": "t3_1itcrl5",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1it42le",
    "title": "Data Integrity: How to start (non-profit edition)",
    "selftext": "Hi all- I work at a non-profit that collects a variety of data points from donor demographics to contributions into our organization to grants made out of our organization.\n\nWe currently report on this data out into the community, to our board and to our funders however, we have found it difficult to “trust” the data we pull. \n\nWe have two main systems for data input: Salesforce and Foundation Power. Foundation Power is considered our “source of truth” for financial data that comes over through an API into Salesforce, but we constantly find that the data between these two systems are not showing the same data (e.g total contributions into the organization are hundreds of dollars off).\n\nIn regard to ensuring data integrity, how do you suggest our organization starts with ensuring our data is correct? What’s our step one get consistent data reporting across the organization? ",
    "url": "https://www.reddit.com/r/data/comments/1it42le/data_integrity_how_to_start_nonprofit_edition/",
    "score": 5,
    "upvote_ratio": 0.86,
    "num_comments": 1,
    "created_utc": 1739967236.0,
    "author": "Prestigious-Stand481",
    "subreddit": "data",
    "permalink": "/r/data/comments/1it42le/data_integrity_how_to_start_nonprofit_edition/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1itahu8",
    "title": "Need help *Research Question**",
    "selftext": "\\*\\*Someone suggested I find 5 or so data files and post them so I could get help developing a question... This is what I've found so far. Not sure if there is a question within this data but I'd love to see what everyone thinks. I am reaching for any angle at this point.\n\n1. [https://www.icpsr.umich.edu/web/NACJD/studies/4699](https://www.icpsr.umich.edu/web/NACJD/studies/4699)\n2. [https://www.icpsr.umich.edu/web/NACJD/studies/36456](https://www.icpsr.umich.edu/web/NACJD/studies/36456)\n3. [https://catalog.data.gov/dataset/death-rates-for-suicide-by-sex-race-hispanic-origin-and-age-united-states-020c1](https://catalog.data.gov/dataset/death-rates-for-suicide-by-sex-race-hispanic-origin-and-age-united-states-020c1)\n\nThese last two sets I was thinking of possibly examining the mental health related emergency room visits in Maryland to its suicide rate but I'm not sure.\n\n4. [https://catalog.data.gov/dataset/ship-emergency-department-visits-related-to-mental-health-conditions-2008-2017](https://catalog.data.gov/dataset/ship-emergency-department-visits-related-to-mental-health-conditions-2008-2017)\n\n5. [https://catalog.data.gov/dataset/ship-suicide-rate-2009-2017](https://catalog.data.gov/dataset/ship-suicide-rate-2009-2017)\n\nI am in dire need of help finding a viable dataset for my research project. I am in my final semester of undergrad and have been tasked with a major research project which will soon need to be transferred into STATA but for now, I need to run basic descriptive statisitcs and come up with my hypothesis, research question, and equation. No matter what topic I bounce around I can't seem to find data to back it up. For example, the effect of Conceal carry laws on crime rates. My professor wants the data to be on the county level with thousands of observations over years and years but that is just adding an extra layer of difficulty. Any ideas? I could use any direction for an interesting research question or useable/understandable data. I feel like this project could be easy if I have the right data and question (my prof also suggested starting with data as it could help make things easier)",
    "url": "https://www.reddit.com/r/data/comments/1itahu8/need_help_research_question/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1739984650.0,
    "author": "Pleasant_Weakness_72",
    "subreddit": "data",
    "permalink": "/r/data/comments/1itahu8/need_help_research_question/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1isns8n",
    "title": "I need an open-sourced multimodal dataset, any suggestion?",
    "selftext": "I'm on the hunt for a multimodal dataset because I'm working on a project where I want my model to understand and interpret data from multiple sources simultaneously. For instance, I'm developing an app that needs to analyze both user reviews (text) and product images (visual) to predict customer satisfaction more accurately. Using a multimodal dataset would allow my model to pick up on nuances that are lost when data is considered in isolation - like the sentiment in the text coupled with visual cues in images. This could lead to a more robust, insightful, and ultimately, more effective application. So, if you know where I can find good resources for multimodal datasets, I'd really appreciate your help!",
    "url": "https://www.reddit.com/r/data/comments/1isns8n/i_need_an_opensourced_multimodal_dataset_any/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1739912993.0,
    "author": "derkinator78",
    "subreddit": "data",
    "permalink": "/r/data/comments/1isns8n/i_need_an_opensourced_multimodal_dataset_any/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1isogut",
    "title": "Research Project **In search of DATA",
    "selftext": "\\*\\*Someone suggested I find 5 or so data files and post them so I could get help developing a question... This is what I've found so far. Not sure if there is a question within this data but I'd love to see what everyone thinks. I am reaching for any angle at this point. \n\n1. [https://www.icpsr.umich.edu/web/NACJD/studies/4699](https://www.icpsr.umich.edu/web/NACJD/studies/4699)\n\n2. [https://www.icpsr.umich.edu/web/NACJD/studies/36456](https://www.icpsr.umich.edu/web/NACJD/studies/36456)\n\n3. [https://catalog.data.gov/dataset/death-rates-for-suicide-by-sex-race-hispanic-origin-and-age-united-states-020c1](https://catalog.data.gov/dataset/death-rates-for-suicide-by-sex-race-hispanic-origin-and-age-united-states-020c1)\n\nThese last two sets I was thinking of possibly examining the mental health related emergency room visits in Maryland to its suicide rate but I'm not sure. \n\n4. [https://catalog.data.gov/dataset/ship-emergency-department-visits-related-to-mental-health-conditions-2008-2017](https://catalog.data.gov/dataset/ship-emergency-department-visits-related-to-mental-health-conditions-2008-2017)\n\n5. [https://catalog.data.gov/dataset/ship-suicide-rate-2009-2017](https://catalog.data.gov/dataset/ship-suicide-rate-2009-2017)\n\nI am in dire need of help finding a viable dataset for my research project. I am in my final semester of undergrad and have been tasked with a major research project which will soon need to be transferred into STATA but for now, I need to run basic descriptive statisitcs and come up with my hypothesis, research question, and equation. No matter what topic I bounce around I can't seem to find data to back it up. For example, the effect of Conceal carry laws on crime rates. My professor wants the data to be on the county level with thousands of observations over years and years but that is just adding an extra layer of difficulty. Any ideas? I could use any direction for an interesting research question or useable/understandable data. I feel like this project could be easy if I have the right data and question (my prof also suggested starting with data as it could help make things easier)",
    "url": "https://www.reddit.com/r/data/comments/1isogut/research_project_in_search_of_data/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 6,
    "created_utc": 1739914602.0,
    "author": "Pleasant_Weakness_72",
    "subreddit": "data",
    "permalink": "/r/data/comments/1isogut/research_project_in_search_of_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mdi76es",
        "body": "To disabuse some idiot of the lies about knife crime in the UK vs the US a few years ago, I dug around and pulled some stats from data services in the US. It wasn’t that hard to get some useable info IIRC. \n\nHave you considered going about this the other way around? Have a look at govt data services and see what is available, then come up with some interesting questions based on that? Might be easier than coming up with questions and finding the data is not there. \n\nThere is a lot of available data kicking around.",
        "score": 1,
        "created_utc": 1739915229.0,
        "author": "Tomatoflee",
        "is_submitter": false,
        "parent_id": "t3_1isogut",
        "depth": 0
      },
      {
        "id": "mdmhtwh",
        "body": "Would news dat be viable?",
        "score": 1,
        "created_utc": 1739977833.0,
        "author": "Mountain-Hedgehog128",
        "is_submitter": false,
        "parent_id": "t3_1isogut",
        "depth": 0
      },
      {
        "id": "mdigty5",
        "body": "That is what my professor is suggesting so I have been bouncing around a few govt sites, but honestly, I have no idea what I am looking at when I do look at the datasets. I have had to download so many codebooks trying to find an interesting question there and still no luck.",
        "score": 1,
        "created_utc": 1739918171.0,
        "author": "Pleasant_Weakness_72",
        "is_submitter": true,
        "parent_id": "t1_mdi76es",
        "depth": 1
      },
      {
        "id": "mdn0rh4",
        "body": "?",
        "score": 1,
        "created_utc": 1739983158.0,
        "author": "Pleasant_Weakness_72",
        "is_submitter": true,
        "parent_id": "t1_mdmhtwh",
        "depth": 1
      },
      {
        "id": "mdk4hx5",
        "body": "Maybe find 5 suitable datasets and post them and people could help you develop a question.",
        "score": 2,
        "created_utc": 1739937993.0,
        "author": "Tomatoflee",
        "is_submitter": false,
        "parent_id": "t1_mdigty5",
        "depth": 2
      },
      {
        "id": "mdn6csz",
        "body": "Seemed like you had a pretty opened ended request. Was just thinking news data (and what you can extract from it) might be interesting in terms of things to correlate against.",
        "score": 1,
        "created_utc": 1739984663.0,
        "author": "Mountain-Hedgehog128",
        "is_submitter": false,
        "parent_id": "t1_mdn0rh4",
        "depth": 2
      }
    ],
    "comments_extracted": 6
  },
  {
    "id": "1irq17e",
    "title": "Opinion of Quinnipiac Online MSBA program",
    "selftext": "I've been accepted to the Quinnipiac online MS in Business Analytics program and wanted to get others' opinions/reviews of the program. My goal for a masters in data analytics program is to do a mid-career pivot (from marketing) into business analytics, so I'm looking for coursework that will give me the skills employers are looking for, solid training in data analytics, and a business school with a solid career pipeline.\n\nKnow Georgia Tech is affordable and very reputable, but I worry I don't have the statistics foundations to be able to pass it. What I like about the Quinnipiac program is that it offers more runway to getting up to speed with analytics foundations while also teaching hard skills like SQL, Python, Tableau, etc, and their accellerated course model... but I'm not seeing strong career pathing yet... hoping people can chime in!",
    "url": "https://www.reddit.com/r/data/comments/1irq17e/opinion_of_quinnipiac_online_msba_program/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 0,
    "created_utc": 1739814990.0,
    "author": "boudica_whodica",
    "subreddit": "data",
    "permalink": "/r/data/comments/1irq17e/opinion_of_quinnipiac_online_msba_program/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ir2e8k",
    "title": "PSID dataset enquiries..",
    "selftext": "Hi! I would like to carry out a research that studies the effect of average total family income during early childhood on children's long-run outcome. I will run 3 different regressions. My independent variables are the average total family income of the child when he/she is 0-5, 6-10, and 11-15 years old. My dependent variable is the child's outcome (education attainment and mental health level) when he/she reaches 20 years old. \n\nI would like to use the PSID dataset for my analysis but I have encountered difficulties extracting the data I want (choosing the right variables and from which year) due to the very huge dataset. \n\nMy thinking is that: I will fix a year (say 1970) and consider all families with children born into them since 1970. I will extract the total family income (and relevant family control variables) for these families from the PSID family-level file for the years 1970-1985. Then, I will extract their children variables (education attainment and mental health level) from the individual-level files for the year 1990, i.e. when the children already reached 20 years old. \n\nI was wondering if there's anyone here who is experienced with the PSID dataset? Is this thinking of data extraction 'feasible'? If not, what is your recommendation? If yes, how do I interpret each row of data downloaded? How can I ensure that each child is matched to his/her family? Should the children data even be extracted from the individual-level files? (I have a problem with this because the individual-level files do not seem to have the relevant outcome variables I want. I have also thought of using the CDS data which is more extensive but it is only completed for children under 18 years old)...\n\nI am in the early stage of my research now and feel very stuck.. so any guidance or comments to point me to a 'better' direction would be very much appreciated!! \n\nThank you..",
    "url": "https://www.reddit.com/r/data/comments/1ir2e8k/psid_dataset_enquiries/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1739739468.0,
    "author": "Character-Tangelo-69",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ir2e8k/psid_dataset_enquiries/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1iqm25q",
    "title": "Could someone help me find open-access databases for caffeine consumption by age in the US/UK or hours of sleep per night by age in the US/UK?",
    "selftext": "A lot of the data bases that I have come across have restricted access, like the UK data service requiring a researcher account. Any help would be much appreciated.",
    "url": "https://www.reddit.com/r/data/comments/1iqm25q/could_someone_help_me_find_openaccess_databases/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1739687010.0,
    "author": "Crab_Comfortable",
    "subreddit": "data",
    "permalink": "/r/data/comments/1iqm25q/could_someone_help_me_find_openaccess_databases/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1iq3ddv",
    "title": "Data on keyword searches per day by U.S. County",
    "selftext": "Hello everyone,\n\nI was wondering if someone knows where I could access data about keyword searches per day by U.S. County. I know **Google Trends** used to provide data with that resolution, but they don't do it anymore. I looked at the following sources without success:\n\n**Dewey** doesn't seem to have data at the County level (1st image)  \n**Treendly** is super slow and crashes continuously (I am not sure if this is because I was using a free version). I was unable to access the preview data.  \n**SEMrush** have data at the municipality level, but average scores for a keyword over the last 12 months.  \n**Keysearch** do not have information at the county level (only for the entire country).  \n**Mangools** have data on keyword searches at the county level but averaged by month.\n\nI do not mind if the access to the data is blocked behind a paywall. \n\nThank you!",
    "url": "https://www.reddit.com/r/data/comments/1iq3ddv/data_on_keyword_searches_per_day_by_us_county/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1739632279.0,
    "author": "CarlitosTheCat",
    "subreddit": "data",
    "permalink": "/r/data/comments/1iq3ddv/data_on_keyword_searches_per_day_by_us_county/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ipxr2o",
    "title": "Finlex data bank",
    "selftext": " I am currently working on an academic project that involves analyzing Finnish legal datasets. While I can access the PDFs through Finlex data bank, I have not found a way to download the translated versions in bulk instead of retrieving them manually. Also the original data (in Finnish and in jsonld format ) looked really nested that it was completely difficult for me to extract the content I needed without finding missing content or values which made me think I’m doing something wrong.\nIf any of you has an idea of how I can access Finnish legal data from Finlex that is actually useful and concrete, your help would be greatly appreciated🙏",
    "url": "https://www.reddit.com/r/data/comments/1ipxr2o/finlex_data_bank/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1739611256.0,
    "author": "Pale_Produce1712",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ipxr2o/finlex_data_bank/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mcyxbfa",
        "body": "Have you tried asking an AI Chatbot to write a Python script for you that will walk through the website and download everything that you need? I know is sounds silly, but these are the exact sorts of use cases where ChatGPT/Claude/etc. could save you a ton of time.",
        "score": 1,
        "created_utc": 1739655894.0,
        "author": "vrdatageek",
        "is_submitter": false,
        "parent_id": "t3_1ipxr2o",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1ipe51m",
    "title": "Learn how to scrape data from Apple App Store and filter results based on categories",
    "selftext": "",
    "url": "https://serpapi.com/blog/easily-scrape-apple-app-store-and-filter-results-by-categories-for-better-insights/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1739549665.0,
    "author": "BandicootOwn4343",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ipe51m/learn_how_to_scrape_data_from_apple_app_store_and/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ip7p7l",
    "title": "S&P 1500 historical constituents",
    "selftext": "Hi all,\n\nI am currently writing my Master's thesis and to that end I need the historical constituents of the S&P 1500 stock index. However, S&P has recently pulled this data from many data providing services and I therefore do not have access to it. I have tried requesting access to the data for academic purposes, but it seems like they can only provide historical data on a 10 year horizon. \n\nDoes anyone know of a way to get the historical constituents of the S&P 1500 index in the years 1994-2024?\n\nThanks in advance!",
    "url": "https://www.reddit.com/r/data/comments/1ip7p7l/sp_1500_historical_constituents/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1739528966.0,
    "author": "Dankarang420",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ip7p7l/sp_1500_historical_constituents/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mcplbhj",
        "body": "Do you mean the S&P 500?",
        "score": 1,
        "created_utc": 1739529336.0,
        "author": "jeffcgroves",
        "is_submitter": false,
        "parent_id": "t3_1ip7p7l",
        "depth": 0
      },
      {
        "id": "mdsz9fg",
        "body": "If I recall correctly, the historical constituent name data got licensed in 2020 and was removed then.\n\nThe easiest way to retrieve historical constituents now is via the CRSP database (check if your university has a subscription via WRDS). I also think Eikon could provide you with ticker lists.\n\nAlternatively, the best “free” workaround I know is scraping the ticker lists for the S&P500, S&P400 and S&P600 of Wikipedia (Python script for S&P500: https://medium.com/@rodrigo.maciel.rubio/web-scraping-historical-s-p-500-constituents-for-quantitative-trading-da29596d10cb). You may also find some datasets in Github (e.g., S&P500: https://github.com/fja05680/sp500).",
        "score": 1,
        "created_utc": 1740061041.0,
        "author": "ruboin",
        "is_submitter": false,
        "parent_id": "t3_1ip7p7l",
        "depth": 0
      },
      {
        "id": "mcpmv5w",
        "body": "No, I mean the S&P 1500. It's an index consisting of S&P 400, 500 and 600 (small-, mid- and large cap indices)",
        "score": 1,
        "created_utc": 1739530255.0,
        "author": "Dankarang420",
        "is_submitter": true,
        "parent_id": "t1_mcplbhj",
        "depth": 1
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1ip0g6p",
    "title": "Which is better option to transition to a data job?",
    "selftext": "I want to work in something related to data (data analyst, data science, etc)\nI applied to Niagara falls university (they have a master in data) and I also applied to Brown college to a programmer diploma. I've got accepted to both. \nI'm an engineer with previous but not extensive experience programming. \nNiagara is relatively new and almost double the cost but is a master. \nAny helpful comments would be great 👍 \nThanks ",
    "url": "https://www.reddit.com/r/data/comments/1ip0g6p/which_is_better_option_to_transition_to_a_data_job/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1739500514.0,
    "author": "Glum-Option3094",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ip0g6p/which_is_better_option_to_transition_to_a_data_job/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mdcudoq",
        "body": "I think both are interesting opportunities, especially in the tech industry additionally, a master's degree at this level is a great deal so I recommend you choose the data program.",
        "score": 1,
        "created_utc": 1739842839.0,
        "author": "Otherwise_Aside8763",
        "is_submitter": false,
        "parent_id": "t3_1ip0g6p",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1iola8e",
    "title": "Does anyone have a Gallup Analytics Subscription that could help get me some data my institution doesn’t have access to?",
    "selftext": "I’m looking for individual level data for the GPSS Governance, Confidence in Institutions, and Consumption Habit data. I know it is a huge ask but would be ever so grateful! ",
    "url": "https://www.reddit.com/r/data/comments/1iola8e/does_anyone_have_a_gallup_analytics_subscription/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1739459751.0,
    "author": "NavisWorld",
    "subreddit": "data",
    "permalink": "/r/data/comments/1iola8e/does_anyone_have_a_gallup_analytics_subscription/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1iomo83",
    "title": "Remote Data Engineering Job Search Experience",
    "selftext": "Since 2023, I've been actively pursuing remote job opportunities, particularly in data engineering. I've had some success, securing two interviews—one through a referral and another via direct application to a company.\n\nRecently, I applied to Proxify and Andela. Unfortunately, I couldn't attend the final round interview for Proxify as I was traveling, and they informed me that I could reapply after six months. For Andela, I am still waiting to schedule the final interview, but I remain hopeful for that opportunity.\n\nFrom my experience so far, I’ve found that securing a remote job often falls into two main categories:\n\n1. **Referral-based applications**\n2. **Hiring platforms for talent, such as Andela and Proxify**\n\nAdditionally, I’ve noticed that data engineering roles appear to be less prevalent compared to backend or full-stack developer positions, which makes it a bit more challenging to find remote opportunities in data engineering. I’ll be giving my final interview with Andela next week, which I am excited about.\n\nThat said, I'm wondering if there are other platforms or websites that specialize in remote data engineering jobs, as I have not yet explored Turing. I’m open to suggestions!\n\nWith six years of experience in data engineering, I've been reflecting on my career trajectory and the challenges of securing remote roles in this field. It seems that compared to backend and AI positions, remote opportunities for data engineers are somewhat less abundant. As a result, I’m considering the possibility of transitioning to either AI or backend engineering to broaden my chances of landing a remote role.",
    "url": "https://www.reddit.com/r/data/comments/1iomo83/remote_data_engineering_job_search_experience/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1739463350.0,
    "author": "__1l0__",
    "subreddit": "data",
    "permalink": "/r/data/comments/1iomo83/remote_data_engineering_job_search_experience/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1iofqce",
    "title": "Suggestions for real estate listings api (any country is ok)?",
    "selftext": "",
    "url": "https://www.reddit.com/r/data/comments/1iofqce/suggestions_for_real_estate_listings_api_any/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1739440615.0,
    "author": "danita255",
    "subreddit": "data",
    "permalink": "/r/data/comments/1iofqce/suggestions_for_real_estate_listings_api_any/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mclxkb4",
        "body": "We have real estate listings  for the UAE (https://www.propertyfinder.ae),  we also have for Spain (https://www.fotocasa.es/en),  We also have Zillow (USA)  all are for Buy/Sell.  \nOther Datasets we have are;  \n1)Hotels.com all hotesl and rooms  availability and prices for the  entire EU countries, USA,UK, Canada, Australia. Also ahve all the Hotels Reviews  \n2)TripAdvisor UK and USA Restaurants,  with details/properties and all Reviews updated monthly.  \nlet me know how we can be of help  \n[https://marketdatainsightica.com/](https://marketdatainsightica.com/)",
        "score": 1,
        "created_utc": 1739477423.0,
        "author": "Comfortable-Ad-6686",
        "is_submitter": false,
        "parent_id": "t3_1iofqce",
        "depth": 0
      },
      {
        "id": "mcox35q",
        "body": "Hey do you get the data for uae only from propertyfinder?",
        "score": 1,
        "created_utc": 1739514649.0,
        "author": "danita255",
        "is_submitter": true,
        "parent_id": "t1_mclxkb4",
        "depth": 1
      },
      {
        "id": "mcp43i1",
        "body": "Yes, we have data from propertyfinder for UAE at the moment, but we are working on [https://www.bayut.com](https://www.bayut.com) as well. Which other sources did you have in mind?",
        "score": 1,
        "created_utc": 1739518588.0,
        "author": "Comfortable-Ad-6686",
        "is_submitter": false,
        "parent_id": "t1_mcox35q",
        "depth": 2
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1inwdr6",
    "title": "I built an open-source library for machine learning model and synthetic data generation via natural language + minimal code",
    "selftext": "I built a library combining graph search and LLM code generation to build task-specific ML models from natural language descriptions. The library also generates synthetic data if you don't have enough.\n\nHere's an example:\n\nimport smolmodels as sm\n\n# Define model via natural language\nmodel = sm.Model(\n    intent=\"Predict sentiment on a news article such that positive indicates optimistic outlook, negative indicates pessimistic outlook, and neutral indicates factual reporting only\",\n    input_schema={\"headline\": str, \"content\": str},\n    output_schema={\"sentiment\": str}\n)\n\n# Generate synthetic training data and build\nmodel.build(\n    generate_samples=1000,\n    provider=\"openai/gpt-4o\"\n)\n\n# Use the model\nsentiment = model.predict({\n    \"headline\": \"600B wiped off NVIDIA market cap\",\n    \"content\": \"NVIDIA shares fell 38% after...\"\n})\n\n\nCore functionality:\n\n- LLM-driven synthetic data generation to bootstrap training\n- Graph search over model architectures\n- Code generation for training and inference\n\nLink: https://github.com/plexe-ai/smolmodels\n\nThe library is fully open-source (Apache-2.0), so feel free to use it however you like. Or just tear us apart in the comments if you think this is dumb. We’d love some feedback, and we’re very open to code contributions!",
    "url": "https://www.reddit.com/r/data/comments/1inwdr6/i_built_an_opensource_library_for_machine/",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1739381283.0,
    "author": "Imaginary-Spaces",
    "subreddit": "data",
    "permalink": "/r/data/comments/1inwdr6/i_built_an_opensource_library_for_machine/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1inlunj",
    "title": "NFL data",
    "selftext": "Hello all!\n\nI am very interested in data, but sometimes I do not know where to begin. I would like to analyze NFL football data, but often do not know how to get the data. Others have probably already done this, so even finding somewhere I can access datasets that people have already compiled would be fine. I have looked at places like ESPN and other sites, but I am uncertain how I can get their data.\n\nAny information would be greatly appreciated.\n\nThanks.",
    "url": "https://www.reddit.com/r/data/comments/1inlunj/nfl_data/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1739346401.0,
    "author": "dfolk13",
    "subreddit": "data",
    "permalink": "/r/data/comments/1inlunj/nfl_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mccz69b",
        "body": "https://pypi.org/project/nfl-data-py/ Is a useful tool. I believe that there is also an R tool for NFL data as well.",
        "score": 1,
        "created_utc": 1739366151.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_1inlunj",
        "depth": 0
      },
      {
        "id": "mcd7033",
        "body": "You want to forecast the plays haha",
        "score": 1,
        "created_utc": 1739368983.0,
        "author": "ProjectManuel",
        "is_submitter": false,
        "parent_id": "t3_1inlunj",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1inf6l1",
    "title": "Linkedin/Email and Data Scraping",
    "selftext": "1. is it somehow possible to map linkeidn emails to get linkeidn accounts. if no? would having someones  linkeidn pfp img aswell, help? if so how...\n\n2. is searching {random name}  site:linkedin.com,  and from there using any indexing results, considered breaking linkedins TOS, if i automate it?",
    "url": "https://www.reddit.com/r/data/comments/1inf6l1/linkedinemail_and_data_scraping/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 2,
    "created_utc": 1739323250.0,
    "author": "Open_Ad5090",
    "subreddit": "data",
    "permalink": "/r/data/comments/1inf6l1/linkedinemail_and_data_scraping/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mcajc1o",
        "body": "Oh I’m so not helping you on this. Don’t need more spam!",
        "score": 3,
        "created_utc": 1739324114.0,
        "author": "fodargh",
        "is_submitter": false,
        "parent_id": "t3_1inf6l1",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1in4t5x",
    "title": "is data going to be still new oil?",
    "selftext": "do you think a startup, who does collection and annotation of data for all different verticals such as medical, manufacturing etc so that this can be used to train models to have better accuracy in real world, can be a good idea?, given rise of robotics in future?",
    "url": "https://www.reddit.com/r/data/comments/1in4t5x/is_data_going_to_be_still_new_oil/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1739296778.0,
    "author": "Character-Welcome535",
    "subreddit": "data",
    "permalink": "/r/data/comments/1in4t5x/is_data_going_to_be_still_new_oil/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mc8ba5p",
        "body": "I've worked at two \"data\" companies that have made Multiple Millions is just collecting and putting a UI and API in front of it.  For some data, I'm sure there is a licensing model for LLM training as another area to monitize.\n\nDM and let's chat about it",
        "score": 3,
        "created_utc": 1739300405.0,
        "author": "EdTwoONine",
        "is_submitter": false,
        "parent_id": "t3_1in4t5x",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1in56zk",
    "title": "Looking to interview data analysts for upcoming project",
    "selftext": "I’m conducting a short survey to better understand the writing styles and expectations in your field. This is part of an assignment where I analyze how writing is used in your field, and your insights will help me gain a clearer perspective on the types of writing required in professional settings.\n\nYour responses will be incredibly valuable in helping me connect real-world writing practices with academic learning. The survey is brief, and I’d truly appreciate your time and expertise!\n\nThank you in advance for your help!\n\nBest,  \nAlex P.\n\nUndergraduate at UNC - Chapel Hill",
    "url": "https://www.reddit.com/r/data/comments/1in56zk/looking_to_interview_data_analysts_for_upcoming/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1739297678.0,
    "author": "Good_Bet1701",
    "subreddit": "data",
    "permalink": "/r/data/comments/1in56zk/looking_to_interview_data_analysts_for_upcoming/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1im8url",
    "title": "Is there any public dataset for USPS EDDM Mailing Routes for the Entire US?",
    "selftext": "I need a full dataset of most, if not all mailing routes set up by USPS. They have a web app to calculate by zipcode, and there are also third party sites that you can look up the data by zipcode. But I need the massive dataset of every mailing route in the country, or at least in my state. Theoretically, I could go and get the data for each zipcode in the US one by one but that's not feasible. Even if the data is outdated somewhat, any sort of full dataset like this would be appreciated.",
    "url": "https://www.reddit.com/r/data/comments/1im8url/is_there_any_public_dataset_for_usps_eddm_mailing/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1739201716.0,
    "author": "EightBallJuice",
    "subreddit": "data",
    "permalink": "/r/data/comments/1im8url/is_there_any_public_dataset_for_usps_eddm_mailing/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1im3gpn",
    "title": "Does anyone know how to export the Audience dimensions using the Google API with Python? I cannot find anything on the internet so far.",
    "selftext": "Hi all! I am writing to you out of desperation because you are my last hope. Basically I need to export GA4 data using the Google API(BigQuery is not an option) and in particular, I need to export the dimension userID(Which is traced by our team). [Here I can see](https://developers.google.com/analytics/devguides/reporting/data/v1/quickstart-client-libraries#python_1) I can see how to export most of the dimensions, but the code provided in this documentation provides [these dimensions and metrics](https://developers.google.com/analytics/devguides/reporting/data/v1/api-schema) , while I need to export [the ones here](https://developers.google.com/analytics/devguides/reporting/data/v1/audience-export-api-schema) , because they have the userID . I went to [Google Analytics Python API GitHub](https://github.com/googleanalytics/python-docs-samples/tree/main/google-analytics-data) and there were no code samples with the audience whatsoever. I asked 6 LLMs for code samples and I got 6 different answers that all failed to do the API call. By the way, the API call with the sample code of the first documentation is executed perfectly. It's the Audience Export that I cannot do. The only thing that I found on Audience Export [was this one](https://github.com/googleapis/google-cloud-python/issues/12802) , which did not work. In particular, in the comments it explains how to create audience_export, which works until the operation part, but it still does not work. In particular, if I try the code that he provides initially(after correcting the AudienceDimension field from name= to dimension_name=), I take \nTypeError: Parameter to MergeFrom() must be instance of same class: expected <class 'Dimension'> got <class 'google.analytics.data_v1beta.types.analytics_data_api.AudienceDimension'>.\n\nSo, here is one of the 6 code samples(the credentials are inserted already in the environment with the os library):\n\nproperty_id = 123\n\naudience_id = 456\n\nfrom google.analytics.data_v1beta.types import (\n\nDateRange,\n\nDimension,\n\nMetric,\n\nRunReportRequest,AudienceDimension,\n\nAudienceDimensionValue,\n\nAudienceExport,\n\nAudienceExportMetadata,\n\nAudienceRow,\n\n)\n\nfrom google.analytics.data_v1beta.types import GetMetadataRequest\n\nclient = BetaAnalyticsDataClient()\n\n# Create the request for Audience Export\n\nrequest = AudienceExport(\n\nname=f\"properties/{property_id}/audienceExports/{audience_id}\",\n\ndimensions=[{\"dimension_name\": \"userId\"}]  # Correct format for requesting userId dimension\n\n)\n\n# Call the API\n\nresponse = client.get_audience_export(request)\n\nThe sample code might have some syntax mistakes because I couldn't copy the whole original one from the work computer, but again, with the Core Reporting code, it worked perfectly. Would anyone here have an idea how I should write the Audience Export code in Python? Thank you!",
    "url": "https://www.reddit.com/r/data/comments/1im3gpn/does_anyone_know_how_to_export_the_audience/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1739184859.0,
    "author": "Tsipouromelo",
    "subreddit": "data",
    "permalink": "/r/data/comments/1im3gpn/does_anyone_know_how_to_export_the_audience/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1im0bcd",
    "title": "Need advice about customer database",
    "selftext": "I want to create a customer database :  \n1. easy to use  \n2. sometimes, competitors can be customers also, that's why I need like relations to understand which customers are customers of our competitors also  \n3. map view\n\nwhich tools can i use?",
    "url": "https://www.reddit.com/r/data/comments/1im0bcd/need_advice_about_customer_database/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1739170890.0,
    "author": "zoohand",
    "subreddit": "data",
    "permalink": "/r/data/comments/1im0bcd/need_advice_about_customer_database/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mbzwgon",
        "body": "As in, CRM or are you actually planning an SQL database?",
        "score": 1,
        "created_utc": 1739184739.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_1im0bcd",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1ilbmnb",
    "title": "Collaborate for a data analysis project",
    "selftext": "I’m looking to form a team of 4 people to work on a data analysis project. I would consider myself as a beginner and I’m trying to find a job. My interests are travel & business strategy. So if anyone can resonate with this and wants to sincerely work on something then dm me.\nI also want one person who is well versed to guide us. If anyone is interested please dm me. ",
    "url": "https://www.reddit.com/r/data/comments/1ilbmnb/collaborate_for_a_data_analysis_project/",
    "score": 6,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1739095656.0,
    "author": "Designer_Actuator974",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ilbmnb/collaborate_for_a_data_analysis_project/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mbwuflz",
        "body": "Hey what kind of project do you have in mind? I am game",
        "score": 1,
        "created_utc": 1739138273.0,
        "author": "Proper6ioq",
        "is_submitter": false,
        "parent_id": "t3_1ilbmnb",
        "depth": 0
      },
      {
        "id": "mbzh90y",
        "body": "Im in",
        "score": 1,
        "created_utc": 1739175050.0,
        "author": "ClubOk8688",
        "is_submitter": false,
        "parent_id": "t3_1ilbmnb",
        "depth": 0
      },
      {
        "id": "mc7gx2y",
        "body": "I’m in too. Have my engineering degree, interested in aviation and travel.",
        "score": 1,
        "created_utc": 1739291913.0,
        "author": "HoppersDad",
        "is_submitter": false,
        "parent_id": "t3_1ilbmnb",
        "depth": 0
      },
      {
        "id": "mmawg7i",
        "body": "I'm learning and would like to collaborate",
        "score": 1,
        "created_utc": 1744242333.0,
        "author": "Aggravating_Ship_682",
        "is_submitter": false,
        "parent_id": "t3_1ilbmnb",
        "depth": 0
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "1ikwf69",
    "title": "Experience with health data from MIMIC?",
    "selftext": "Does anyone have experience using health data from mimic? Id love to know if you used any resources when getting started. ",
    "url": "https://www.reddit.com/r/data/comments/1ikwf69/experience_with_health_data_from_mimic/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1739045514.0,
    "author": "aevum24",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ikwf69/experience_with_health_data_from_mimic/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ik8kp2",
    "title": "Government data potentially taken down tonight",
    "selftext": "Forwarding from a group chat of environmental professionals:\n\n\"Hey guys, just a PSA. I've heard indirectly from employees of NREL, the US Fish and Wildlife Services, and National Resource Conservation Service that their databases will be taken offline tonight. I'm not sure what the extent of this will be, but it may be good to download/back up any critical data/material you use from those agencies just in case if you're able, and probably other related gov agencies as well.\n\nCan confirm. Also a message from a friend: A note for people who use GitHub, if you fork a repository that is public, if the initial repository gets deleted the fork will remain. If you fork a repository that was originally public and it goes private and then it is deleted that fork will still exist. If you use GitHub, I strongly recommend forking your government repositories.\n\nHeads up, we heard the database situation from: NREL, EIA, NRCS, and USFWS.\"",
    "url": "https://www.reddit.com/r/data/comments/1ik8kp2/government_data_potentially_taken_down_tonight/",
    "score": 13,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1738969760.0,
    "author": "MagePages",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ik8kp2/government_data_potentially_taken_down_tonight/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ik4alu",
    "title": "How can I build it?",
    "selftext": "I would like to build a GPT for environmental issues. I however, need some guidance on how to colect the data and the most credible souces to consider. I'd appreciate any pointers for real!",
    "url": "https://www.reddit.com/r/data/comments/1ik4alu/how_can_i_build_it/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 3,
    "created_utc": 1738958762.0,
    "author": "ProjectManuel",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ik4alu/how_can_i_build_it/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mblvfox",
        "body": "If you mean environmental science, go for 1st or 2nd quartile journals. If you mean environmental activism, i donno.",
        "score": 2,
        "created_utc": 1738988728.0,
        "author": "New_Alarm3749",
        "is_submitter": false,
        "parent_id": "t3_1ik4alu",
        "depth": 0
      },
      {
        "id": "mbmfi7u",
        "body": "Environmental science is exactly what I mean",
        "score": 1,
        "created_utc": 1738998346.0,
        "author": "ProjectManuel",
        "is_submitter": true,
        "parent_id": "t1_mblvfox",
        "depth": 1
      },
      {
        "id": "mbmfkix",
        "body": "Any relevant journals you would recommend please",
        "score": 1,
        "created_utc": 1738998382.0,
        "author": "ProjectManuel",
        "is_submitter": true,
        "parent_id": "t1_mblvfox",
        "depth": 1
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1ijx22t",
    "title": "Help Figuring out Data Collection Method",
    "selftext": "I work at a Museum and it's important for us to track zip code data with each transaction so we can know where people are coming from and make marketing decisions. Unfortunately our point of sale system won't allow us to add an additional field for this.\n\nThere are just two things we need from each visitor. The date and the zipcode. Even if we just had a spreadsheet with thousands of rows, we can use a pivot table to analyze what we need.\n\nWhat we can't figure out is the best way to track this. All the transactions are done on tablets and it's fussy/slow for our staff to switch screens to another app in the middle of doing a transaction.\n\nI keep picturing some kind of little data input pad they can punch it into that logs the data. Is that a thing? Am I crazy? Any genius ideas?\n\nRight now they are WRITING THEM DOWN ON PAPER and then recording them on a spreadsheet at the end of the day. It feels so dumb. There has to be a better way...",
    "url": "https://www.reddit.com/r/data/comments/1ijx22t/help_figuring_out_data_collection_method/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1738940778.0,
    "author": "dosmalacaras",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ijx22t/help_figuring_out_data_collection_method/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mbkxw5h",
        "body": "maybe just make a google form and have the URL converted to a QR code people can scan when they come in and they just put in there zip code, i think time/date will automatically be collected and you can view the form data as a spreadsheet on the back end, and its free to do",
        "score": 1,
        "created_utc": 1738976703.0,
        "author": "RHiNDR",
        "is_submitter": false,
        "parent_id": "t3_1ijx22t",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1ijs23k",
    "title": "Business Intelligence Analyst ou Data Analyst",
    "selftext": "Hello everyone,\nI would like to follow a diploma course on Openclassroom, I am hesitating between Business Intelligence Analyst or Data Analyst. Advice on which one to choose and which one offers more professional opportunities please. THANKS",
    "url": "https://www.reddit.com/r/data/comments/1ijs23k/business_intelligence_analyst_ou_data_analyst/",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 0,
    "created_utc": 1738924265.0,
    "author": "ze_mediateur",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ijs23k/business_intelligence_analyst_ou_data_analyst/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ijexnp",
    "title": "Help with Twitter API for Research Thesis on Twitter data analysis",
    "selftext": "Hi everyone,\n\nI’m working on a research thesis about analyzing Twitter data, comparing the pre and post-Elon Musk eras. I need to download a corpus of tweets for analysis, but I’m having trouble accessing historical data.\n\nHere’s what I’ve tried so far:\n\n1. I used elizaOS, but it only allows me to download recent tweets, not historical data.\n2. I considered using the free version of the Twitter API, but I’m not sure how to proceed after downloading it. I’ve heard that tweepy may be useful but I also struggle in the step to connect tweepy to the API. \n\nMy questions are:\n1. Is there a way to access historical tweets (pre-Elon Musk era) using the free version of the Twitter API or any other tool?\n2. If not, what’s the best way to use the free API to analyze recent tweets?\n3. Are there any updated tools or libraries (other than Tweepy) that work well with the current Twitter API?\n\nAny advice or guidance would be greatly appreciated! Thank you in advance.",
    "url": "https://www.reddit.com/r/data/comments/1ijexnp/help_with_twitter_api_for_research_thesis_on/",
    "score": 4,
    "upvote_ratio": 0.84,
    "num_comments": 1,
    "created_utc": 1738879624.0,
    "author": "maarramiiro",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ijexnp/help_with_twitter_api_for_research_thesis_on/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mbgxdxw",
        "body": "I’ve been trying to access the Twitter dataset from Archive.org that you recommended, but it looks like the actual tweet data is restricted. I was only able to download metadata files, but not the WARC files that seem to contain the tweets.\n\nDo you know if there’s a way to access them, or if any additional permissions are needed?",
        "score": 1,
        "created_utc": 1738932643.0,
        "author": "maarramiiro",
        "is_submitter": true,
        "parent_id": "t1_mben647",
        "depth": 1
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1ijdltf",
    "title": "Going from Rstudio to VScode Sucks",
    "selftext": "Any tips to help make the transition easier? ",
    "url": "https://www.reddit.com/r/data/comments/1ijdltf/going_from_rstudio_to_vscode_sucks/",
    "score": 2,
    "upvote_ratio": 0.67,
    "num_comments": 1,
    "created_utc": 1738876374.0,
    "author": null,
    "subreddit": "data",
    "permalink": "/r/data/comments/1ijdltf/going_from_rstudio_to_vscode_sucks/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mbo9ybn",
        "body": "I’ve never used R Studio so I have no context for what is bothering you specifically but I have had to learn VS Code over the past year and have found that intelligent use of plugins can really help. There’s an icon on the left for all their pre-built plugins to provide better type hints, color coding, navigation features , and more. I’m not an R person so I can’t say exactly which ones but I assume that an R or VS Code subreddit will have lots of suggestions",
        "score": 1,
        "created_utc": 1739029699.0,
        "author": "amosmj",
        "is_submitter": false,
        "parent_id": "t3_1ijdltf",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1ijenlf",
    "title": "Movie Data Set",
    "selftext": "I’m looking for an Data set related to Movies . The data should contain how many movies released every year their collections, verdict, genre, Duration.   I want to use this data for my Power BI project building a dashboard related to this .  ",
    "url": "https://www.reddit.com/r/data/comments/1ijenlf/movie_data_set/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1738878946.0,
    "author": "Beautiful-Cost3160",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ijenlf/movie_data_set/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mbjanas",
        "body": "I can build you this dataset for 20USD.Credible and raw",
        "score": 1,
        "created_utc": 1738958498.0,
        "author": "ProjectManuel",
        "is_submitter": false,
        "parent_id": "t3_1ijenlf",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1ijj1vm",
    "title": "Is a certification in data management enough to land me an entry-level job in the field?",
    "selftext": "I'm interested in data management and want to enter the industry. I'm currently seeking a certification in the program. But I'm not sure a certification would be enough. Is a degree in CS a must, or a certificate in the subject be enough to get me an entry-level job?",
    "url": "https://www.reddit.com/r/data/comments/1ijj1vm/is_a_certification_in_data_management_enough_to/",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 5,
    "created_utc": 1738890915.0,
    "author": "Fabulous_Jury_9063",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ijj1vm/is_a_certification_in_data_management_enough_to/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mbg4bj3",
        "body": "Depends on the country you're in and the company.\nI'm in the UK, I do have a foundation degree but I never completed the last year of university, I also have 3 certifications in data. My degree never got me a job. My certifications however did put me at the running list when getting my first entry level job.\n\nAnd same again for my third job 6 years into my career the company never once asked about my university. They did however specifically ask me a fair bit about the certifications I'd done.\n\nIt might be different in the US or other countries though.\n\nWhat certification are you planning on doing if you don't mind me asking?",
        "score": 2,
        "created_utc": 1738916620.0,
        "author": "smilelilpenguin",
        "is_submitter": false,
        "parent_id": "t3_1ijj1vm",
        "depth": 0
      },
      {
        "id": "mbfccqx",
        "body": "Yes a degree is needed. Certificates are crap",
        "score": 1,
        "created_utc": 1738902647.0,
        "author": "hroaks",
        "is_submitter": false,
        "parent_id": "t3_1ijj1vm",
        "depth": 0
      },
      {
        "id": "mcbocd1",
        "body": "I don’t believe a CS degree is required or very applicable for data as a cs degree. I also don’t believe certifications alone are a great thing ether. \n\nMy experience is that industry experience is more important than certifications and then adding a degree that shows that you can apply mathematics is a strong background. \n\nI think a degree in math, stats, economics, finance, biochemistry, etc based on your field is more valuable than a CS degree. \n\nA CS degree is great for building the program, those degrees are for taking the program and making use out of things. \n\nIf you know your industry and how to build some basic code, you are much more useful to me than someone who is a god at coding and knows nothing about the field. I can’t train 3 years of industry experience in 4 months.",
        "score": 1,
        "created_utc": 1739340312.0,
        "author": "BigSwingingMick",
        "is_submitter": false,
        "parent_id": "t3_1ijj1vm",
        "depth": 0
      },
      {
        "id": "mbjyow1",
        "body": "I asked chatgpt what's the best cert for data management. It replied that the Certified Data Management Professional (CDMP) is the best.",
        "score": 1,
        "created_utc": 1738965418.0,
        "author": "Fabulous_Jury_9063",
        "is_submitter": true,
        "parent_id": "t1_mbg4bj3",
        "depth": 1
      },
      {
        "id": "mwk69dg",
        "body": "Which 3 certifications do you hold if you don’t mind me asking?",
        "score": 1,
        "created_utc": 1749333907.0,
        "author": "Longjumping-Fly-4207",
        "is_submitter": false,
        "parent_id": "t1_mbg4bj3",
        "depth": 1
      }
    ],
    "comments_extracted": 5
  },
  {
    "id": "1ij75j6",
    "title": "How time and money change international relationships [JP EXPORTS 2022]",
    "selftext": "",
    "url": "https://i.redd.it/1dos8iectjhe1.png",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 2,
    "created_utc": 1738860703.0,
    "author": "Rodant-",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ij75j6/how_time_and_money_change_international/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mbc1qrz",
        "body": "What is it measuring?",
        "score": 3,
        "created_utc": 1738866091.0,
        "author": "Mr_DarkCircles",
        "is_submitter": false,
        "parent_id": "t3_1ij75j6",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1ij2op5",
    "title": "National Data: Traffic Count / Traffic Volume / Average Daily Traffic (AADT) or Vehicles Per Day (VPD)",
    "selftext": "I have coordinates within the USA. Ideally trying to recreate this at scale: [https://screencapturePL.tinytake.com/msc/MTA1NjIxMjlfMjQyNjM2MTU](https://screencapturePL.tinytake.com/msc/MTA1NjIxMjlfMjQyNjM2MTU)\n\n  \nBut a poor man on a budget. This data is commonly freely available at the state DOT level for small roads. For highways and national routes you can get it from [USDOT sources](https://geodata.bts.gov/datasets/5a9462b519854ec6a2334b3c0bdfc3c1/about).\n\n\n\nAny and all advice?\n\n",
    "url": "https://www.reddit.com/r/data/comments/1ij2op5/national_data_traffic_count_traffic_volume/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1738848978.0,
    "author": "LaughLately100",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ij2op5/national_data_traffic_count_traffic_volume/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1iiwf6a",
    "title": "Does anyone have the results the first-past-the-post seats in the 2022 Italian Parliamentary election by region?",
    "selftext": "Everything I find only has what both major coalitions won as a whole, not what each party won. I can find how many first-past-the-post seats each party won in total, but that is not by region. The results aren't even listed on the Italian government's website. They have the proportional seats by party, but the first-past-the-post seats are by coalition. I would like to do a project that analyzes what would happen if Italy used a different electoral system, but this data is integral to that project. Any help would be appreciated!",
    "url": "https://www.reddit.com/r/data/comments/1iiwf6a/does_anyone_have_the_results_the_firstpastthepost/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1738824017.0,
    "author": "jonassthebest",
    "subreddit": "data",
    "permalink": "/r/data/comments/1iiwf6a/does_anyone_have_the_results_the_firstpastthepost/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1iibnsc",
    "title": "Data concern with OpenAI",
    "selftext": "I deleted my ChatGPT account months ago, and just did a data request. The data request still had my email, name and even my location saved on your servers under both a \"support file\" and authentication metadata. Is this normal for them to keep?\n\nHow long this information is retained once an account is deleted?",
    "url": "https://www.reddit.com/r/data/comments/1iibnsc/data_concern_with_openai/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1738767007.0,
    "author": null,
    "subreddit": "data",
    "permalink": "/r/data/comments/1iibnsc/data_concern_with_openai/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ii49nk",
    "title": "Data engineer R1 Interviews questions with JP Morgan chase",
    "selftext": "I have my Round 1 interviews for a Data Engineer role with JPMC. Can anyone suggest the best way to prepare for it and key aspects I should focus on to perform well?",
    "url": "https://www.reddit.com/r/data/comments/1ii49nk/data_engineer_r1_interviews_questions_with_jp/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1738738438.0,
    "author": "amarpal123",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ii49nk/data_engineer_r1_interviews_questions_with_jp/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ihmvos",
    "title": "What’s the difference between data management and business intelligence?",
    "selftext": " I (32F) am trying to switch careers and would like a career that has a good work life balance, opportunity to grow, financially be a better. \n\nI have the option of finding a mentor at work and one of the VPs is a director of Data Governance Management and the other is a VP in Business Intelligence. I currently have a data analytics cert but nothing else. (I will look into going back for my masters as I have a BA in psych)\n\nI do understand BI would be more on how the data affects the business and data management would be more focused on data. \nI was wondering which would be a better field to focus on? What is a day like? Mostly meetings? Presentations?",
    "url": "https://www.reddit.com/r/data/comments/1ihmvos/whats_the_difference_between_data_management_and/",
    "score": 2,
    "upvote_ratio": 0.75,
    "num_comments": 3,
    "created_utc": 1738689491.0,
    "author": "Few-Mycologist4238",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ihmvos/whats_the_difference_between_data_management_and/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mb290y9",
        "body": "Data Management focuses on organizing, governing, and ensuring data quality, while Business Intelligence uses that data to generate insights, create reports, and support business decisions.",
        "score": 2,
        "created_utc": 1738734553.0,
        "author": "sabrinagao",
        "is_submitter": false,
        "parent_id": "t3_1ihmvos",
        "depth": 0
      },
      {
        "id": "mbbj3ld",
        "body": "DM cleans raw data to make it useful.\nBI uses cleaned data to easily understand what's going on.\n\nWorkflow usually is OR > DM > BI > FI > OP",
        "score": 1,
        "created_utc": 1738860947.0,
        "author": "Rodant-",
        "is_submitter": false,
        "parent_id": "t3_1ihmvos",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1ihc8md",
    "title": "ISTATAPI - Does anyone know how to get Volume chained GDP Data ?",
    "selftext": "I ve been trying to get volume chained gdp data, seasonally adjusted from istatiapi but I can't find it. I have tried under National account quarterly databases and GDP Databases but I can only see GDp at market prices. The api is not well documented and messy.  ",
    "url": "https://www.reddit.com/r/data/comments/1ihc8md/istatapi_does_anyone_know_how_to_get_volume/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1738653548.0,
    "author": "Vegetable-Message-75",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ihc8md/istatapi_does_anyone_know_how_to_get_volume/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mawfat9",
        "body": "If you haven’t already, try checking the National Accounts - Volume Measures section. Sometimes the data is buried under weird labels. \n\nAlso, you can try to adjust parameters in the API call",
        "score": 1,
        "created_utc": 1738664598.0,
        "author": "Interesting_Pie_2232",
        "is_submitter": false,
        "parent_id": "t3_1ihc8md",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1iggyhx",
    "title": "Is this site full of it or is there a real concern here?",
    "selftext": "The article seems to suggest a spike in early voters going exactly 60-40 where we would expect a smooth curve of percentages. What are the possible explanations for this?",
    "url": "https://electiontruthalliance.org/clark-county%2C-nv",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1738557119.0,
    "author": "JustConsoleLogIt",
    "subreddit": "data",
    "permalink": "/r/data/comments/1iggyhx/is_this_site_full_of_it_or_is_there_a_real/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ig7wi4",
    "title": "Hacked Data",
    "selftext": "Hi all \nMy league of legends account, LinkedIn and X were all hacked after downloading a file that contained a malicious malware.\nLinkedIn and X are both blocked as I contacted support to explain things, however my lol' account can't be recovered due to lack of registration email that I couldn't provide (got it from a friend in 2012 when I started playing the game ) \nSo as I suppose that some here are experts and might have a clue ! What are the motivations of the hacker and where my data can be sold knowing that no valuable banking details are gathered as we don't use any international payment tools here.\nThank you ",
    "url": "https://www.reddit.com/r/data/comments/1ig7wi4/hacked_data/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 4,
    "created_utc": 1738530867.0,
    "author": "Alternative_Earth882",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ig7wi4/hacked_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mamhspx",
        "body": "Maybe they just wanted to ruin your day?",
        "score": 2,
        "created_utc": 1738531625.0,
        "author": "k00_x",
        "is_submitter": false,
        "parent_id": "t3_1ig7wi4",
        "depth": 0
      },
      {
        "id": "mamv3l9",
        "body": "Maybe they were hoping that you disclosed some valuable info in your private conversations on X and LinkedIn. You could have sent a photo of your passport to someone privately or for whatever reason banking information.",
        "score": 1,
        "created_utc": 1738535589.0,
        "author": "SaltyVanilla6223",
        "is_submitter": false,
        "parent_id": "t3_1ig7wi4",
        "depth": 0
      },
      {
        "id": "mamiehn",
        "body": "It's been almost a week and he/she is still attempting to hack my steam and Ubisoft ..",
        "score": 1,
        "created_utc": 1738531801.0,
        "author": "Alternative_Earth882",
        "is_submitter": true,
        "parent_id": "t1_mamhspx",
        "depth": 1
      },
      {
        "id": "mamw3lj",
        "body": "Yeah my passport info was scanned on my pc .. so dunno if he'v taken it or not ..",
        "score": 1,
        "created_utc": 1738535890.0,
        "author": "Alternative_Earth882",
        "is_submitter": true,
        "parent_id": "t1_mamv3l9",
        "depth": 1
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "1ieer34",
    "title": "MACbook how to read, move and write from/to ExternalHardDrive or SDcard",
    "selftext": "MACbook how to read, move and write from/to ExternalHardDrive or SDcard\n\nI have MACbook and whern I connect external hard drive, or sdcard, I can not move anything to these meda, from Mac.\n\nI tried EasyUS and it worked, but 80dollars a month is very expensive.",
    "url": "https://www.reddit.com/r/data/comments/1ieer34/macbook_how_to_read_move_and_write_fromto/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1738330069.0,
    "author": "InterestingAccess784",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ieer34/macbook_how_to_read_move_and_write_fromto/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "maf89zw",
        "body": "Should be drag and drop without external software. Maybe you need to format the drives. SD cards have a little physical slide on them read/write. Also check the permissions of the drive. It might be protected",
        "score": 1,
        "created_utc": 1738437266.0,
        "author": "Shillyshee",
        "is_submitter": false,
        "parent_id": "t3_1ieer34",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1ie6skm",
    "title": "FB Marketplace Autos",
    "selftext": "I’m shopping for a car and thought if I could extract all the data from a Facebook marketplace page and dump it in a spreadsheet it would be easier to look at the offerings. I tried using a Chrome extension (Data Scraper) but it’s a little hinky sometimes.\n\nDoes anybody know of any tools that they have used that work particularly well with Facebook?\nTIA. ",
    "url": "https://www.reddit.com/r/data/comments/1ie6skm/fb_marketplace_autos/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1738297784.0,
    "author": "codenerd80",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ie6skm/fb_marketplace_autos/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1idx0w4",
    "title": "My TV Show Master List (a snippet , suggestions welcome)",
    "selftext": "",
    "url": "https://i.redd.it/alvbfjmr07ge1.png",
    "score": 3,
    "upvote_ratio": 0.8,
    "num_comments": 3,
    "created_utc": 1738269905.0,
    "author": "MysteriousPickles",
    "subreddit": "data",
    "permalink": "/r/data/comments/1idx0w4/my_tv_show_master_list_a_snippet_suggestions/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "ma2qmjh",
        "body": "Hi there! I have been working on a master list of all the shows I have watched and want to watch! Its the work of an insanely bored person, who enjoys TV maybe a little too much. This is only a snippet of a \"finished\" section, there are about 150+ more shows after Cabinet of Curiosities. I plan to post my final version soon once I've managed to fill everything out, its just very time consuming and I seem to constantly be getting reminded of shows I have seen to heard of and need to add so I don't forget them!\n\nLet me know if theres somewhere else I should post this, and let me know if theres any improvements to be made. This is a work in progress. The ratings and suggestions section is fully based on vibes. I tend to like a lot of things so lots of my shows are rated 8/10.",
        "score": 1,
        "created_utc": 1738269917.0,
        "author": "MysteriousPickles",
        "is_submitter": true,
        "parent_id": "t3_1idx0w4",
        "depth": 0
      },
      {
        "id": "ma2zoi7",
        "body": "man I WISH I WISH I could organize my watch list like this, I guess it is time to start",
        "score": 1,
        "created_utc": 1738272412.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_1idx0w4",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1id29ea",
    "title": "CS / DS NewsLetters",
    "selftext": "Do you guys know about any CS or DS NewsLetters to keep updated with the trends?",
    "url": "https://www.reddit.com/r/data/comments/1id29ea/cs_ds_newsletters/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1738178902.0,
    "author": "SecretIll1644",
    "subreddit": "data",
    "permalink": "/r/data/comments/1id29ea/cs_ds_newsletters/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m9zsn4d",
        "body": "i think the best place to get news about is the medium, but sorry bro i dont know any newsletters about.",
        "score": 1,
        "created_utc": 1738237022.0,
        "author": "IhateOnions0427",
        "is_submitter": false,
        "parent_id": "t3_1id29ea",
        "depth": 0
      },
      {
        "id": "m9zvbe2",
        "body": "Try Feedly app",
        "score": 1,
        "created_utc": 1738238355.0,
        "author": "Better_Practice5365",
        "is_submitter": false,
        "parent_id": "t3_1id29ea",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1icog63",
    "title": "Activities or demonstrations to promote data literacy to your average worker?",
    "selftext": "Hi all, \n\nI'm delivering a 30 minute online presentation / workshop in my organisation on the value of developing one's data literacy in the workplace.\n\nI'm collecting ideas for simple activities or demonstrations to help promote this idea to lay people. Does anyone know of or has anyone seen anything that fits the bill?\n\nThanks in advance!",
    "url": "https://www.reddit.com/r/data/comments/1icog63/activities_or_demonstrations_to_promote_data/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1738135470.0,
    "author": "SpittingLava",
    "subreddit": "data",
    "permalink": "/r/data/comments/1icog63/activities_or_demonstrations_to_promote_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m9xlv56",
        "body": "Maybe take some of the slides from r/data is beautiful and talk through them \nPeople like visual charts \nIf you were able to go through different forms of statistical charts and explain how they work that would also be cool",
        "score": 1,
        "created_utc": 1738200520.0,
        "author": "ZealousidealHippo528",
        "is_submitter": false,
        "parent_id": "t3_1icog63",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1iccleb",
    "title": "Circana, Neilson, IRI alternative for foodservice",
    "selftext": "Has anyone ever had any luck with finding a similar insights data database like Neilson and Circana IRI but for food service? We use Circana for our retail division but are looking to gain better insights into the food service sector and build a demand landscape. I know that Circana has its own version called SupplyTrack, but it only gathers broad-liner data. We use broad-liners, but they are only about 50% of our business. We rely heavily on cash-and-carry retailers like Restaurant Depot, but I have zero insight into the product category as a whole. Has anyone had a similar issue and found a tool to help?",
    "url": "https://www.reddit.com/r/data/comments/1iccleb/circana_neilson_iri_alternative_for_foodservice/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1738099042.0,
    "author": "ExistingWrongdoer264",
    "subreddit": "data",
    "permalink": "/r/data/comments/1iccleb/circana_neilson_iri_alternative_for_foodservice/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ib1vly",
    "title": "How can I migrate apache airflow metadata?",
    "selftext": "I am trying to migrate apache airflow  metadata from mySQL to postgresql and every tutorial i watch is for linux, does anyone know how can I do same steps bit with Windows operating system? ",
    "url": "https://www.reddit.com/r/data/comments/1ib1vly/how_can_i_migrate_apache_airflow_metadata/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 5,
    "created_utc": 1737959915.0,
    "author": "qristinius",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ib1vly/how_can_i_migrate_apache_airflow_metadata/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "ma025co",
        "body": "Hi! You can try this \n\n\t1.\tInstall PostgreSQL and run it\n\t2.\tInstall necessary libraries: pip install psycopg2 mysql-connector-python.\n\t3.\tBackup MySQL metadata with mysqldump and adjust SQL syntax if needed.\n\t4.\tCreate a PostgreSQL database and adjust the sql_alchemy_conn in airflow.cfg.\n\t5.\tImport the backup into PostgreSQL with psql.\n\t6.\tRun airflow db upgrade to finalize.\n\nHope it helps!",
        "score": 2,
        "created_utc": 1738241406.0,
        "author": "Interesting_Pie_2232",
        "is_submitter": false,
        "parent_id": "t3_1ib1vly",
        "depth": 0
      },
      {
        "id": "ma02zt0",
        "body": " I already have Postgresql database with user and password, tricky thing for me is to where is that Metadata placed and how do i back it up like is it in apache-airflow directory?",
        "score": 1,
        "created_utc": 1738241748.0,
        "author": "qristinius",
        "is_submitter": true,
        "parent_id": "t1_ma025co",
        "depth": 1
      },
      {
        "id": "ma04cnt",
        "body": "I think the metadata is stored in the database specified in airflow.cfg, not in the Apache Airflow. You can use use mysqldump to back up the MySQL database that Airflow is using:\n\nmysqldump -u username -p airflow > airflow_backup.sql\n\nAfter that you can import it into the PostgreSQL database.",
        "score": 2,
        "created_utc": 1738242291.0,
        "author": "Interesting_Pie_2232",
        "is_submitter": false,
        "parent_id": "t1_ma02zt0",
        "depth": 2
      },
      {
        "id": "ma05s6a",
        "body": "I will try it this evening! And i will tell how it goes 😇",
        "score": 1,
        "created_utc": 1738242846.0,
        "author": "qristinius",
        "is_submitter": true,
        "parent_id": "t1_ma04cnt",
        "depth": 3
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "1i9zwm1",
    "title": "Learning Data Science",
    "selftext": "",
    "url": "https://i.redd.it/zpntghx838fe1.png",
    "score": 14,
    "upvote_ratio": 0.79,
    "num_comments": 8,
    "created_utc": 1737846963.0,
    "author": "StephenMcGannon",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i9zwm1/learning_data_science/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m96rt65",
        "body": "Where I can get a 4k quality version?",
        "score": 2,
        "created_utc": 1737852717.0,
        "author": "Dra_caos",
        "is_submitter": false,
        "parent_id": "t3_1i9zwm1",
        "depth": 0
      },
      {
        "id": "m97jzmj",
        "body": "No",
        "score": 2,
        "created_utc": 1737862627.0,
        "author": "Snarky_Quip",
        "is_submitter": false,
        "parent_id": "t3_1i9zwm1",
        "depth": 0
      },
      {
        "id": "m9853zx",
        "body": "Down voted cause i couldn't read the branches clearly.. \n\nNeed a higher resolution image",
        "score": 1,
        "created_utc": 1737871796.0,
        "author": "bobbyrreddy",
        "is_submitter": false,
        "parent_id": "t3_1i9zwm1",
        "depth": 0
      },
      {
        "id": "m9d8kff",
        "body": "Looks good, but awful to read...",
        "score": 1,
        "created_utc": 1737938236.0,
        "author": "LeKarget",
        "is_submitter": false,
        "parent_id": "t3_1i9zwm1",
        "depth": 0
      },
      {
        "id": "m9ienil",
        "body": "Math",
        "score": 1,
        "created_utc": 1738008436.0,
        "author": "DSPGerm",
        "is_submitter": false,
        "parent_id": "t3_1i9zwm1",
        "depth": 0
      },
      {
        "id": "m98jtwy",
        "body": "[https://imgur.com/a/RC9RRgD](https://imgur.com/a/RC9RRgD)",
        "score": 2,
        "created_utc": 1737880180.0,
        "author": "StephenMcGannon",
        "is_submitter": true,
        "parent_id": "t1_m96rt65",
        "depth": 1
      }
    ],
    "comments_extracted": 6
  },
  {
    "id": "1i9sdlk",
    "title": "How does youtube store our data?",
    "selftext": "Every couple weeks I delete all of my browser data (history, cookies,cache,...). This also logs me out of every website. After doing this, i went to YouTube and I was indeed logged out like usual and my recommendation page didn’t look the same as it usually does when i’m logged in. However, all of the content on there was still very obviously tailored to me specifically: videos in my mother tongue, youtubers that make videos close to the ones i watch, and some very niche subjects that interest me. I am 100% sure this wasn’t just a coincidence, but i decided to check anyway by opening youtube in a private window. In the private window, the recommendation page was just typical, generic, page you get when you’ve never been on youtube. So, how is it possible that YouTube still had access to my data?\n\nTLDR: my youtube recommendations weren’t fully reset after deleting all my data. How? ",
    "url": "https://www.reddit.com/r/data/comments/1i9sdlk/how_does_youtube_store_our_data/",
    "score": 5,
    "upvote_ratio": 0.86,
    "num_comments": 4,
    "created_utc": 1737826933.0,
    "author": "Middle-Employer-638",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i9sdlk/how_does_youtube_store_our_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m94khu3",
        "body": "YouTube uses multiple tracking mechanisms beyond just browser cookies and cache. Even after you delete your browsing data, several factors could contribute to YouTube still recommending content that aligns with your interests:\n\n1. IP Address Tracking – YouTube may associate your viewing patterns with your IP address. If you’re using the same network, some recommendations may persist.\n\n\n2. Fingerprinting Techniques – YouTube (and many other websites) use browser fingerprinting, which collects details like your screen resolution, installed fonts, browser version, and system settings to create a unique identifier.\n\n\n3. Google Account and Cross-Site Tracking – If you use other Google services while logged in (e.g., Gmail, Google Search), YouTube may still infer preferences even when you're logged out.\n\n\n4. Cached Data on YouTube's Servers – YouTube might store session-based recommendations based on past behavior, even if you clear local data.\n\n\n5. AI-Based Predictions – Even with a fresh session, YouTube’s algorithm can make educated guesses based on minimal data, like trending topics from users in your region.\n\n\n\nYour test with a private browsing window shows that a genuinely \"new\" session gives generic results, meaning YouTube likely ties recommendations to your device, IP, or previous interactions before you cleared your data.\n\nIf you want a true \"fresh start,\" you could try:\n\nUsing a VPN to change your IP.\n\nAccessing YouTube from a completely different device.\n\nClearing all Google activity (https://myactivity.google.com).\n\nUsing a fresh Google account.\n\n\nHope this helps!",
        "score": 2,
        "created_utc": 1737828105.0,
        "author": "BassCandid3457",
        "is_submitter": false,
        "parent_id": "t3_1i9sdlk",
        "depth": 0
      },
      {
        "id": "m94kb8y",
        "body": "There are other identifiers they can track that you cannot easily change like the device id. When that is combined with things like your IP address it becomes obvious it’s still you or someone close to you.",
        "score": 1,
        "created_utc": 1737828053.0,
        "author": "rodeengel",
        "is_submitter": false,
        "parent_id": "t3_1i9sdlk",
        "depth": 0
      },
      {
        "id": "m94kbce",
        "body": "It's probably using geospatial data based on your connection's metadata. Specifically IP address, but maybe also machine OS, and browser model.\n\nOf course if cookie information is available and/or you are logged in, you'll get your personalized experience.",
        "score": 1,
        "created_utc": 1737828054.0,
        "author": "mike-manley",
        "is_submitter": false,
        "parent_id": "t3_1i9sdlk",
        "depth": 0
      },
      {
        "id": "ma7rb5g",
        "body": "YouTube likely uses data tied to your account or device beyond just cookies and cache. Even if you delete your browser data, YouTube can still track you through other things (Google account for example, which collects data across devices)\n\nHave you checked if you’re signed into your Google account on other devices?",
        "score": 1,
        "created_utc": 1738339932.0,
        "author": "Interesting_Pie_2232",
        "is_submitter": false,
        "parent_id": "t3_1i9sdlk",
        "depth": 0
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "1i9ibsc",
    "title": "Raw / CDR data",
    "selftext": "I am looking for a RAW / CDR data for over 65 age US citizens. Where can I get the list of Phone numbers? Please help me out. Thanks",
    "url": "https://www.reddit.com/r/data/comments/1i9ibsc/raw_cdr_data/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1737792444.0,
    "author": "Scared-Bullfrog7150",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i9ibsc/raw_cdr_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1i8zo3u",
    "title": "Help finding NFT Data!",
    "selftext": "I am starting my undergraduate dissertation and I am looking for a dataset of historical NFT price and sales volumes during the period 2017-2024. I only need the data for Art and Collectibles. I thought it would be easy enough to find a cvs file online, but have had no luck.\n\nMost of the academic articles I have read have have stated they found their data from [nonfungible.com](http://nonfungible.com/) . I have emailed them a number of times to request it, but have not received any response.\n\nI am starting to worry as I need it quite soon. Does anyone have some tips as to where I can find it?\n\nThank you!",
    "url": "https://www.reddit.com/r/data/comments/1i8zo3u/help_finding_nft_data/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1737736853.0,
    "author": "Frosty-Marsupial4055",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i8zo3u/help_finding_nft_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m8zi3f8",
        "body": "Just use nftpricefloor api, you have categories and the oldest floor price historical data (from August 21)",
        "score": 1,
        "created_utc": 1737755066.0,
        "author": "AcrobaticLand1876",
        "is_submitter": false,
        "parent_id": "t3_1i8zo3u",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1i8wrnd",
    "title": "Ai prices are crashing",
    "selftext": "DeepSeek’s first reasoning model has arrived - over 25x cheaper than OpenAI’s o1\n\nHighlights from our initial benchmarking of DeepSeek R1:\n➤ Trades blows with OpenAI’s o1 across our eval suite to score the second highest in Artificial Analysis Quality Index ever\n➤ Priced on DeepSeek’s own API at just $0.55/$2.19 input/output - significantly cheaper than not just o1 but o1-mini\n➤ Served by DeepSeek at 71 output tokens/s (comparable to DeepSeek V3)\n➤ Reasoning tokens are wrapped in <thinking> tags, allowing developers to easily decide whether to show them to users\n\nStay tuned for more detail coming next week - big upgrades to the Artificial Analysis eval suite launching soon.",
    "url": "https://www.reddit.com/r/data/comments/1i8wrnd/ai_prices_are_crashing/",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 0,
    "created_utc": 1737729354.0,
    "author": "Annual_Judge_7272",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i8wrnd/ai_prices_are_crashing/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1i8l5mp",
    "title": "Data Management Associate Role in JP Morgan",
    "selftext": "Hello everyone,\n\nI am currently working as a Data Analyst at a startup. Yesterday, I received a call for a Data Management Associate role at J.P. Morgan. I researched the responsibilities of Data Management, but I’m unsure about the types of questions they might ask and their expectations for this role.\n\nIf anyone could guide me or share their insights, it would be greatly appreciated.",
    "url": "https://www.reddit.com/r/data/comments/1i8l5mp/data_management_associate_role_in_jp_morgan/",
    "score": 2,
    "upvote_ratio": 0.75,
    "num_comments": 1,
    "created_utc": 1737686026.0,
    "author": "taricho_xd",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i8l5mp/data_management_associate_role_in_jp_morgan/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m8w6if5",
        "body": "Please someone respond 😭",
        "score": 1,
        "created_utc": 1737718054.0,
        "author": "taricho_xd",
        "is_submitter": true,
        "parent_id": "t3_1i8l5mp",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1i8g40t",
    "title": "Need help finding data of UFC fighters and their follower count.",
    "selftext": "Hello People !\n\nI am an undergrad economics student who's doing a study that requires instagram follower count of all UFC Fighters in a CSV file. from my understanding it is possible to filter for ufc fighters (verified only) and export their respective follower counts in a CSV file on [HypeAuditor.com](http://HypeAuditor.com) business plan account witch costs around $300 USD a month. Does anyone have a business plan on this website or have a similar website with the same feature ? Please help as this is time sensitive and MY ENTIRE CAREER DEPENDS ON IT LIKE NEVER BEFORE.",
    "url": "https://www.reddit.com/r/data/comments/1i8g40t/need_help_finding_data_of_ufc_fighters_and_their/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1737671632.0,
    "author": "Direct_Guess_8780",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i8g40t/need_help_finding_data_of_ufc_fighters_and_their/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m8t5r4g",
        "body": "I will gladly share my super cool study for witch i need this data as well !",
        "score": 1,
        "created_utc": 1737672116.0,
        "author": "Direct_Guess_8780",
        "is_submitter": true,
        "parent_id": "t3_1i8g40t",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1i85s0v",
    "title": "Car database",
    "selftext": "Hello fellow nerds!\n\nI am working on a project that requires a chunky amount of data on car sensors (all type of sensors, not just vision). I have struggled to find it so far, any lead helps.\n\nMany thanks! ",
    "url": "https://www.reddit.com/r/data/comments/1i85s0v/car_database/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1737645809.0,
    "author": "Sharp_Today_7797",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i85s0v/car_database/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1i7owwc",
    "title": "Standard Deviation and Outliers detection",
    "selftext": "Hey! This is my first time working with Standard Deviation, and I would love to hear some feedback from people who already worked on it. \n\n  \nLet's grab one example, a measure called ADR (average daily revenue). The visualization in Looker shows this measure on a daily basis. What I am trying to achieve is to detect deviation. For instance, if an item from my products got an ADR higher than expected, I would like to be able to detect it and categorize it as an expected deviation or an outlier. \n\nMy question is, how do you think is the best way to approach this type of analysis, having in mind that I would like to make it work within Looker, probably some kind of visualization showing the deviation for the metric. ",
    "url": "https://www.reddit.com/r/data/comments/1i7owwc/standard_deviation_and_outliers_detection/",
    "score": 2,
    "upvote_ratio": 0.75,
    "num_comments": 1,
    "created_utc": 1737587426.0,
    "author": "jugogastrico",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i7owwc/standard_deviation_and_outliers_detection/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m8pohf6",
        "body": "For analyzing ADR (Average Daily Rate) deviations in Looker, I recommend a multi-step approach:\n\n1. Statistical Analysis\n\n* Calculate mean and standard deviation of ADR\n* Use 1.5 or 2 standard deviations as threshold for identifying outliers\n* Z-score method: Flag values beyond ±2-3 standard deviations\n\n1. Visualization Techniques\n\n* Box plot to show distribution and outliers\n* Control chart with upper/lower control limits\n* Heatmap showing deviation percentages\n\n1. Implementation in Looker\n\n* Create calculated fields for:\n   * Z-score calculation\n   * Deviation percentage\n   * Outlier flag (boolean)",
        "score": 1,
        "created_utc": 1737635390.0,
        "author": "IhateOnions0427",
        "is_submitter": false,
        "parent_id": "t3_1i7owwc",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1i7dosn",
    "title": "Help: looking for weather data for airline predictions",
    "selftext": "Hi, my task in University requires me to calc predictions on the delays of planes. Weather conditions are an important feature, hence why I want to implement real data. Does anyone know of an open source Weather channel that shares their data? Is there maybe research on it which shares their datasets, especially in the time range 2016-2018? \n\nThank you for reading, in regards\n\nKen",
    "url": "https://www.reddit.com/r/data/comments/1i7dosn/help_looking_for_weather_data_for_airline/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1737559794.0,
    "author": "Plane_Driver4408",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i7dosn/help_looking_for_weather_data_for_airline/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1i6t17b",
    "title": "Alternative for chatrecap ai?",
    "selftext": "Any mod or alternative for chat recap ai?",
    "url": "https://www.reddit.com/r/data/comments/1i6t17b/alternative_for_chatrecap_ai/",
    "score": 7,
    "upvote_ratio": 1.0,
    "num_comments": 11,
    "created_utc": 1737492898.0,
    "author": "Napil_333",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i6t17b/alternative_for_chatrecap_ai/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "mehu5xf",
        "body": "did you find something?",
        "score": 1,
        "created_utc": 1740391631.0,
        "author": "Odd-Reference-9997",
        "is_submitter": false,
        "parent_id": "t3_1i6t17b",
        "depth": 0
      },
      {
        "id": "mjil66b",
        "body": "Did u do now?",
        "score": 1,
        "created_utc": 1742839113.0,
        "author": "moduler_phi",
        "is_submitter": false,
        "parent_id": "t3_1i6t17b",
        "depth": 0
      },
      {
        "id": "mywkvf6",
        "body": "I",
        "score": 1,
        "created_utc": 1750464063.0,
        "author": "ReviewBackground1116",
        "is_submitter": false,
        "parent_id": "t3_1i6t17b",
        "depth": 0
      },
      {
        "id": "n16bbj7",
        "body": "Sls",
        "score": 1,
        "created_utc": 1751568587.0,
        "author": "WeddingAdditional423",
        "is_submitter": false,
        "parent_id": "t3_1i6t17b",
        "depth": 0
      },
      {
        "id": "mek0dcf",
        "body": "I did not",
        "score": 1,
        "created_utc": 1740420060.0,
        "author": "Napil_333",
        "is_submitter": true,
        "parent_id": "t1_mehu5xf",
        "depth": 1
      },
      {
        "id": "mf9wr6t",
        "body": "[https://chatrecapai.org/](https://chatrecapai.org/) It has a free trial.",
        "score": 1,
        "created_utc": 1740760411.0,
        "author": "SupaCtx",
        "is_submitter": false,
        "parent_id": "t1_mek0dcf",
        "depth": 2
      }
    ],
    "comments_extracted": 6
  },
  {
    "id": "1i6rbhu",
    "title": "Where to find drone registration / part 107 data?",
    "selftext": "Anyone know where to get data on drone registrations in the US? I tried the FAA Data portal, google big query and Kaggle with no luck.",
    "url": "https://www.reddit.com/r/data/comments/1i6rbhu/where_to_find_drone_registration_part_107_data/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1737488677.0,
    "author": "Flippigan",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i6rbhu/where_to_find_drone_registration_part_107_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1i5vdyt",
    "title": "Technical Documentation Advice",
    "selftext": "I work as a Data Project Manager at a small startup and have initiated a project to document all our ETL processes. Currently, only one programmer fully understands the code. As our team grows, I want to create clear and accessible documentation for our data analysts so they can better understand these processes.\n\nHere’s my initial plan:\n\n* Create a Google Doc with an overview of each process\n* Include a link to the Azure DevOps repository containing the process code and relevant comments\n* Outline the execution steps for each process\n* Provide example outputs for reference\n\nSince I don’t have prior experience in professional technical documentation, I’d love your feedback on the most effective approach to structuring this documentation efficiently.",
    "url": "https://www.reddit.com/r/data/comments/1i5vdyt/technical_documentation_advice/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1737393532.0,
    "author": "rehanali_007",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i5vdyt/technical_documentation_advice/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m870x1f",
        "body": "I work as a data engineer and our documentation lives where we work, sometimes in dbt, the code, git repos etc.\n\nWe also made an attempt to create an internal portal with documentation, but most employees (including yours truly) don’t seem to be updating it continuously.\n\nBut something that is really appreciated is flow charts describing how data flows through the ETL process. I would say that visualization is almost mandatory for ETL documentation.",
        "score": 1,
        "created_utc": 1737394871.0,
        "author": "mybitsareonfire",
        "is_submitter": false,
        "parent_id": "t3_1i5vdyt",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1i5hoha",
    "title": "Courses on EDX",
    "selftext": "Due to financial issues, paying for Coursers is expensive to me and in my country it's expensive. I was looking that EDX has good data science and other courses related and it's cheaper to me, what's your opinion on EDX.",
    "url": "https://www.reddit.com/r/data/comments/1i5hoha/courses_on_edx/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1737345907.0,
    "author": "kroix666",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i5hoha/courses_on_edx/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m83wk03",
        "body": "[removed]",
        "score": 3,
        "created_utc": 1737346030.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_1i5hoha",
        "depth": 0
      },
      {
        "id": "m867eu5",
        "body": "I don’t know what your goals are or what systems you are using but there are a ton of free courses on YouTube. There are a lot of conference speakers who have been recorded as well as MIT’s courses.\nWhat do you want courses on?",
        "score": 1,
        "created_utc": 1737386604.0,
        "author": "amosmj",
        "is_submitter": false,
        "parent_id": "t3_1i5hoha",
        "depth": 0
      },
      {
        "id": "m83wnp6",
        "body": "Thank you for your reply",
        "score": 2,
        "created_utc": 1737346073.0,
        "author": "kroix666",
        "is_submitter": true,
        "parent_id": "t1_m83wk03",
        "depth": 1
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1i55co4",
    "title": "A New PostgreSQL Block Storage Layout for Full Text Search",
    "selftext": "",
    "url": "https://www.paradedb.com/blog/block_storage_part_one",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1737311636.0,
    "author": "philippemnoel",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i55co4/a_new_postgresql_block_storage_layout_for_full/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1i4ymbl",
    "title": "Ideas for collecting Hungarian business owners data?",
    "selftext": "Hi, I am trying to gather data about Hungarian business owners in the US for a university project. One idea I had was searching for Hungarian last names in business databases and on the web, I still have not found such databases, I appreciate any advice you can give or any new idea to gather such data.\n\nThank you once again.",
    "url": "https://www.reddit.com/r/data/comments/1i4ymbl/ideas_for_collecting_hungarian_business_owners/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1737293911.0,
    "author": "Rayanski1",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i4ymbl/ideas_for_collecting_hungarian_business_owners/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1i4pw4n",
    "title": "Tik Tok ban data ",
    "selftext": "I’m in now way qualified to accomplish this, but I love the thought of seeing what apps see the increases of use, and all the other metrics you beautiful people will think of!",
    "url": "https://www.reddit.com/r/data/comments/1i4pw4n/tik_tok_ban_data/",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 0,
    "created_utc": 1737259729.0,
    "author": "AZHWY88",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i4pw4n/tik_tok_ban_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1i4o5or",
    "title": "How to prepare for Data science interviews, especially the coding ones? And also is it recommended to study first & then apply or do both things simultaneously?",
    "selftext": "",
    "url": "https://www.reddit.com/r/data/comments/1i4o5or/how_to_prepare_for_data_science_interviews/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 3,
    "created_utc": 1737254265.0,
    "author": "Vaidehi16_08",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i4o5or/how_to_prepare_for_data_science_interviews/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m7zpcq3",
        "body": "[removed]",
        "score": 3,
        "created_utc": 1737299862.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_1i4o5or",
        "depth": 0
      },
      {
        "id": "m80cpd9",
        "body": "makes sense! Thank you so much!!",
        "score": 2,
        "created_utc": 1737306591.0,
        "author": "Vaidehi16_08",
        "is_submitter": true,
        "parent_id": "t1_m7zpcq3",
        "depth": 1
      },
      {
        "id": "m8cih6o",
        "body": "What was said? 😅",
        "score": 1,
        "created_utc": 1737467448.0,
        "author": "JerryBond106",
        "is_submitter": false,
        "parent_id": "t1_m80cpd9",
        "depth": 2
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1i3jrso",
    "title": "Book Review: Fundamentals of Data Engineering ",
    "selftext": "Hi guys, I just finished reading Fundamentals of Data Engineering and wrote up a review in case anyone is interested! \n\n**Key takeaways:**\n\n1. This book is great for anyone looking to get into data engineering themselves, or understand the work of data engineers they work with or manage better.\n\n2. The writing style in my opinion is very thorough and high level / theory based. \n\nWhich is a great approach to introduce you to the whole field of DE, or contextualize more specific learning.\n\nBut, if you want a tech-stack specific implementation guide, this is not it (nor does it pretend to be)\n\nhttps://medium.com/@sergioramos3.sr/self-taught-reviews-fundamentals-of-data-engineering-by-joe-reis-and-matt-housley-36b66ec9cb23",
    "url": "https://www.reddit.com/r/data/comments/1i3jrso/book_review_fundamentals_of_data_engineering/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1737130093.0,
    "author": "0sergio-hash",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i3jrso/book_review_fundamentals_of_data_engineering/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1i3h567",
    "title": "Data Request Mental health ",
    "selftext": "I need anual mental health chrisis numbers from 2013-2023 for an important paper can’t find it anywhereeeee. Please help",
    "url": "https://www.reddit.com/r/data/comments/1i3h567/data_request_mental_health/",
    "score": 2,
    "upvote_ratio": 0.75,
    "num_comments": 0,
    "created_utc": 1737122963.0,
    "author": "GoodWaves89",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i3h567/data_request_mental_health/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1i3ejjm",
    "title": "What are the key steps to building a data warehouse from scratch? ",
    "selftext": "# Hey everyone, I'm curious about the process of building a data warehouse from scratch. What are the essential steps, and what should someone prioritize when starting out? Are there specific tools or platforms you’d recommend for beginners or small organizations? I’d love to hear your thoughts or experiences!",
    "url": "https://www.reddit.com/r/data/comments/1i3ejjm/what_are_the_key_steps_to_building_a_data/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1737114257.0,
    "author": "Majestic-Fig3921",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i3ejjm/what_are_the_key_steps_to_building_a_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m7n58ey",
        "body": "Besides the obvious answer of it depends on the purpose, you can easily start building in bigquery from Google for free. Organize your datasets (tables) with similar language I.e. Salesforce might be a data set with lead, contact, opportunity, project tables while your hr has a payroll and employee roster datasets.\n\nUnderstand the relationships so you can join the data easily, make sure you grab any unique keys or identifiers where possible. In fact, if you can push your own unique IDs into systems you can actually save yourself and your company a lot of time and frustration while increasing accuracy in cross system reporting. This is not always possible, but with solid programming skills you can really level up your data warehouse and reporting game.\n\nUnderstand your data pipelines - how is this data going to refresh or update and how often? Document and describe the data sets, create data dictionaries where possible to help other engineers and analysts navigate the warehouse. This can prevent awkward joins and frustration from teams when they try to connect two data sets with different data pipelines that may not be in sync.\n\nThe key to a good data warehouse is to understand how it will be used, while preparing for unintended use cases as best as possible. Now this part is the art, and takes time and experience but as you build you will start to understand common use cases beyond your own. In bigquery, partitioning and clustering help to deal with massive data sets to offset costs and increase query performance. I’m assuming you have a basic understanding of SQL, but this is important as your data scales.\n\nMost importantly if you are just beginning, have fun! Make mistakes, try things out. Just make sure you don’t destroy any production data sources. Export csv files or make sure you are comfortable with ETL solutions (fivetran, workato, mulesoft, python, JavaScript) so you don’t accidentally alter production data sources. My first salesforce api I crossed my fingers that I didn’t delete our entire companies sales data. I did not, but make SURE you know what you are doing because confusing a GET and DELETE can cost more than you your job.\n\nDidn’t intend to write so much and I am on my phone so forgive any spelling and grammar mistakes. Also watch for those in your schemas 🫠",
        "score": 4,
        "created_utc": 1737128331.0,
        "author": "treasurecoastdata",
        "is_submitter": false,
        "parent_id": "t3_1i3ejjm",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1i2wfgh",
    "title": "Explore the latest tool to power up investigations via the Offshore Leaks database",
    "selftext": "",
    "url": "https://www.icij.org/inside-icij/2025/01/explore-the-latest-tool-to-power-up-investigations-via-the-offshore-leaks-database/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1737054066.0,
    "author": "ICIJ",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i2wfgh/explore_the_latest_tool_to_power_up/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1i2cnod",
    "title": "Help with finding raw data sources as opposed to averages ",
    "selftext": "I’m working on a data management project where my teacher wants us to include a box plot and have at least 90 data points. We had the option of collecting our own data or finding it online and I chose to research it online. Problem is, I’m having trouble finding any sources that just provide raw data in the form of tables with each individual response listed. Is this just not something that is made public ever? I’m\nfinding a lot of sources that have the information I want in averages and medians, so it seems weird to me that none of them would include their raw data tables. Can anyone help me out? My project is on resource consumption in Canada. Most of the data I’ve been using is from stats Canada, but now that I need more raw unfiltered data I’m not finding anything. Any help is greatly appreciated.",
    "url": "https://www.reddit.com/r/data/comments/1i2cnod/help_with_finding_raw_data_sources_as_opposed_to/",
    "score": 5,
    "upvote_ratio": 0.86,
    "num_comments": 3,
    "created_utc": 1736988423.0,
    "author": "fesora122",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i2cnod/help_with_finding_raw_data_sources_as_opposed_to/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m7fmjcq",
        "body": "[Car Dataset](https://www.kaggle.com/datasets/qubdidata/auto-market-dataset)\n\n\n[Electric power consumption ](https://www.kaggle.com/datasets/qubdidata/electric-power-consumption)\n\n\n\nmaybe any of those dataset helps u? \nI think if not u can find more this type of datasets on kaggle",
        "score": 1,
        "created_utc": 1737026557.0,
        "author": "qristinius",
        "is_submitter": false,
        "parent_id": "t3_1i2cnod",
        "depth": 0
      },
      {
        "id": "m7dz0ep",
        "body": "Hey thanks so much! Unfortunately my school’s website is very lacklustre and doesn’t have a lot of data available (I’m in high school). Also I kinda already committed to my topic.",
        "score": 1,
        "created_utc": 1736995733.0,
        "author": "fesora122",
        "is_submitter": true,
        "parent_id": "t1_m7duyfn",
        "depth": 1
      },
      {
        "id": "m7e9ign",
        "body": "I’m only seeing more averages and yearly consumption tables here. What I’m looking for is like a data table with every single entry in the census (or just a sample of that) that has individual entries for each household on their energy consumption. I didn’t see that in your link but maybe I’m missing something?",
        "score": 1,
        "created_utc": 1736999601.0,
        "author": "fesora122",
        "is_submitter": true,
        "parent_id": "t1_m7e0dw4",
        "depth": 3
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1i28ve7",
    "title": "How to drive business outcomes with data and AI products (price optimization)",
    "selftext": "We must not forget that our job is to create value with our data initiatives. So, here is an example of how to drive business outcome.\n\nCASE STUDY: Machine learning for price optimization in grocery retail (perishable and non-perishable products).\n\nBUSINESS SCENARIO: A grocery retailer that sells both perishable and non-perishable products experiences inventory waste and loss of revenue. The retailer lacks dynamic pricing model that adjusts to real-time inventory and market conditions.\n\nConsequently, they experience the following.\n\n1. Perishable items often expire unsold leading to waste.\n2. Non-perishable items are often over-discounted. This reduces profit margins unnecessarily.\n\nMETHOD: Historical data was collected for perishable and non-perishable items depicting shelf life, competitor pricing trends, seasonal demand variations, weather, holidays, including customer purchasing behavior (frequency, preferences and price sensitivity etc.).\n\nData was cleaned to remove inconsistencies, and machine learning models were deployed owning to their ability to handle large datasets. Linear regression or gradient boosting algorithm was employed to predict demand elasticity for each item. This is to identify how sensitive demand is to price changes across both categories. The models were trained, evaluated and validated to ensure accuracy.\n\nINFERENCE: For perishable items, the model generated real-time pricing adjustments based on remaining shelf life to increase discounts as expiry dates approach to boost sales and minimize waste.\n\nFor non-perishable items, the model optimized prices based on competitor trends and historical sales data. For instance, prices were adjusted during peak demand periods (e.g. holidays) to maximize profitability.\n\nFor cross-category optimization, Apriori algorithm was able to identify complementary products (e.g. milk and cereal) for discount opportunities and bundles to increase basket size to optimize margins across both categories. These models were continuously fed new data and insights to improve its accuracy.\n\nCONCLUSION: Companies in the grocery retail industry can reduce waste from perishables through dynamic discounts. Also, they can improve profit margins on non-perishables through targeted price adjustments. With this, grocery retailers can remain competitive while maximizing profitability and sustainability.\n\nDM me to join the 1% of club of business savvy data professionals who are becoming leaders in the data space. I will send you to a learning resource that will turn you into a strategic business partner.\n\nWishing you Goodluck in your career.",
    "url": "https://www.reddit.com/r/data/comments/1i28ve7/how_to_drive_business_outcomes_with_data_and_ai/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1736977976.0,
    "author": "Substantial_Rub_3922",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i28ve7/how_to_drive_business_outcomes_with_data_and_ai/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1i22zk9",
    "title": "New platform draws on investigative journalism to identify cross-border patterns of corruption",
    "selftext": "",
    "url": "https://www.icij.org/news/2025/01/new-platform-draws-on-investigative-journalism-to-identify-cross-border-patterns-of-corruption/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1736962877.0,
    "author": "ICIJ",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i22zk9/new_platform_draws_on_investigative_journalism_to/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1i0dyso",
    "title": "Data request",
    "selftext": "Hello, \nI got into a debate with a friend on whether remote workers get paid more, we couldn't settle on an answer so I decided that I would look into it for fun.\n\nTo do this I need data, and I have been trying to get my hands on it for a week or so now but BLS, eurostat, ATUS and ACS are all very difficult to navigate. I have not managed to find a dataset with remote work and wages. (There are plenty of datasets for example education and wages, and other economic characteristics)\n\nCould someone please give me a clue or point me towards the right subreddit to ask?",
    "url": "https://www.reddit.com/r/data/comments/1i0dyso/data_request/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1736775021.0,
    "author": "An0therR3dditor",
    "subreddit": "data",
    "permalink": "/r/data/comments/1i0dyso/data_request/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hzlgxu",
    "title": "Recommend a lightweight data quality evaluation tool - Dingo",
    "selftext": "📢 This project belongs to the production toolchain for **large models**.\n\n\nDingo offers a variety of built-in rules and model evaluation methods, while also supporting custom evaluation methods. It facilitates the automated detection of data quality issues in datasets.\n\n\n\nGitHub repository: https://github.com/DataEval/dingo. Welcome to **star** it!. 🎉 🎉 🎉",
    "url": "https://www.reddit.com/r/data/comments/1hzlgxu/recommend_a_lightweight_data_quality_evaluation/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1736684207.0,
    "author": "chupei0",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hzlgxu/recommend_a_lightweight_data_quality_evaluation/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hz1s1x",
    "title": "Funtime Data Collection ",
    "selftext": "Tracked our funtime over the course of last year.\n\nEmpty heart: Started but neither finished.\n\nHalf heart filled on left: Started, I finished.\n\nHalf heart filled on right: Started, wife finished.\n\nFull heart: Both finished.\n\n\nPopulation Demographics: Husband and Wife.\n\nEnvironmental Factors: Parents to three children.\n\nVariables: Nap/bed time, family watching the kids, door lock. ",
    "url": "https://i.redd.it/b1t3bxt0nece1.png",
    "score": 4,
    "upvote_ratio": 0.61,
    "num_comments": 4,
    "created_utc": 1736618708.0,
    "author": "peggedsquare",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hz1s1x/funtime_data_collection/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": true,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m6m1xvx",
        "body": "Username checks out",
        "score": 3,
        "created_utc": 1736619663.0,
        "author": "WhoaMonchichi",
        "is_submitter": false,
        "parent_id": "t3_1hz1s1x",
        "depth": 0
      },
      {
        "id": "m6m9dyb",
        "body": "Ha! Sometimes.",
        "score": 1,
        "created_utc": 1736621973.0,
        "author": "peggedsquare",
        "is_submitter": true,
        "parent_id": "t1_m6m1xvx",
        "depth": 1
      },
      {
        "id": "m6n3iks",
        "body": "It’s the best!",
        "score": 0,
        "created_utc": 1736631595.0,
        "author": "JababyMan",
        "is_submitter": false,
        "parent_id": "t1_m6m9dyb",
        "depth": 2
      },
      {
        "id": "m6rvwjy",
        "body": "I don't know about best, but it certainly is a fun addition to the arsenal.",
        "score": 1,
        "created_utc": 1736703065.0,
        "author": "peggedsquare",
        "is_submitter": true,
        "parent_id": "t1_m6n3iks",
        "depth": 3
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "1hyzsue",
    "title": "Any fully-funded tech conference in North America 2025???",
    "selftext": "Please who knows about any fully-funded data science conferences in North America.I want to expand my data science network and knowledge.I have  cold emailed a couple and they don't offer scholarships",
    "url": "https://www.reddit.com/r/data/comments/1hyzsue/any_fullyfunded_tech_conference_in_north_america/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 0,
    "created_utc": 1736613544.0,
    "author": "Thick-Serve4982",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hyzsue/any_fullyfunded_tech_conference_in_north_america/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hyr8hv",
    "title": "tech advice/help needed asap!",
    "selftext": "\nhi there! in an attempt to tidy up my phone, i have accidentally deleted over 10,000 of my photos from my icloud account and there is no way to recover them in this way. however, i have just realised that these photos are saved on an older unsynced device, and would like to find the safest way of uploading these to my hardrive (which has plenty of storage). i don’t want to reconnect this device to my apple account as i’m worried the photos (which were not taken on that device) will then be deleted. advice needed on how to do this safely please!!! e.g airdrop to other device, upload to computer then to hardrive etc",
    "url": "https://www.reddit.com/r/data/comments/1hyr8hv/tech_advicehelp_needed_asap/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 1,
    "created_utc": 1736582528.0,
    "author": "West-Enthusiasm4666",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hyr8hv/tech_advicehelp_needed_asap/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m6jq2od",
        "body": "Are you able to use a new computer that isn't connected to the internet? You can plug in your iPhone and mount it as a USB device. Then you could copy over all of the data from your phone to your hard drive without any fear of syncing.\n\n[https://www.copytrans.net/](https://www.copytrans.net/) and [https://imazing.com/](https://imazing.com/) are both programs you could use to pull data off of your iPhone.",
        "score": 1,
        "created_utc": 1736582966.0,
        "author": "Guavifo",
        "is_submitter": false,
        "parent_id": "t3_1hyr8hv",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1hypycj",
    "title": " How do you know if the data you use for analysis is significant?",
    "selftext": "Came across this question online and I'm not sure how I would answer it for a real world setting. How would you all answer it relative to your work/industry?",
    "url": "https://www.reddit.com/r/data/comments/1hypycj/how_do_you_know_if_the_data_you_use_for_analysis/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 3,
    "created_utc": 1736576957.0,
    "author": "chicanatifa",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hypycj/how_do_you_know_if_the_data_you_use_for_analysis/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m6n5ivh",
        "body": "It doesn’t surprise me that it’s been 15 hours since you asked because it’s really just a feeling more or less as random as a margins of error. \n\nI mean when I say it’s a feeling, it’s a feeling Ive honed over a 15 year career as an economist and 5 years as a Data Scientist. ",
        "score": 1,
        "created_utc": 1736632226.0,
        "author": "CleanDataDirtyMind",
        "is_submitter": false,
        "parent_id": "t3_1hypycj",
        "depth": 0
      },
      {
        "id": "m6pcqrs",
        "body": "If we’re talking testing then check the p-value.",
        "score": 1,
        "created_utc": 1736660934.0,
        "author": "data_story_teller",
        "is_submitter": false,
        "parent_id": "t3_1hypycj",
        "depth": 0
      },
      {
        "id": "m6tyt73",
        "body": "Telemetry data?",
        "score": 1,
        "created_utc": 1736724890.0,
        "author": "mike-manley",
        "is_submitter": false,
        "parent_id": "t3_1hypycj",
        "depth": 0
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1hwgz0h",
    "title": "Ideas for customer data collection at F&B restaurants",
    "selftext": "Hey guys!\n\nI want the details of the daily customers at a Food and Beverages restaurant. I need the Name, Phone number, and email address of the customers for whatsapp and email marketing. What are some of the ideas which I can use to get data of the customers. I also need to make sure the data is authentic and not fake.\n\nAlso, which is the best place to store the data and easy to access for various operations?\n\nPlease share your ideas here where I can get data of the customers without making them feel irritated. Would really appreciate your views!\n\nThanks in advance!",
    "url": "https://www.reddit.com/r/data/comments/1hwgz0h/ideas_for_customer_data_collection_at_fb/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1736331635.0,
    "author": "PlutoExists03",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hwgz0h/ideas_for_customer_data_collection_at_fb/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hwb9xz",
    "title": "Algerian Data Center Opportunities: DZ DATA Consortium",
    "selftext": "",
    "url": "https://i.redd.it/789yst095pbe1.jpeg",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1736310061.0,
    "author": null,
    "subreddit": "data",
    "permalink": "/r/data/comments/1hwb9xz/algerian_data_center_opportunities_dz_data/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hvt0bk",
    "title": "Open sourcing my python browser SDK that allows you use LLMs to scrape data from any site with prompts instead of scripts",
    "selftext": "Dendrite can be used to code AI agents / AI workflows that can:\n\n- 👆🏼 Interact with elements\n- 💿 Extract structured data\n- 🔓 Authenticate on websites\n- ↕️ Download/upload files\n- 🚫 Browse without getting blocked\n– 🛠️ Self-heal if website updates\n\nCheck it out here: [https://github.com/dendrite-systems/dendrite-python-sdk](https://github.com/dendrite-systems/dendrite-python-sdk)",
    "url": "https://www.reddit.com/r/data/comments/1hvt0bk/open_sourcing_my_python_browser_sdk_that_allows/",
    "score": 6,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1736261270.0,
    "author": "rivernotch",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hvt0bk/open_sourcing_my_python_browser_sdk_that_allows/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hvrz39",
    "title": "Organizing Files Across Multiple Hard Drives – Need Advice\n",
    "selftext": "I currently have 30-35 hard drives, and often I find myself needing a specific video or photo but can’t remember which hard drive it’s stored on.\n\nFor now, my workaround is to keep a folder on one of my drives containing screenshots of the folder structures on each hard drive. However, every time I update or move a file, I have to take a new screenshot and replace the old one, which is tedious and not very efficient.\n\nDo you know of any software or methods that could help me better organize or search across all my hard drives? I’d greatly appreciate your suggestions!",
    "url": "https://www.reddit.com/r/data/comments/1hvrz39/organizing_files_across_multiple_hard_drives_need/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1736258245.0,
    "author": "mipxtube",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hvrz39/organizing_files_across_multiple_hard_drives_need/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m6jnr5j",
        "body": "First and foremost, you need to make a backup copy of these files. Get a NAS and put all your files on there and that will solve your backup problem as well as your not being able to find things problem!",
        "score": 2,
        "created_utc": 1736581509.0,
        "author": "Guavifo",
        "is_submitter": false,
        "parent_id": "t3_1hvrz39",
        "depth": 0
      },
      {
        "id": "m608h84",
        "body": "What system you have on ?",
        "score": 1,
        "created_utc": 1736314605.0,
        "author": "AShmed46",
        "is_submitter": false,
        "parent_id": "t3_1hvrz39",
        "depth": 0
      },
      {
        "id": "m6123b1",
        "body": "I usually work on a Mac, but I frequently switch to a Windows system as well.",
        "score": 1,
        "created_utc": 1736332296.0,
        "author": "mipxtube",
        "is_submitter": true,
        "parent_id": "t1_m608h84",
        "depth": 1
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1hvtfnv",
    "title": "Collecting traffic data for the impacts of congestion pricing",
    "selftext": "As the title states, I want to pull traffic data for major roads in the NYC-Metro Area, specifically the following roads:\n\n* I-278\n* I-87\n* I-495\n* I-78\n* I-80\n* I-95\n\nI feel like google maps and waze would be my best bets (maybe apple maps if it's at all possible), but I've been unable to find a means to find historic data (only really need to go back 1yr). Does anyone know of an API or data broker from which I can pull data?",
    "url": "https://www.reddit.com/r/data/comments/1hvtfnv/collecting_traffic_data_for_the_impacts_of/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1736262454.0,
    "author": "cantcodeawaygluten",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hvtfnv/collecting_traffic_data_for_the_impacts_of/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m5w99u7",
        "body": "not sure if they have historical but TomTom API has a ton of traffic related data. Historical traffic data without paying is quite sparse from my experience. Hope this helps and good luck!\n\n  \n[Home | TomTom Developer Portal](https://developer.tomtom.com/) \n\nYou can sign up for a free API key",
        "score": 1,
        "created_utc": 1736268301.0,
        "author": "Wooden_Advantage_913",
        "is_submitter": false,
        "parent_id": "t3_1hvtfnv",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1hvqlhv",
    "title": "Best Practices for Identifying and Merging Duplicate records?",
    "selftext": "I’m working to identify and merge a large number of duplicate contact records for a client, and I need to have a bit more accuracy than I’ve had in the past. (In the past, I’ve had a larger team available to do a manual cleanup of potential duplicates that were identified)\n\nWe have basic details like First Name, Last Name, Company Name, Email, and Phone Number.\n\nAfter cleaning up all the exact duplicates, I got us down to around 1,000 to 2,000 remaining potential duplicates.\n\nHard part is, some contacts switch companies, so their email address changes, and that’s relatively easy, but if someone switches companies, gets married, changes their last name, and has a different phone and email, that’s a bit more difficult. I’m also having trouble creating an algorithm to look at things like Nicknames, Name typos, jr. and sr., etc.\n\nSometimes there a groups of duplicates, like 3 or more matching records, which is helpful, but then I run into issues with one bad match getting included in the Duplicate Group, which messes everything up.\n\n(I can include a GitHub link to my Python script if needed too)\n\nBut anyways, I know this is all kinda broad, but any guidance, best practices, suggestions, or stories about challenges you’ve had with duplicates and how you resolved those challenges would be helpful!",
    "url": "https://www.reddit.com/r/data/comments/1hvqlhv/best_practices_for_identifying_and_merging/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1736253788.0,
    "author": "Univium",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hvqlhv/best_practices_for_identifying_and_merging/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m6950de",
        "body": "the first thing i would do is put all of the variables which change for the same person into a history table and remove them from the primary table. that will help identifying the true duplicates",
        "score": 1,
        "created_utc": 1736442846.0,
        "author": "rwinters2",
        "is_submitter": false,
        "parent_id": "t3_1hvqlhv",
        "depth": 0
      },
      {
        "id": "m6e8sik",
        "body": "Yeah I’m start off by removing all the exact 100% duplicates, and now I’m trying to work through the remaining instances. Going to give it another go this weekend when I can really focus",
        "score": 1,
        "created_utc": 1736513946.0,
        "author": "Univium",
        "is_submitter": true,
        "parent_id": "t1_m6950de",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1hvk0rq",
    "title": "DEBATE : Grad in DATA SCIENCE or MBA?",
    "selftext": "I personally think MBA is better as it allows for more opportunity in the future but as I have studied data science I understand how one opinion should never be considered accurate data\n\nSo let's get your input",
    "url": "https://www.reddit.com/r/data/comments/1hvk0rq/debate_grad_in_data_science_or_mba/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 3,
    "created_utc": 1736226325.0,
    "author": "Tejas1305",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hvk0rq/debate_grad_in_data_science_or_mba/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m5ty9r1",
        "body": "It's a personal decision. The key question is your placement after the degree. \n\nA few years after getting the degree it won't matter except to check HR boxes.\n\nAlso interest and aptitude are typically more critical when dealing with ideas that are both practical. An MBA, a CPA, a Masters in CS, and a Masters in DS are all similar enough where the bigger question will be the domain an individual is more likely to enjoy and succeed at.",
        "score": 1,
        "created_utc": 1736229492.0,
        "author": "Glotto_Gold",
        "is_submitter": false,
        "parent_id": "t3_1hvk0rq",
        "depth": 0
      },
      {
        "id": "m5vrpq0",
        "body": "It depends on your goals. If you want a technical role and lack those skills, an MBA won’t be as helpful as a technical degree like a MS in Data Science. But if you want to be a team lead or be in a more strategic planning role, especially if you already have a technical undergrad, then MBA can be helpful.",
        "score": 1,
        "created_utc": 1736262987.0,
        "author": "data_story_teller",
        "is_submitter": false,
        "parent_id": "t3_1hvk0rq",
        "depth": 0
      },
      {
        "id": "m5u77kf",
        "body": "never thought about it like that THANKS MAN",
        "score": 1,
        "created_utc": 1736234471.0,
        "author": "Tejas1305",
        "is_submitter": true,
        "parent_id": "t1_m5ty9r1",
        "depth": 1
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1hvfd8g",
    "title": "Data script step by step",
    "selftext": "Hello World !\n\nI’m looking for a simple way to visualize the transformations I apply to my data in a Python script.\n\nIdeally, I’d like to see step-by-step changes (e.g., before/after each operation).\nAny tools or libraries you’d recommend ?\n",
    "url": "https://www.reddit.com/r/data/comments/1hvfd8g/data_script_step_by_step/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1736211950.0,
    "author": "Nadnadou",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hvfd8g/data_script_step_by_step/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hul5kk",
    "title": "Data analysis or data science in healthcare?",
    "selftext": "Hello! I am writing the following hoping to find some advice or support regarding the topic mentioned in the title. I am a general physician with 3 years of experience, I live in Tijuana, Mexico, but I have thought that it might not be entirely my thing and I would like to dedicate myself to something else in which I can continue using that medical knowledge. I took a data science course and learned about ML, Deep learning, Python, and even data visualization. But now I don't know how to start; I looked for some projects on Kaggle, but there isn't much focused on health (or maybe I'm not good at searching). If there is any data analyst/scientist who can give me some advice, I would greatly appreciate it. I would be willing to dedicate 20-30 hours per week without pay to a company in order to gain experience, since currently my work as a doctor does not take up much of my time.",
    "url": "https://www.reddit.com/r/data/comments/1hul5kk/data_analysis_or_data_science_in_healthcare/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1736121018.0,
    "author": "0alexmyz0",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hul5kk/data_analysis_or_data_science_in_healthcare/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m5nhcwa",
        "body": "In healthcare, as an analyst, you will be using sql and VBA.",
        "score": 3,
        "created_utc": 1736139634.0,
        "author": "MadMaxfrmShottas",
        "is_submitter": false,
        "parent_id": "t3_1hul5kk",
        "depth": 0
      },
      {
        "id": "m5o5ry8",
        "body": "And excel",
        "score": 3,
        "created_utc": 1736153464.0,
        "author": "k00_x",
        "is_submitter": false,
        "parent_id": "t1_m5nhcwa",
        "depth": 1
      },
      {
        "id": "m5zxba5",
        "body": "Lol fucking excel.\n\nWe’ll be calculating flight paths on Mars to go planet hopping and still be using excel for some shit",
        "score": 1,
        "created_utc": 1736309690.0,
        "author": "CleanDataDirtyMind",
        "is_submitter": false,
        "parent_id": "t1_m5o5ry8",
        "depth": 2
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1ht94dx",
    "title": "Entry Level Job Leads",
    "selftext": "Hi everyone! I am new to this subreddit but I wanted to some help on searching for Entry Level Data Analyst jobs. I'm a Comp Sci graduate, with a minor in Mathematics, looking to break out in the world of Data. I have very little Data experience (only worked as a researcher for a month or two and did some analysis at my current position). \n\nI have applied to about 100 places, but LinkedIn and Indeed do not show me positions matching my criteria (remote if in a different state, or 10-20 miles from where I live. I'm about 20 minutes from New York, NY. Any help would be greatly appreciated!",
    "url": "https://www.reddit.com/r/data/comments/1ht94dx/entry_level_job_leads/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1735975164.0,
    "author": "XxDaUnKn0WnxX",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ht94dx/entry_level_job_leads/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hsw734",
    "title": "Asphalt market",
    "selftext": "Completely new to finding data. Struggling to find credible data related to the segmentation of the asphalt market. Mainly segmenting it on commercial public residential other or roads waterproofing recreation other. Please replay asap im on a time crunch would appreciate any help",
    "url": "https://www.reddit.com/r/data/comments/1hsw734/asphalt_market/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1735936194.0,
    "author": "nkj12",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hsw734/asphalt_market/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hsuemn",
    "title": "Help for data sorting/clustering ",
    "selftext": "I need help with a sorting problem. I have a 90*100 image. Every pixel contains information of up to 3 gaussians, but sometimes there are less gaussians in one pixel. They represent the best fit of an emission line that is made up of multiple components. Each gaussian corresponds to a kinematic component in the emission line. I now have to sort these gaussian components, so the components are consistent across the whole image. Simply sorting by width and mean is not sufficient, as single cuts are not enough for the complex data. How can I sort my data well?",
    "url": "https://www.reddit.com/r/data/comments/1hsuemn/help_for_data_sortingclustering/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1735931681.0,
    "author": "late-nighter",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hsuemn/help_for_data_sortingclustering/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m5mxq0e",
        "body": "I follow everything you are saying except \"Each \\[G\\]aussian corresponds to a kinematic component in the emission line.\"\n\nAlso, why would you have fewer than 3 components in your mixture distribution? \n\nI suppose you have not gotten any comments because further clarification is necessary. Also, an example of what you are trying to do may help.",
        "score": 1,
        "created_utc": 1736132359.0,
        "author": "FFFRabbit",
        "is_submitter": false,
        "parent_id": "t3_1hsuemn",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1hs92n2",
    "title": "How do I get business metadata? (data management)",
    "selftext": "Am I stupid or does it seem like every Data Management platform primarily focuses on functionality around technical metadata (data about tables, columns, etc). We are currently looking at options to buy a data cataloguing tool, but the way I see it, once we ingest all the technical metadata, we need to enrich it with business metadata (context) for the business side.\n\nOur current situation is our business metadata is scattered across many places (excel sheets, pdf files, data models in visual diagrams). It seems like someone will have to go through all the technical metadata and manually add business context to it.\n\nIs there a better way? Any SaaS recommendations?\n\nIndustry: Healthcare, medium size business ",
    "url": "https://www.reddit.com/r/data/comments/1hs92n2/how_do_i_get_business_metadata_data_management/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1735863748.0,
    "author": "OkAcanthaceae6314",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hs92n2/how_do_i_get_business_metadata_data_management/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hs24kf",
    "title": "What features do you consider most important in data protection software? ",
    "selftext": "In today's digital landscape, safeguarding your data is more critical than ever. Choosing the right software ensures that your data remains safe and your organization is prepared to sustain any potential cyber breaches. We've tried to make a checklist of some key features we recommend paying attention to when choosing a data security software. Would love to hear more tips from you as well. \n\n* Robust Security Measures: The primary function of data protection software is to secure your information. Look for encryption, multi-factor authentication, and data loss prevention features to ensure that your data is well-protected against unauthorized access and breaches. \n* User-Friendly Interface: The software should be easy to navigate. A user-friendly interface ensures that your team can quickly learn to use the software effectively without extensive training. \n* Backup and Recovery Options: Data loss can occur for various reasons, including accidental deletion, hardware failure, or cyberattacks. Choose software that offers automatic backups and efficient recovery options. Test the restore process to recover your data promptly when needed. \n* Scalable Solutions: As your business grows, so will your data protection needs. Look for software that can quickly scale with your organization, offering flexible storage options and the ability to manage additional users without significant increases in costs or complexity. \n* Regular Updates and Maintenance: Cyber threats are constantly evolving, and so should your data protection software. Choose a solution that provides regular updates and maintenance to defend against new vulnerabilities and threats. \n* Integration Capabilities: Consider how well the software integrates with your existing systems. Seamless integration can enhance productivity and reduce the time it takes for your team to switch between different tools. \n\nWhat do you usually pay attention to when choosing a data security tool?",
    "url": "https://www.reddit.com/r/data/comments/1hs24kf/what_features_do_you_consider_most_important_in/",
    "score": 4,
    "upvote_ratio": 0.83,
    "num_comments": 1,
    "created_utc": 1735846272.0,
    "author": "Syncplify",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hs24kf/what_features_do_you_consider_most_important_in/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hrs16h",
    "title": "Resume review needed ",
    "selftext": "Currently applying for internships. Participating in hackathons and contributing in open source projects. Need to do some improvements in resume as it is getting rejections from big techs ",
    "url": "https://i.redd.it/cxy3w36bkkae1.jpeg",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1735818743.0,
    "author": "_xtrishx_",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hrs16h/resume_review_needed/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hrk5q4",
    "title": "What is a good first place to start? ",
    "selftext": "Hello! I’m a physics  major and I’m aiming to go into data for my career. Particularly a field in data science like analytics, ML, quantum, etc but I’m not 100% sure on what field in data yet, but all of them seem very enticing to me as I love math, physics, and fixing chaotic situations is something I find very satisfying especially in math. In fact, the main reason I got into physics is because it allows me to make sense of the chaotic world/galaxy we live in! \n\nI have very little experience with stats. In fact, I would consider myself a complete beginner, but from all I have seen, it’s very interesting to me and I also find AI very fascinating. I am also going to be most likely taking a data science minor as my school offers one and I plan on either continuing physics or specializing in data science in grad school. I have to be honest. I’m a bit overwhelmed on where to start given I’m just beginning my journey. I’ve started studying Python on my own with a crash course book and so far I love it! It’s a lot better to work with for me than Java which is a language I took in my previous semester’s intro programming class. \n\nI was also considering purchasing a stats book for beginners but I can’t spend too much. \n\nAny advice on what I can do for my first steps in getting into data? \n\nThank you! ",
    "url": "https://www.reddit.com/r/data/comments/1hrk5q4/what_is_a_good_first_place_to_start/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1735787421.0,
    "author": "Maleficent_Writer297",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hrk5q4/what_is_a_good_first_place_to_start/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m4z11ge",
        "body": "For data science I’d say read “Hands on Machine Learning with Scikit Learn” by O’Reilly. I believe you could find a free ebook version on GitHub.",
        "score": 1,
        "created_utc": 1735799386.0,
        "author": "MadMaxfrmShottas",
        "is_submitter": false,
        "parent_id": "t3_1hrk5q4",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1hr8k2c",
    "title": "Data roles",
    "selftext": "This might not be the correct forum for this so please remove if so.\n\n\nCurrently working as a  junior Project Manager. I have over a decade of financial services experience and a good salary. I feel like my heart isn't in it and have seen some of the challenges more senior Project Managers have endured and don't think it's for me.\n\nI have worked previously as a PMO analyst which I did enjoy more. I have an interest in data and have the basics in PowerBI, Tableau and SQL and would like to work in a role leveraging these tools etc\n\nAnyone been through this or any advice on more data focused roles etc",
    "url": "https://www.reddit.com/r/data/comments/1hr8k2c/data_roles/",
    "score": 2,
    "upvote_ratio": 0.75,
    "num_comments": 0,
    "created_utc": 1735755037.0,
    "author": null,
    "subreddit": "data",
    "permalink": "/r/data/comments/1hr8k2c/data_roles/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hq9609",
    "title": "Resume Review Please",
    "selftext": "https://preview.redd.it/dpmrm5y1t4ae1.png?width=1404&format=png&auto=webp&s=d41cd15dcd8d842ab8f148c56113f2c38e25b111\n\nI will be a senior majoring in Business Data Analytics and Marketing (Digital and Integrated Communications). I need help with a resume review as I am an international student and will be graduating in a year from now. I know I don't have practical experience in my field, sadly, but am aiming to get an internship in Summer 2025. The practical experience should boost my profile. I am struggling with getting anything at all so anything form a data analyst will be very appreciated. P.S. I would love to get an H-1B sponsorship and stay in the states :).",
    "url": "https://www.reddit.com/r/data/comments/1hq9609/resume_review_please/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1735628049.0,
    "author": "Sad_Individual_3857",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hq9609/resume_review_please/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hq0aw2",
    "title": "Learning how to organise some data for the first time. Is Google Spreadsheets my best choice for this?",
    "selftext": "Getting big into a video game and need to start sorting drop tables, quality of drops, and sources of drops into a digestible format for my guild. Heres the break down\n\nYou use a Treasure map of common, uncommon or rare quality.   \nEach rarity of map has chances of different items and those items can drop in different amounts AND at different quality. The higher the rarity of map the better chances. \n\nFor example a common treasure map will have a chance at dropping 30-40 uncommon ruby gemstones, but a uncommon Treasure map will have a higher chance of dropping 30-40 uncommon rubys. \n\nAfter digging around I do think the best way to do this will be learning some more tools on google spreadsheets but wanted to poke here for advised opinions on different tools/methods of organsing this type of data",
    "url": "https://www.reddit.com/r/data/comments/1hq0aw2/learning_how_to_organise_some_data_for_the_first/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1735600131.0,
    "author": "trendyghost",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hq0aw2/learning_how_to_organise_some_data_for_the_first/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hpqtii",
    "title": "How do you keep track of reports/insights?",
    "selftext": "Hey all,\nI was wondering how other people in other companies keep track of reports or insights you made for different stakeholders.\n\nLets say that the marketing team wants to know how well a certain campaign did and you do an analysis on their ab test. Next year they want to do a similar test, how would they find it back, where is it stored?\n\nI'm super curious as I'm thinking about a small SaaS solution to build for this. In our company we self host a small website where Jupyter notebooks could be hosted.",
    "url": "https://www.reddit.com/r/data/comments/1hpqtii/how_do_you_keep_track_of_reportsinsights/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1735575657.0,
    "author": "4percentalpha",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hpqtii/how_do_you_keep_track_of_reportsinsights/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hphau2",
    "title": "You change an algorithm in production - now what?",
    "selftext": "Alright so you've got some custom analytics churning in the cloud or on premise. You worked hard on it, stakeholders are happy.\n\nBut you notice a bug in the business logic, perhaps your confidence in the accuracy is lower than you thought. So you fix the bug, run the tests and all looks good.\n\nBut you now have loads of old results from a sub par algorithm. And the new algorithm will produce slightly different results going forward.\n\nWhat do you do?",
    "url": "https://www.reddit.com/r/data/comments/1hphau2/you_change_an_algorithm_in_production_now_what/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1735541173.0,
    "author": "Solvicode",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hphau2/you_change_an_algorithm_in_production_now_what/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m4i4n5t",
        "body": "I'm not sure I understand your question. What is the problem you're facing? This is a very common scenario. Models always need to be retrained and refined.",
        "score": 1,
        "created_utc": 1735553707.0,
        "author": "goatsnboots",
        "is_submitter": false,
        "parent_id": "t3_1hphau2",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1hmu8tn",
    "title": "is it too late for a 27 years old to enter this field ?",
    "selftext": "hey, i need some advise but i don't have anyone in my circle that can help, so i'm seeking you guys.\n\ni'm a 27 year old guy and i want to enter the data field. i know it's complex and most newcomers don't know exactly what data science is. but i think i have a good grasp about this field for someone who did not have the opportunity to study it officially. i have a masters degree in petrochemistry and worked in it for a while, and I HATE IT, it's not for me at all. though it was a good experience to put under my belt. but through out all this time i developed big interest in IT and data analysis.i didn't think about having a career in it so i persued it like a hobbie and before i know it i have a pretty good grasp of one coding language and a couple a data manipulation libraries. now i find myself skipping my actually work to do random data projects. so i'm seriously thinking to improving my skills and entering DATA science field but i can't help the feeling that maybe i'm late to the train. if i enter this field by the time i get a good grasp on it and enter it i'll find myself as an old guy amongst fresh graduates. is there a stigma for that kind of thing ? if anyone did a career change in his life and entered this field i would love to get your perspective.\n\nsorry if this is not a usual topic around here.",
    "url": "https://www.reddit.com/r/data/comments/1hmu8tn/is_it_too_late_for_a_27_years_old_to_enter_this/",
    "score": 6,
    "upvote_ratio": 0.64,
    "num_comments": 23,
    "created_utc": 1735236903.0,
    "author": "givemeanameplease31",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hmu8tn/is_it_too_late_for_a_27_years_old_to_enter_this/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m3wsb3h",
        "body": "Never too late. My career has always been data adjacent but I didn’t commit to full time data roles until I was in my late 30s. You’ve gotta work for so many years, might as well be doing a job you enjoy.",
        "score": 4,
        "created_utc": 1735238100.0,
        "author": "Logical-Tea5811",
        "is_submitter": false,
        "parent_id": "t3_1hmu8tn",
        "depth": 0
      },
      {
        "id": "m453t7t",
        "body": "Sorry 26 and half is the cutoff; it’s rule and we’re serious about our parameters ",
        "score": 3,
        "created_utc": 1735357208.0,
        "author": "CleanDataDirtyMind",
        "is_submitter": false,
        "parent_id": "t3_1hmu8tn",
        "depth": 0
      },
      {
        "id": "m48cv6g",
        "body": "I got into it at 32. I went to school for finance and marketing and spent 10 years after school doing real estate brokerage but was always more interested in the data side of things and our brokerage was also very data-focused when it came to decision making. \n\nDid some data consulting for my own brokerage and then got into SQL (Postgres) querying blockchains. From there started doing my own projects using tableau and python, did probably 500 hours of coursera learning and projects and wrote up my analyses on Medium to mimic what it’s like to consolidate findings and communicate to an audience of stakeholders. I think I paid $50/mo for coursera and spread those 500 hours across 18-24 months. I would NOT recommend  dropping $1000’s on a boot camp unless you want to bypass analytics and go straight into data science. \n\n27’s a great time, especially if you find it interesting and enjoy problem-solving and modeling complex, real life problems. \n\nFocus your personal projects on stuff that interests you if you can - it was important interview fodder for me. It sounds like you’re already doing this. \n\nLanded a senior data analyst job focused on real-estate site selection in a second tier metro with a great salary. Data analysis is also super friendly as a work from home which opens up a ton of opportunities.\n\nData analysis is relatively easy to parlay into data science, especially given your background in higher ed in a specific domain. DA and DS are ALWAYS in demand too so you get a nice boost in salary even when career switching. \n\nI’ve found the Biggest difference maker as far as standing out is being able to clearly communicate your findings. That is a different skill set from being able to use the tools like python so if you are good at that you’re golden given your background.\n\nGo for it!",
        "score": 3,
        "created_utc": 1735412227.0,
        "author": "glaci0us",
        "is_submitter": false,
        "parent_id": "t3_1hmu8tn",
        "depth": 0
      },
      {
        "id": "m3y2qgw",
        "body": "Absolutely not. I realized I was interested in data analytics at 27-28, no background in math/analytics (bachelor’s in political science). Now 34, working in analytics at FAANG. Yeah, some of my coworkers are significantly younger than me, but you know what? I couldn’t care less, I’m really grateful to be doing what I do at the company I do it at. \n\nWhat worked *for me*: \n- TIP 1: can you get into an analytics adjacent role? At the time, I was working in operations at a tech startup. I made it clear to my manager that I was very interested in data analytics, and he helped connect me some projects where I could start to learn. \n- TIP 2: find a mentor. I was fortunate enough to start working with our main analytics folks on some projects where they helped teach me SQL. During this time, I took on increasingly complex analytics projects. \n- TIP 3: craft your resume for the job you want. I created a resume that highlighted the analytics things I did in my operations role and downplayed the day to day operations work I did. \n- TIP 4: consider analytics focused education (boot camps, masters, etc). I was fortunate enough to be able to use my GI Bill to get a Master’s in analytics during COVID when work was slower. Doing a Master’s while working full time was an absolute grind, but in the end it was the thing that propelled me into a full analytics role at a top company. \n\nCaveats:\n- I’m not so self absorbed as to think that I accomplished this through my own hard work alone. I had great mentors, fortunate situations (living in SF and already in tech, GI Bill to pay for school etc) that gave me a huge leg up. \n- No free lunches - it was hard work to accomplish all this, and required working longer hours for years (to take on extra analytics work) as well as 18+ months of very limited free time while I did my Master’s and worked full time. \n\nAll in all - if you want it, go get it! Believe in yourself, you can do this!",
        "score": 2,
        "created_utc": 1735253802.0,
        "author": "avdata",
        "is_submitter": false,
        "parent_id": "t3_1hmu8tn",
        "depth": 0
      },
      {
        "id": "m44pdyh",
        "body": "It is never too late.\n\nI am in my 70's and believe in continuous learning.\n\nNever stop learning.\n\nNever stop being curious.",
        "score": 2,
        "created_utc": 1735351411.0,
        "author": "rick_1717",
        "is_submitter": false,
        "parent_id": "t3_1hmu8tn",
        "depth": 0
      },
      {
        "id": "m467k99",
        "body": "Not too late.  I was 50+ when I started.",
        "score": 2,
        "created_utc": 1735378932.0,
        "author": "skampnyc",
        "is_submitter": false,
        "parent_id": "t3_1hmu8tn",
        "depth": 0
      },
      {
        "id": "m3wrib8",
        "body": "[removed]",
        "score": 3,
        "created_utc": 1735237840.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_1hmu8tn",
        "depth": 0
      },
      {
        "id": "m3x2s3i",
        "body": "If you’re 27 you’re still pretty green in most people’s eyes. So don’t sweat it. \n\nUnless you’re less than 5-10 years away from your retirement, you’re probably not to old to switch fields. Certainly if it’s to a field with at least some transferable skills…\n\nI would actually be inclined to say that some of the most interesting and successful people I know, have switched fields or careers at some point.",
        "score": 1,
        "created_utc": 1735241528.0,
        "author": "my_key",
        "is_submitter": false,
        "parent_id": "t3_1hmu8tn",
        "depth": 0
      },
      {
        "id": "m3xdwwc",
        "body": "Firstly, Data is not complex, the foundation on databases, AI has largely stayed the same since the 70s , set theory, sorting algorithms, statistics etc. Data science is a swanky term for long standing practice of statistics and math , applied to datasets, I admit that this now happens very easily where previously it was all custom.\nThere is no concept of too late, it all depends on your tolerance for learning, such as intelligence but also circumstances , ie if you have a solid paying job and mortgage but you want to pivot to something that pays half of you current salary.\nCompanies don’t do learn on the job anymore as much as before, so learning is at your own cost and becomes very hard if you need to provision your own infrastructure in the cloud to learn. AWS is very good in this regard for free gamified learning . Azure and google has nothing comparable. Basic data stuff and even pretty advanced insights can now come from AI so you have got to do something more to make a living .\nIt’s never too late and in a good role you can do your real job quickly and then learn on the side , but this learning will be hard to quantify in a job application. \nFor instance I’ve spent 15 years as a project manager but my last few gigs were business analysis, now recruiters think i don’t have any PM experience. Never too late to enter but think the process through",
        "score": 1,
        "created_utc": 1735245277.0,
        "author": "Tofutruffles",
        "is_submitter": false,
        "parent_id": "t3_1hmu8tn",
        "depth": 0
      },
      {
        "id": "m3zv7vg",
        "body": "27 is pretty young, silly Q!",
        "score": 1,
        "created_utc": 1735280135.0,
        "author": "Electrical_Shift_729",
        "is_submitter": false,
        "parent_id": "t3_1hmu8tn",
        "depth": 0
      },
      {
        "id": "m45gaq4",
        "body": "I mean I'm basically just echoing most of what I see here but unless you're asking about starting a professional sports career where you have to start as a literal child, 27 is not too late to change just about anything. \n\nIn fact, I'd say affirmatively if you feel like you need to make some sort of change, do it now. Worse case, you take a step backwards for a couple years. Better that than to look back 20 years from now and wonder what could have been.",
        "score": 1,
        "created_utc": 1735362762.0,
        "author": "ElectrikMetriks",
        "is_submitter": false,
        "parent_id": "t3_1hmu8tn",
        "depth": 0
      },
      {
        "id": "m46bxe7",
        "body": "First you need to answer why do you think this is a field worth to get into for you. Please don’t quote what you heard from people.",
        "score": 1,
        "created_utc": 1735381898.0,
        "author": "notimportant4322",
        "is_submitter": false,
        "parent_id": "t3_1hmu8tn",
        "depth": 0
      },
      {
        "id": "m4gdu09",
        "body": "Absolutely not—it’s not too late at all. I was in a similar position myself when I started in data at 25, after being in Science for 3 years after i finished University with a BSc in Biochemistry and I’m here to tell you that 27 is a great time to make the switch. Let me share some advice and perspective based on my journey:\n\nYour age brings something incredibly valuable: experience. While fresh graduates might have academic knowledge, your professional background (in petrochemistry and beyond) equips you with: Problem-solving skills. The ability to approach problems with a mature, strategic perspective and cross-disciplinary insights that make you stand out.\n\nNo one in this field will see you as “too old.” In fact, career changers are often admired for their motivation and ability to adapt.\n\nThe skills you’ve already picked up, like coding and data manipulation, are an excellent starting point. Take it a step further by aligning your learning with industry tools and techniques. Here’s what I recommend: Build on your proficiency in Python or R and learn SQL (if you haven’t yet). These are foundational for almost any role in data. Learn tools like Tableau, Power BI, or Matplotlib/Seaborn in Python and continue doing personal data projects. These are great portfolio pieces that demonstrate your skills and passion to potential employers.\n\nIf you want to stand out, consider focusing on roles that merge your past experience with your new skills. For instance, analytics in energy, manufacturing, or engineering domains could make you a sought-after candidate. However, if you’re not passionate about sticking with petrochemistry-related roles, that’s fine too. Your domain knowledge will still show employers that you’re adaptable and capable of picking up industry-specific details.\n\nDon’t hesitate to start with an entry-level role or internship. I'm on an apprenticeship and it’s a stepping stone, and your background will likely accelerate your growth.\n\nI can relate to feeling like you’re behind, but that’s just imposter syndrome. Many people in the data field come from non-traditional backgrounds—finance, biology, education, and more. The diversity of experiences is one of the reasons this field is so dynamic.\n\nRemember, this isn’t just about breaking into data it’s about building a rewarding, long-term career. Invest time in developing your skills and gaining experience now, and you’ll thank yourself years down the line when you’re thriving in a field you love.",
        "score": 1,
        "created_utc": 1735523553.0,
        "author": "Extension_Laugh4128",
        "is_submitter": false,
        "parent_id": "t3_1hmu8tn",
        "depth": 0
      },
      {
        "id": "m4t0zje",
        "body": "I went from $20 per hour to $160k per year and I am now 27. I make simple Power BI reports. I’ve studied our user facing information systems very well to understand nuance in data capture. I can write decent sql including window functions. I have saved the company tangible dollars with certain reports. \n\nAnyone can do what I did. \n\nHere is what made all the difference:\n- solid IT leadership. I don’t know shit about fire walls or servers but someone else did.\n\n- work in small to medium size enterprise. You will find lots of business folks and not a lot of technical. Makes it easy to shine. \n\n- I understand accounting. One semester of study will make all the difference coupled with studying financial impacts in ERP.\n\n- be a good listener and study the business.",
        "score": 1,
        "created_utc": 1735706745.0,
        "author": "Silent_Success_9371",
        "is_submitter": false,
        "parent_id": "t3_1hmu8tn",
        "depth": 0
      },
      {
        "id": "m5my81v",
        "body": "It is never too late. I have been on the analysis side of the defense industry for over 20 years and eager minds who can understand how to analyze data and provide context-specific interpretations are still in short supply.",
        "score": 1,
        "created_utc": 1736132533.0,
        "author": "FFFRabbit",
        "is_submitter": false,
        "parent_id": "t3_1hmu8tn",
        "depth": 0
      },
      {
        "id": "m62txs2",
        "body": "Haha🥴🤪",
        "score": 1,
        "created_utc": 1736356826.0,
        "author": "debdebweb",
        "is_submitter": false,
        "parent_id": "t1_m453t7t",
        "depth": 1
      },
      {
        "id": "n0l5ny1",
        "body": "Sorry for the month's old reply, but I just wanted to say that I am in a similar situation and that your comment really inspired me.\n\nI am glad it worked out for you.\n\nI just wanted to ask about math, I have a background in programming so I am not worried about that, but I am worried that my weak (almost non existent) stat and linear algebra will prevent me from progressing.\n\nDo you mind sharing a tip or two?",
        "score": 1,
        "created_utc": 1751295504.0,
        "author": "CyperFlicker",
        "is_submitter": false,
        "parent_id": "t1_m48cv6g",
        "depth": 1
      },
      {
        "id": "n02w6jx",
        "body": "I know this is 6 months later, but thanks for this comment.\n\nI am 24, and I already started working as frontend dev, but I am realizing that it is not my cup of tea. I was thinking about starting to study DS in my free time, but I was worried that my weak math background and starting this late would hinder me.\n\nAnyway I just wanted to say, your comment gave me some encouragement, I actually started reading an introduction to statistical learning (which I am enjoying) and I'll try my best to push forward (math still is scary lol).",
        "score": 2,
        "created_utc": 1751037982.0,
        "author": "CyperFlicker",
        "is_submitter": false,
        "parent_id": "t1_m3y2qgw",
        "depth": 1
      },
      {
        "id": "m3xd94g",
        "body": "What the hell is wrong with you? Man should do what he's passionate about. I'd rather hire a well versed passionate data scientist than some shitty half assed money grab. If he lvoes data science sure, but if he doesn't he shouldn't. That's you, and I'd imagine you'll skip on doing things the right way and mislead your employers. You should probably switch to something else then. It's too easy to lie/fail even unintentionally, and you'll need other specialists to tell you you're wrong. And noone is as wrong as half arsed unpassionate money grabber.",
        "score": 4,
        "created_utc": 1735245055.0,
        "author": "JerryBond106",
        "is_submitter": false,
        "parent_id": "t1_m3wrib8",
        "depth": 1
      },
      {
        "id": "myui28s",
        "body": "because data, is that good enough alternative answer",
        "score": 1,
        "created_utc": 1750440691.0,
        "author": "Lanarde",
        "is_submitter": false,
        "parent_id": "t1_m46bxe7",
        "depth": 1
      },
      {
        "id": "n02yh6f",
        "body": "I’m really glad to hear it! Good luck with your journey, the hardest part is getting started.",
        "score": 1,
        "created_utc": 1751038631.0,
        "author": "avdata",
        "is_submitter": false,
        "parent_id": "t1_n02w6jx",
        "depth": 2
      },
      {
        "id": "m3xex5z",
        "body": "[removed]",
        "score": -4,
        "created_utc": 1735245617.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t1_m3xd94g",
        "depth": 2
      }
    ],
    "comments_extracted": 22
  },
  {
    "id": "1hm186x",
    "title": "App data recovery, help",
    "selftext": "Hi, if possible could someone tell me if there is a way to get all my old messages back from the closed app called Zenly? The location doesn’t really matter to me but the messages does a lot",
    "url": "https://www.reddit.com/r/data/comments/1hm186x/app_data_recovery_help/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1735135249.0,
    "author": "Important-Truth8524",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hm186x/app_data_recovery_help/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hld20u",
    "title": "What Does a Beginner Need to Start in Data Science?",
    "selftext": "**Hello,**\n\nI'm currently enrolled in a data science course, and I understand the importance of mastering various libraries, statistical concepts, SQL queries, and creating PowerBI dashboards. However, as a beginner, I'm looking for guidance on where to start and what to practice daily to build a strong foundation.\n\nCould you please share your recommendations on essential skills, tools, and daily practices that would benefit a beginner in data science? Any advice on how to structure my learning and what resources to use would be greatly appreciated!\n\nThank you!",
    "url": "https://www.reddit.com/r/data/comments/1hld20u/what_does_a_beginner_need_to_start_in_data_science/",
    "score": 5,
    "upvote_ratio": 1.0,
    "num_comments": 6,
    "created_utc": 1735045248.0,
    "author": "ms_cutie",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hld20u/what_does_a_beginner_need_to_start_in_data_science/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m3pfwyq",
        "body": "Starting in data science as a beginner involves mastering a mix of programming, statistical concepts, and data visualization. Focus on learning Python and its essential libraries like NumPy, Pandas, and Matplotlib for data manipulation and visualization. SQL is crucial for data management, so practice writing queries regularly. Engage with daily coding challenges and tackle real-world problems on platforms like StrataScratch and Kaggle to apply your skills. Supplement your learning with online courses from platforms like Coursera. Setting weekly goals and working on small projects will help you apply what you've learned and build a strong portfolio.",
        "score": 5,
        "created_utc": 1735110174.0,
        "author": "Ans979",
        "is_submitter": false,
        "parent_id": "t3_1hld20u",
        "depth": 0
      },
      {
        "id": "m3rwfvt",
        "body": "I'm biased, but statistics and SQL. A data scientist will be applying statistics to a selection of data most of the time. Master SQL, it's not going anywhere and it's the easiest code you will ever use.",
        "score": 3,
        "created_utc": 1735155917.0,
        "author": "k00_x",
        "is_submitter": false,
        "parent_id": "t3_1hld20u",
        "depth": 0
      },
      {
        "id": "m3v7r41",
        "body": "You're enrolled in data science so you're learning the complicated interesting bits that will be the frosting on the cake. The real cake though is a lot of soft skills and grinding through a lot of data cleansing and rudimentary analysis. I don't know of a good way to learn the soft skills other than the hard way but I think working through unfamiliar datasets, especially unclean ones will help a lot. \n\nYou can start with the usual kaggle type stuff and just get really good at exploring and profiling the data but it gets more interesting when you start grabbing data from local governments and those types of places. While it will be somewhat cleaned up, you'll find lookups are missing, some fields aren't as described, and so on. If you can take those datasets and connect them with other real-world data and work up to a visualization or functional analysis, you're basically doing the job. You'll know you have the balance about right when you spend your time about 10:1, data cleansing & grooming : final analysis.",
        "score": 2,
        "created_utc": 1735216320.0,
        "author": "amosmj",
        "is_submitter": false,
        "parent_id": "t3_1hld20u",
        "depth": 0
      },
      {
        "id": "m3nnip7",
        "body": "I made a career out of straight SQL, which has worked well for me. If you want more in-depth knowledge of SQL Server with some practice queries, [check out my book.](https://www.amazon.com/dp/B0DQCDQBHN)",
        "score": 1,
        "created_utc": 1735078356.0,
        "author": "AnalogKid-82",
        "is_submitter": false,
        "parent_id": "t3_1hld20u",
        "depth": 0
      },
      {
        "id": "m3ug8xf",
        "body": "Thank you so much \nIt's a helpful",
        "score": 1,
        "created_utc": 1735197232.0,
        "author": "ms_cutie",
        "is_submitter": true,
        "parent_id": "t1_m3pfwyq",
        "depth": 1
      },
      {
        "id": "m3ugbdw",
        "body": "Thank you 😊",
        "score": 1,
        "created_utc": 1735197279.0,
        "author": "ms_cutie",
        "is_submitter": true,
        "parent_id": "t1_m3rwfvt",
        "depth": 1
      }
    ],
    "comments_extracted": 6
  },
  {
    "id": "1hlmvb4",
    "title": "Survey data on what Americans think of Luigi Mangione",
    "selftext": "Found this poll quite interesting. Seems like Americans outside of Reddit are pretty divided on their views on Luigi Mangione.\n\nSome trends to point out:\n\n- Older folks have a significantly less favourable view of Luigi Mangione despite overall having worse opinions of the health care industry and higher prevalence of chronic pain compared to younger folks\n\n- Older folks share similar views on the poor accountability of corporations as younger folks but are significantly more against violence against corporations compared to younger folks\n\n- People with higher income are generally more informed and more opinionated on the whole ordeal compared to people with lower income\n\nObviously sample size is quite small and the assumption that it was anonymous with random sampling. Views might have also changed compared to 2 weeks ago. Welcome your thoughts and discussion.",
    "url": "https://d3nkl3psvxxpe9.cloudfront.net/documents/Health_Insurance___Murder_of_Brian_Thompson_poll_results.pdf",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1735075474.0,
    "author": "zerorange-101",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hlmvb4/survey_data_on_what_americans_think_of_luigi/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hlbozn",
    "title": "37-year-old career changer seeking advice: University degree vs self-taught path to Data Science",
    "selftext": "Background: I'm 37 and discovered data analytics through Google's Data Analytics certification last year. I've learned the basics of SQL, R, and Tableau, created several portfolio projects, and recently started learning Python. I find immense satisfaction in working with data tools and creating meaningful insights.\n\nCurrent situation:\n\n* Completed Google Data Analytics certification\n* Basic knowledge of SQL, R, and Tableau\n* Beginning to learn Python\n* Created several portfolio projects\n* Looking to transition into Data Science with remote work possibilities\n\nKey questions for the community:\n\n1. Given my background, would pursuing a formal degree (BS/MS in Data Science) be more valuable than continuing self-study?\n2. With current AI tools making coding more accessible and numerous online resources available, how important is formal education in today's data science landscape?\n3. Beyond Python, what core skills should I prioritize in my learning journey?\n4. For those who've successfully transitioned into the field: how did your educational background (formal vs self-taught) impact your job search?\n\nI'm prepared to fully commit to this career change and would greatly appreciate insights from experienced professionals, particularly those who've made similar transitions.\n\nThank you for your guidance!",
    "url": "https://www.reddit.com/r/data/comments/1hlbozn/37yearold_career_changer_seeking_advice/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1735039653.0,
    "author": "Nervous-Letter4588",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hlbozn/37yearold_career_changer_seeking_advice/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hl4tqm",
    "title": "Junior in highschool looking for data related projects at my internships. Any Ideas?",
    "selftext": "I'm a junior in highschool who has a internship at my school district specifically in HR. I've been interested in the data science felid for a while now and would like to major into it. My school requires us to do projects at our internship and I am lost on what to do that might show colleges I am interested in data science. I know minimal python and use chatgbt to code for me but I ask it to teach me along as it works. A potential project idea that I told my school I might do is gather data on how long it takes to do tedious tasks and then try to automate them, then once again collect data to see how much time I am saving them. But I am not sure how well this fits into the data science field. If anyone here can guide towards the right direction I would appreciate it.",
    "url": "https://www.reddit.com/r/data/comments/1hl4tqm/junior_in_highschool_looking_for_data_related/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1735011170.0,
    "author": "International_Boat14",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hl4tqm/junior_in_highschool_looking_for_data_related/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m3w7y4p",
        "body": "In my opinion, your project would better communicate your data engineering (DE) skills more than data science (DS). Automation and time saved tends to fall into the DE camp. \n\nDS tends to focus more on drawing insights from data that help drive business decisions forward. In an HR department, they are likely interested in things like: average time to resolve conflicts, overall employee satisfaction, utilization of employee benefits, the impact that employee benefits have on employee retention, churn, and/or hiring, the efficacy of DEI initiatives, etc. My recommendation would be to start by speaking to your HR department and identify two things: what are they being asked to focus on, and who is asking them to focus on that. Use that information to identify what might have the most impact, then identify what data you need to analyze that topic, and then identify what analysis you can run to provide insight. \n\nHope that helps!",
        "score": 1,
        "created_utc": 1735231319.0,
        "author": "therealtibblesnbits",
        "is_submitter": false,
        "parent_id": "t3_1hl4tqm",
        "depth": 0
      },
      {
        "id": "m467g7j",
        "body": " Check out pandemicoversight.gov. All the data on how US government spent $5 trillion.",
        "score": 1,
        "created_utc": 1735378852.0,
        "author": "skampnyc",
        "is_submitter": false,
        "parent_id": "t3_1hl4tqm",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1hkmliv",
    "title": "How is the community doing?",
    "selftext": "Just collecting input from the community. There is a decent amount of spam given the “recent” reawakening of the AI field.\n\nIt’s hard for me to read all the posts and more so to identify peoples personal projects from people marketing a SaaS.\n\n**Any other options/thoughts from the community? Ideas for improving the sub?**\n\nAnybody with significant Reddit experience interested in tackling the spam problem as a mod?",
    "url": "https://www.reddit.com/r/data/comments/1hkmliv/how_is_the_community_doing/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1734956409.0,
    "author": "heresacorrection",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hkmliv/how_is_the_community_doing/",
    "is_self": true,
    "distinguished": "moderator",
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hkhslz",
    "title": "What do you want turned into a fun data visualiser? ",
    "selftext": "Hi, I'm a visual designer and I just took a short course on turning data into visual graphs and infographics, and would love to practise what I learnt! Comment if you have some data you want to see turned into a visualiser! \n\nI'm fond of data related to nature, the climate, population, and cities, but am open to just about anything!",
    "url": "https://www.reddit.com/r/data/comments/1hkhslz/what_do_you_want_turned_into_a_fun_data_visualiser/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1734935010.0,
    "author": "shitisfertilizer",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hkhslz/what_do_you_want_turned_into_a_fun_data_visualiser/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hivdd6",
    "title": "Do you have a data recovery plan?",
    "selftext": "Hey everyone,\n\nIf you're part of your org's IT team, you know that unexpected accidents and disasters can hit when you least expect them (especially now in the holiday season). Losing sensitive data is expensive and damaging, both for the company and for anyone whose information gets compromised.\n\nHaving a solid data security strategy can help stop data loss before it even happens. However, a detailed disaster recovery plan can help limit the damage if something goes sideways. \n\nTo ensure you're prepared for any unexpected data breaches when forming your disaster recovery plan, we recommend the following:\n\n* Identify the biggest threats to your data and systems. Using threat research and mitigation solutions can help you identify those pesky risks and prevent unwanted data leaks. So you can focus on what matters without getting bogged down by false alarms.\n* Identify the data that contains the most sensitive information \n* Designate a disaster recovery team with clear roles and responsibilities. This ensures everyone knows what to do in the event of a crisis.\n* Establish how your team will communicate during a disaster. It's crucial to keep all stakeholders informed to avoid confusion.\n* Test your disaster recovery plan through drills. This practice ensures your team is ready to act when real issues occur.\n* Regularly review and update your strategies based on new technologies, threats, and changes within your organization. \n\nData breaches can occur at any moment, especially during peak seasons. By proactively implementing a robust data security strategy and a comprehensive disaster recovery plan, you can protect your organization and your customers.\n\nWhat measures are you taking in your organization to prepare for unexpected data loss? ",
    "url": "https://www.reddit.com/r/data/comments/1hivdd6/do_you_have_a_data_recovery_plan/",
    "score": 5,
    "upvote_ratio": 0.86,
    "num_comments": 0,
    "created_utc": 1734733311.0,
    "author": "Syncplify",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hivdd6/do_you_have_a_data_recovery_plan/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hj0dn3",
    "title": "Seeking income data by county in NYS",
    "selftext": "I'm shocked that I can not find any dataset of low income by county in NY. \n\nthis table- or some form of it is the closest thing I can find, but many counties are missing, and there are seemingly random groupings of 'sister cities.' Many locations are not represented on this sheet at all. Can anyone help me find a table that lists income in exactly this way, but including all the counties?\n\n[https://hcr.ny.gov/ahc-income-limits](https://hcr.ny.gov/ahc-income-limits)",
    "url": "https://www.reddit.com/r/data/comments/1hj0dn3/seeking_income_data_by_county_in_nys/",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 0,
    "created_utc": 1734748943.0,
    "author": "ChrissyChrissyPie",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hj0dn3/seeking_income_data_by_county_in_nys/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hipdvw",
    "title": "ONE CLASS SVM",
    "selftext": "What is the best way to encode my 3 categorical variables for OCSVM? I want to use target encoder but not sure how exactly as my train data is positive class only.Any ideas?",
    "url": "https://www.reddit.com/r/data/comments/1hipdvw/one_class_svm/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1734716971.0,
    "author": "Nice-Researcher-8694",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hipdvw/one_class_svm/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hh9elr",
    "title": "Tool to Identify and Group Misspelled Names",
    "selftext": "I am working with mortgage borrower names, seeking a tool to group and address misspellings efficiently.\n\nMy dataset includes 150,000 names, with some repeated 1-1,000 times. To manage this, I deduplicate the names in Excel, create a pivot table, and prioritize frequently repeated names by sorting them. This manual process addresses high-frequency names but takes significant time.\n\nAbout 50,000 names in my dataset are repeated only once, making manual review impractical as it would take about two months. However, skipping them entirely isn't an option because critical corporate borrower names could be missed. For instance, while \"John Properties LLC\" (repeated 15 times) has been corrected, a single instance of \"Johnn Properties LLC\" could still appear and harm data quality if overlooked.\n\nI am looking for a tool or method to identify and group similar names, particularly catching single occurrences of misspellings related to high-frequency names. Any recommendations would be appreciated.",
    "url": "https://www.reddit.com/r/data/comments/1hh9elr/tool_to_identify_and_group_misspelled_names/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1734549819.0,
    "author": "rehanali_007",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hh9elr/tool_to_identify_and_group_misspelled_names/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m2ra1uz",
        "body": "Try using fuzzy merge in Power Query which is probably the most accessible option. I did a quick search and with some trial and error, you could probably get to where you want. If you're comfortable with Python, I know for a fact that it's possible on there. ChatGPT could probably generate all the code for you.",
        "score": 1,
        "created_utc": 1734572987.0,
        "author": "SneakyTurtle2002",
        "is_submitter": false,
        "parent_id": "t3_1hh9elr",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1hgzkbm",
    "title": "How to grow faster in data science/ML jobs?",
    "selftext": "I am 24M, working as a remote data scientist. I have 2 yrs of IT exp and currently I am being paid 8LPA. I think this CTC is quite low for me based on my skills, but my company is reluctant on increasing my salary as they are fixed upon my experience level. What should I do, please advise :)",
    "url": "https://www.reddit.com/r/data/comments/1hgzkbm/how_to_grow_faster_in_data_scienceml_jobs/",
    "score": 4,
    "upvote_ratio": 0.75,
    "num_comments": 11,
    "created_utc": 1734521173.0,
    "author": "LongDefinition1910",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hgzkbm/how_to_grow_faster_in_data_scienceml_jobs/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m2nhv4s",
        "body": "What you need is a better offer on the table. So update your linkedin, cv, etc and go for it",
        "score": 3,
        "created_utc": 1734527125.0,
        "author": "fnxplayer",
        "is_submitter": false,
        "parent_id": "t3_1hgzkbm",
        "depth": 0
      },
      {
        "id": "m2n7igi",
        "body": "Data science is all about:\n* asking the right questions\n* anticipating follow up questions\n\nBy that I mean: it is about how well you understand the business you are working in, how do you translate business problems into code and how well do you explain technical results to non-technical audiences, including high executives.",
        "score": 1,
        "created_utc": 1734521985.0,
        "author": "Equivalent_Bench2081",
        "is_submitter": false,
        "parent_id": "t3_1hgzkbm",
        "depth": 0
      },
      {
        "id": "m2sueda",
        "body": "Right, I am on it. Just wanted to know if someone has a similar experience and if they did something different that I am not doing. Thanks a lot :)",
        "score": 1,
        "created_utc": 1734601775.0,
        "author": "LongDefinition1910",
        "is_submitter": true,
        "parent_id": "t1_m2nhv4s",
        "depth": 1
      },
      {
        "id": "m2nazmz",
        "body": "bro, please read the discription.",
        "score": 0,
        "created_utc": 1734523866.0,
        "author": "LongDefinition1910",
        "is_submitter": true,
        "parent_id": "t1_m2n7igi",
        "depth": 1
      },
      {
        "id": "m2tk9hp",
        "body": "I think another tip I can give you is that being a great DS, at the end of the day, is about delivering impact to the business. So what you can do is trying to justify your raise showing your manages how your work have impacted the company and if you think your work is not too impactful you could ask for more exposure on this sense.",
        "score": 1,
        "created_utc": 1734615680.0,
        "author": "fnxplayer",
        "is_submitter": false,
        "parent_id": "t1_m2sueda",
        "depth": 2
      },
      {
        "id": "m2njkvx",
        "body": "First, not a *”bro”…*\n\nNow, I did read what you wrote and I was pointing out that, given that you have technical expertise, you either \n lack business depth or communication skills.\n\nI have been a data scientist for 15 years now and because of my background in physics I quickly grasped the math and coding parts of it, but I am still polishing my communication",
        "score": 1,
        "created_utc": 1734527866.0,
        "author": "Equivalent_Bench2081",
        "is_submitter": false,
        "parent_id": "t1_m2nazmz",
        "depth": 2
      },
      {
        "id": "m32xjhi",
        "body": "This is good advice. Depending on your relationship with your manager, I'd be telling them: \"I'm not quite sure how my work benefits the company bottom line. I think I'd find my work more satisfying if I knew this. Could you show me how that happens?\"\n\nThis might give you some context. Caution: your manager might not have an answer to this so you might want to give them time to prepare for the question.",
        "score": 2,
        "created_utc": 1734749595.0,
        "author": "ThePluckyJester",
        "is_submitter": false,
        "parent_id": "t1_m2tk9hp",
        "depth": 3
      },
      {
        "id": "m2suajm",
        "body": "Grateful for your response. As per your suggestion, I should stay where I am and Improve my skills, right? \n\nMy question here is, I have the business skills too, and a strong math understanding for all the ML stuff. I want to know how can I improve my pay scale quickly based on your experience?",
        "score": 1,
        "created_utc": 1734601699.0,
        "author": "LongDefinition1910",
        "is_submitter": true,
        "parent_id": "t1_m2njkvx",
        "depth": 3
      },
      {
        "id": "m2ujt7o",
        "body": "I am not sure *staying* is the right answer, but let’s think for a moment. I can see 3 scenarios you are facing:\n* Salaries in DS/ML are low in your area\n* Your company pays bellow market\n* You are filling a position below your qualifications.\n\nIn the first case you can either change careers or find remote work from another part of the globe.\n\nIn the second case, unless the company has some serious benefits (like a powerful brand that will make your resume shine) best option is either leaving or unionizing - as a collective you and your colleagues will have better bargaining power.\n\nIn the last scenario it is all about understanding what are the gaps between where you are and a promotion, and if a promotion is not available… leave.\n\nMy only advice is… make sure your soft skills are on par with your technical skills .",
        "score": 1,
        "created_utc": 1734627803.0,
        "author": "Equivalent_Bench2081",
        "is_submitter": false,
        "parent_id": "t1_m2suajm",
        "depth": 4
      },
      {
        "id": "m307732",
        "body": "What kind of jobs/internships should I (24) be looking for with B.S in statistics, M.S. in data science and no experience at all? Plenty of interviews, but seems like no one wants to take the risk on someone with no experience which is understandable, but I’m having trouble getting my foot in the door for data/ML career. Too old for undergrad internships",
        "score": 1,
        "created_utc": 1734713726.0,
        "author": "-BonerSoup",
        "is_submitter": false,
        "parent_id": "t1_m2ujt7o",
        "depth": 5
      },
      {
        "id": "m33gjzf",
        "body": "There are the obvious tech things (Google, Meta, Amazon, Pinterest…) and startups\n\nThen, some industries you could be look into: *Pharmaceutical* and *Banking* because of statistics, *Telecommunications,* *Retail*, *Oil and Gas,* and *Mining* because of your MS in Data Science.\n\nIf you don't mind the lifestyle and travel, *consulting* might be an option - think Accenture, Deloitte, Bain, McKinsey.\n\nMost companies today do data science in some level",
        "score": 2,
        "created_utc": 1734758382.0,
        "author": "Equivalent_Bench2081",
        "is_submitter": false,
        "parent_id": "t1_m307732",
        "depth": 6
      }
    ],
    "comments_extracted": 11
  },
  {
    "id": "1hgzikc",
    "title": "What program would fit for my data?",
    "selftext": "Hey all,\n\nI'm working at a small company that measures various products for other companies, such as food and plants.\n\nWe aim to create a database that provides a comprehensive overview of all measurement data to identify significant changes in a particular company's products. While we've previously used Excel, we're exploring alternative options to streamline the process.\n\nSome products, like \"Granny Smith Apple,\" are used by multiple companies. We want to filter results to see specific data, such as average sugar content, pesticide levels, and more, for a particular company's \"Granny Smith Apple.\" And additionally if it has some outliers.\n\nIs there an easy-to-use, preferably free, app that can help us achieve this?",
    "url": "https://www.reddit.com/r/data/comments/1hgzikc/what_program_would_fit_for_my_data/",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1734520941.0,
    "author": "Rethjo",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hgzikc/what_program_would_fit_for_my_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m2n9638",
        "body": "You seem to be describing a database. I mean, you can keep running it in Excel, hundreds of companies do that way beyond the time for that transition. If you have Access in your company’s version of Office I’d start there. It’s a good stepping stone from Excel. It probably won’t scale well if your company grows by 10x but a year or two with it will help you work through many of the conversion issues I would expect.\n\nIf you decide to step all the way to a proper database, I’m not totally sure where to tell you to start. Every solution has its own leaning curve and that will be the hardest part for everyone. I would honestly pay for a consultant to help manage the transition, how you interact with your business will change. You’ll experience growing pains and on the job training will be hard on everyone. You’ll likely end up on AWS or something similar but, again, it’s own curve.",
        "score": 1,
        "created_utc": 1734522904.0,
        "author": "amosmj",
        "is_submitter": false,
        "parent_id": "t3_1hgzikc",
        "depth": 0
      },
      {
        "id": "m2nhpa2",
        "body": "Excel seems like the appropriate tool.",
        "score": 1,
        "created_utc": 1734527054.0,
        "author": "ItsSignalsJerry_",
        "is_submitter": false,
        "parent_id": "t3_1hgzikc",
        "depth": 0
      },
      {
        "id": "m562nbl",
        "body": "[removed]",
        "score": 1,
        "created_utc": 1735904448.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_1hgzikc",
        "depth": 0
      },
      {
        "id": "m5c8gja",
        "body": "Thanks! I already found a way using Excel web app, wasn't as difficult as I expected tbh 😭😂",
        "score": 1,
        "created_utc": 1735990122.0,
        "author": "Rethjo",
        "is_submitter": true,
        "parent_id": "t1_m562nbl",
        "depth": 1
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "1hgy89n",
    "title": "Data requirement - Set of all related Banking/Insurance Laws documents",
    "selftext": "Hey everyone. I’m working on RAG search tools - particularly in the banking and insurance domains. I would like to build a use case around searches in the banking/ insurance domains related to the government rules/laws/regulations.\n\nFor this, I’m searching for documents that have the above mentioned details (open source). And when I say documents, I’m referring to inter related documents like amendments or laws of different categories etc.\nBut for a start, even a single document related to these laws would do.\n\nAny help would be appreciated.",
    "url": "https://www.reddit.com/r/data/comments/1hgy89n/data_requirement_set_of_all_related/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1734515076.0,
    "author": "Longjumping_Job_4451",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hgy89n/data_requirement_set_of_all_related/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m2n5cwa",
        "body": "In what place do you want to do this? Insurance/bank laws are not the same depending on the country I think..",
        "score": 1,
        "created_utc": 1734520714.0,
        "author": "Illustrious-Fan4485",
        "is_submitter": false,
        "parent_id": "t3_1hgy89n",
        "depth": 0
      },
      {
        "id": "m2n5fcv",
        "body": "Preferably United States!",
        "score": 1,
        "created_utc": 1734520756.0,
        "author": "Longjumping_Job_4451",
        "is_submitter": true,
        "parent_id": "t1_m2n5cwa",
        "depth": 1
      },
      {
        "id": "m2n5khh",
        "body": "Don't know much about US but maybe first have a look to the gov website ? In Europe we have open data on public website, and the gov website may help you about the differents laws ?",
        "score": 1,
        "created_utc": 1734520843.0,
        "author": "Illustrious-Fan4485",
        "is_submitter": false,
        "parent_id": "t1_m2n5fcv",
        "depth": 2
      },
      {
        "id": "m2n5r74",
        "body": "Understood, I’ll follow that approach first. I did try that with US but it was not very helpful.\nBut thank you :)",
        "score": 1,
        "created_utc": 1734520953.0,
        "author": "Longjumping_Job_4451",
        "is_submitter": true,
        "parent_id": "t1_m2n5khh",
        "depth": 3
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "1hg7znp",
    "title": " I built an end-to-end data pipeline tool in Go called Bruin ",
    "selftext": "Hi all, I have been pretty frustrated with how I had to bring together bunch of different tools together, so I built a CLI tool that brings together data ingestion, data transformation using SQL and Python and data quality in a single tool called Bruin:\n\n[https://github.com/bruin-data/bruin](https://github.com/bruin-data/bruin)\n\nBruin is written in Golang, and has quite a few features that makes it a daily driver:\n\n* it can ingest data from many different sources using [ingestr](https://github.com/bruin-data/ingestr)\n* it can run SQL & Python transformations with built-in materialization & Jinja templating\n* it runs Python fully locally using the amazing [uv](https://github.com/astral-sh/uv), setting up isolated environments locally, mix and match Python versions even within the same pipeline\n* it can run data quality checks against the data assets\n* it has an open-source [VS Code extension](https://bruin-data.github.io/bruin/vscode-extension/overview.html) that can do things like syntax highlighting, lineage, and more.\n\nWe had a small pool of beta testers for quite some time and I am really excited to launch Bruin CLI to the rest of the world and get feedback from you all. I know it is not often to build data tooling in Go but I believe we found ourselves in a nice spot in terms of features, speed, and stability.\n\nLooking forward to hearing your feedback!\n\n[https://github.com/bruin-data/bruin](https://github.com/bruin-data/bruin)",
    "url": "https://www.reddit.com/r/data/comments/1hg7znp/i_built_an_endtoend_data_pipeline_tool_in_go/",
    "score": 6,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1734432166.0,
    "author": "karakanb",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hg7znp/i_built_an_endtoend_data_pipeline_tool_in_go/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hfs0r3",
    "title": "Need advice from experienced data scientists and/or analysts, please thanks in advance",
    "selftext": "Hi everyone, I’m considering a career pivot into the data field and would love your advice! I'm brazilian and hold a degree in Forest Engineering, with a short course in Project Management. Since graduating, I've worked in two multinational pulp and paper companies here in Brazil, always in sustainability-related positions. My background includes managing projects that involved analysis, reporting, and stakeholder collaboration, and I’m hoping to leverage these skills to land a remote data-focused role. Here’s a bit about my experience:\n\n* **Data-Driven Decision Making:** I’ve managed projects in corporate sustainability where tracking ESG metrics and analysing data was key to evaluating progress and making strategic decisions.\n* **Reporting & Visualisation:** I’ve prepared detailed reports for technical and executive audiences, turning complex data into actionable insights.\n* **Stakeholder Engagement:** I’ve worked closely with diverse stakeholders to gather requirements, align priorities, and communicate findings—skills that seem critical in data-related roles.\n* **Process Optimisation:** I’ve applied LSS methodologies to improve workflows and ensure efficiency, often relying on data analysis to identify bottlenecks and measure impact.\n* **Problem-Solving Mindset:** Whether working with traditional communities or optimising business processes, I’ve always approached challenges with curiosity and a focus on finding scalable solutions.\n\nHere’s some of the topics I've been thinking about:\n\n1. How can I position my existing skills and experience to break into a data-related career?\n2. Are there specific certifications, courses, or tools you’d recommend to build a strong foundation for data analytics or data science?\n3. How can I build a portfolio or demonstrate my skills to potential employers if I’m transitioning from another field?\n4. Any advice for networking and finding remote data-focused opportunities or networking in the field?\n\nThank you so much for your time and insights. ",
    "url": "https://www.reddit.com/r/data/comments/1hfs0r3/need_advice_from_experienced_data_scientists/",
    "score": 5,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1734379263.0,
    "author": null,
    "subreddit": "data",
    "permalink": "/r/data/comments/1hfs0r3/need_advice_from_experienced_data_scientists/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m2e5gvq",
        "body": "As a data analyst, you're expected to be more technical. As a business analyst, you have to be somewhat technical (Excel and PowerBI are bare minimum) but know about the business side. To pivot into data analysis now, I would suggest highlighting your technical expertise which would include all the software you've used, what programming languages you know (SQL and Python suggested), and how you helped your previous companies from a technical standpoint.\n\nWhat I would suggest, though, is look for remote Business Analyst roles in sustainability. That's a better fit for you and would allow you to work remotely. Your main selling point is the fact that you have business experience in sustainability and probably know about the different regulation surrounding it. It would be easier to piggyback off that business experience, slowly start learning the technical side, then switch fully to a data analyst. That would be a lot easier to do than convince somebody you'd be a good data analyst just based off your current, very limited, technical skills.",
        "score": 2,
        "created_utc": 1734384314.0,
        "author": "SneakyTurtle2002",
        "is_submitter": false,
        "parent_id": "t3_1hfs0r3",
        "depth": 0
      },
      {
        "id": "m2gz0xu",
        "body": "You should emphasize your strong background in data-driven decision making, reporting, and stakeholder engagement from your experience in sustainability. Consider taking comprehensive courses in data science essentials like Python, SQL, and machine learning from platforms like Coursera or DataCamp. Using StrataScratch and Kaggle to build a portfolio of data projects, particularly those related to sustainability metrics, will demonstrate your capabilities. Host these projects on GitHub and write about your insights on LinkedIn to showcase your ability to translate complex data into actionable insights. Additionally, engage in data science communities online and attend virtual meetups to network and discover remote job opportunities. Tailoring your applications to highlight how your sustainability background offers a unique perspective can also set you apart in the job market.",
        "score": 2,
        "created_utc": 1734427851.0,
        "author": "msn018",
        "is_submitter": false,
        "parent_id": "t3_1hfs0r3",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1hfgscd",
    "title": "Multi-sources rich social media dataset - a full month of global chatters!",
    "selftext": "Hey, data enthusiasts and web scraping aficionados!   \nWe’re thrilled to share a massive new social media dataset that just dropped on Hugging Face! 🚀\n\n# Access the Data:\n\n# [👉Exorde Social Media One Month 2024](https://huggingface.co/datasets/Exorde/exorde-social-media-one-month-2024)\n\n# What’s Inside?\n\n* **Scale**: 270 million posts collected over one month (Nov 14 - Dec 13, 2024)\n* **Methodology**: Total sampling of the web, statistical capture of **all topics**\n* **Sources**: 6000+ platforms including Reddit, Twitter, BlueSky, YouTube, Mastodon, Lemmy, and more\n* **Rich Annotations**: Original text, metadata, emotions, sentiment, top keywords, and themes\n* **Multi-language**: Covers 122 languages with translated keywords\n* **Unique features:** English top keywords, allowing super-quick statistics, trends/time series analytics!\n* **Source**: At Exorde Labs, we are processing \\~4 billion posts per year, or 10-12 million every 24 hrs.\n\n# Why This Dataset Rocks\n\nThis is a goldmine for:\n\n* Trend analysis across platforms\n* Sentiment/emotion research (algo trading, OSINT, disinfo detection)\n* NLP at scale (language models, embeddings, clustering)\n* Studying information spread & cross-platform discourse\n* Detecting emerging memes/topics\n* Building ML models for text classification\n\nWhether you're a startup, data scientist, ML engineer, or just a curious dev, this dataset has something for everyone. It's perfect for both serious research and fun side projects.  Do you have questions or cool ideas for using the data? Drop them below. \n\nWe’re processing over 300 million items monthly at Exorde Labs—and we’re excited to support open research with this Xmas gift 🎁. Let us know your ideas or questions below—let’s build something awesome together!\n\nHappy data crunching!\n\n*Exorde Labs Team - A unique network of smart nodes collecting data like never before*",
    "url": "https://www.reddit.com/r/data/comments/1hfgscd/multisources_rich_social_media_dataset_a_full/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1734347318.0,
    "author": "Exorde_Mathias",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hfgscd/multisources_rich_social_media_dataset_a_full/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1heyzf0",
    "title": "DP-900 Exam question",
    "selftext": "Hi everyone,  \n\nI’m currently a freshman at Texas A&M University pursuing a degree in Management Information Systems (MIS).\n\nWhile researching SQL certifications to enhance my technical skills, I noticed the Microsoft Azure DP-900 exam kept coming up. My question is: Is the DP-900 exam worth taking, and how will it be perceived by future employers in the tech and business sectors?\n\nI’d love to hear your insights on whether this certification adds value to my resume or if I should focus on other certifications more aligned with SQL or MIS.  \n\nThanks in advance for your advice!",
    "url": "https://www.reddit.com/r/data/comments/1heyzf0/dp900_exam_question/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1734287630.0,
    "author": "Time-Cattle7590",
    "subreddit": "data",
    "permalink": "/r/data/comments/1heyzf0/dp900_exam_question/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m2axwfy",
        "body": "I wouldn't take it unless you also have Azure experience to back it up. It's not the kind of exam you want to take when you have no Azure experience.",
        "score": 1,
        "created_utc": 1734338949.0,
        "author": "goatsnboots",
        "is_submitter": false,
        "parent_id": "t3_1heyzf0",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1hfbiep",
    "title": "Kkkkkk",
    "selftext": "",
    "url": "https://www.reddit.com/r/data/comments/1hfbiep/kkkkkk/",
    "score": 0,
    "upvote_ratio": 0.2,
    "num_comments": 2,
    "created_utc": 1734324603.0,
    "author": "Middle_Television731",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hfbiep/kkkkkk/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": true,
    "spoiler": true,
    "locked": false,
    "comments": [
      {
        "id": "m2abef6",
        "body": "r/lostredditors",
        "score": 3,
        "created_utc": 1734325469.0,
        "author": "Key-Hyena5292",
        "is_submitter": false,
        "parent_id": "t3_1hfbiep",
        "depth": 0
      },
      {
        "id": "m2ayhjh",
        "body": "If you use RLE you can store that as 1K5k.",
        "score": 2,
        "created_utc": 1734339361.0,
        "author": "lgastako",
        "is_submitter": false,
        "parent_id": "t3_1hfbiep",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1hegoes",
    "title": "How can i find internships.",
    "selftext": "I am not an experienced data analyst or data scientist, but nor am I a complete neophyte, meaning I have a small portfolio of data projects that I have done. I am looking for an internship where I can learn and make connections into the data world. \n\nThe rub is, that I am currently working full time (as a teacher) and can only devote about 4-8 hours a week well outside of business hours. \n\nIt does not matter much, whether I am paid or not for this internship but it is important that i learn and make connections. \n\nAre there any ideas where i can find such opportunities?",
    "url": "https://www.reddit.com/r/data/comments/1hegoes/how_can_i_find_internships/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 5,
    "created_utc": 1734222499.0,
    "author": "heisenberger",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hegoes/how_can_i_find_internships/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m23inx5",
        "body": "Sounds like you should continue to work on larger more complex projects while trying to do some freelancing. Upwork or fiver are a great place to at least try to get connected with potential customers.\nInternships generally speaking would want you to work full time. Sounds like you are not willing to gamble and quit your teaching job for the internship. \nI’m joining some nerd club at college for the networking opportunities and getting involved with career center. I was told to be active on LinkedIn in maybe start reaching out to recruiters.\nGood luck",
        "score": 2,
        "created_utc": 1734222975.0,
        "author": "CompetitiveLeader965",
        "is_submitter": false,
        "parent_id": "t3_1hegoes",
        "depth": 0
      },
      {
        "id": "m2mrbze",
        "body": "bro is trying to find an internship in this economy. bro is cooked",
        "score": 1,
        "created_utc": 1734511336.0,
        "author": "Tabibyte",
        "is_submitter": false,
        "parent_id": "t3_1hegoes",
        "depth": 0
      },
      {
        "id": "m2524fy",
        "body": "thank you. \n\nI am most assuredly not willing to gamble on a chance on a job/career that is notoriously difficult to even find a job.",
        "score": 1,
        "created_utc": 1734249359.0,
        "author": "heisenberger",
        "is_submitter": true,
        "parent_id": "t1_m23inx5",
        "depth": 1
      },
      {
        "id": "m258beh",
        "body": "There’s a saying where I’m from. \nThose who don’t take the risk don’t drink Champagne.",
        "score": 1,
        "created_utc": 1734253538.0,
        "author": "CompetitiveLeader965",
        "is_submitter": false,
        "parent_id": "t1_m2524fy",
        "depth": 2
      },
      {
        "id": "m2757up",
        "body": "yeah, but i don't have the money or support system to potentially be unemployed.",
        "score": 1,
        "created_utc": 1734285239.0,
        "author": "heisenberger",
        "is_submitter": true,
        "parent_id": "t1_m258beh",
        "depth": 3
      }
    ],
    "comments_extracted": 5
  },
  {
    "id": "1hdyqpn",
    "title": "I am sharing Data Science courses and projects on YouTube",
    "selftext": "Hello, I wanted to share that I am sharing free courses and projects on my YouTube Channel. I have more than 200 videos and I created playlists for learning Data Science. I am leaving the playlist link below, have a great day!\n\nData Science Full Courses & Projects -> [https://youtube.com/playlist?list=PLTsu3dft3CWiow7L7WrCd27ohlra\\_5PGH&si=6WUpVwXeAKEs4tB6](https://youtube.com/playlist?list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&si=6WUpVwXeAKEs4tB6)\n\nData Science Projects -> [https://youtube.com/playlist?list=PLTsu3dft3CWg69zbIVUQtFSRx\\_UV80OOg&si=go3wxM\\_ktGIkVdcP](https://youtube.com/playlist?list=PLTsu3dft3CWg69zbIVUQtFSRx_UV80OOg&si=go3wxM_ktGIkVdcP)",
    "url": "https://www.reddit.com/r/data/comments/1hdyqpn/i_am_sharing_data_science_courses_and_projects_on/",
    "score": 8,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1734165667.0,
    "author": "onurbaltaci",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hdyqpn/i_am_sharing_data_science_courses_and_projects_on/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m20tlwa",
        "body": "Love your content! More projects with time series data please!",
        "score": 1,
        "created_utc": 1734188401.0,
        "author": "khaili109",
        "is_submitter": false,
        "parent_id": "t3_1hdyqpn",
        "depth": 0
      },
      {
        "id": "m27y222",
        "body": "Thank you for your time and knowledge mate",
        "score": 1,
        "created_utc": 1734294325.0,
        "author": "uknow_Slayer",
        "is_submitter": false,
        "parent_id": "t3_1hdyqpn",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1hdx800",
    "title": "Advice about a new career as Data Analyst",
    "selftext": "Hi, I'm currently a decision engine analyst my main mansion is the automation of credit risk policy and i like that pretty much. But, In the last year, my boss wanted me to be a data analyst and to share my analysis , to find features linked to customer behaviour and to predict the next performance of the portfoglio deterioration. It's hard for me to start, to speak in front of people and the board. how can i start ? which analysis i have to do and which tools are necessary ?\n\nPS: I use SPSS modeler, Qlikview, EXcel...\n\nCan you give me an advice to start my new path ? Thanks",
    "url": "https://www.reddit.com/r/data/comments/1hdx800/advice_about_a_new_career_as_data_analyst/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1734159150.0,
    "author": "InevitableWay7648",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hdx800/advice_about_a_new_career_as_data_analyst/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m26oir0",
        "body": "Python for the data analysis and visualizations.",
        "score": 1,
        "created_utc": 1734279718.0,
        "author": "Library_Spidey",
        "is_submitter": false,
        "parent_id": "t3_1hdx800",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1hday9o",
    "title": "Multi-lingual multi-source social media dataset - a full week",
    "selftext": "Hey fellow datasets enthusiasts!\n\nWe're excited to announce the release of a new, large-scale social media dataset from Exorde Labs. We've developed a robust public data collection engine that's been quietly amassing an impressive dataset via a distributed network.\n\n**The Origin Dataset**\n\n* **Scale**: Over 1 billion data points, with 10 million added daily (3.5-4 billion per year at our current rate)\n* **Sources**: 6000+ diverse public social media platforms (X, Reddit, BlueSky, YouTube, Mastodon, Lemmy, TradingView, bitcointalk, jeuxvideo dot com, etc.)\n* **Collection**: Near real-time capture since August 2023, at a growing scale.\n* **Rich Annotations**: Includes original text, metadata (URL, Author Hash, date) emotions, sentiment, top keywords, and theme\n\n**Sample Dataset Now Available**\n\nWe're releasing a 1-week sample from December 1-7th, 2024, containing 65,542,211 entries.\n\n**Key Features**:\n\n* Multi-source and multi-language (122 languages)\n* High-resolution temporal data (exact posting timestamps)\n* Comprehensive metadata (sentiment, emotions, themes)\n* Privacy-conscious (author names hashed)\n\n**Use Cases**: Ideal for trend analysis, cross-platform research, sentiment analysis, emotion detection, and more, financial prediction, hate speech analysis, OSINT, etc.\n\n**This dataset includes many conversations around the period of CyberMonday, Syria regime collapse and UnitedHealth CEO killing & many more topics. The potential seems large.**\n\n**Access the Dataset**: [https://huggingface.co/datasets/Exorde/exorde-social-media-december-2024-week1](https://huggingface.co/datasets/Exorde/exorde-social-media-december-2024-week1)\n\nA larger dataset of \\~1 month will be available next week, over the period: November 14th 2024 - December 13th 2024.\n\nFeel free to ask any questions.\n\n**We hope you appreciate this Xmas Data gift.**\n\n*Exorde Labs*",
    "url": "https://www.reddit.com/r/data/comments/1hday9o/multilingual_multisource_social_media_dataset_a/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1734092188.0,
    "author": "Exorde_Mathias",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hday9o/multilingual_multisource_social_media_dataset_a/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hd6n6d",
    "title": "Web of Data",
    "selftext": "",
    "url": "https://chrisperkins505.medium.com/4e4272c7031c?source=friends_link&sk=b3865c06f0f96b7a69bdd85c4b29a91f",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1734072797.0,
    "author": "cps001",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hd6n6d/web_of_data/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1hcntho",
    "title": "Mapping Service",
    "selftext": "I’m having trouble coming up with a solution and would love a nudge in the right direction.\n\nI manage a home health service where we employee 40 nurses and have about one thousand patients across the state.\n\nI’m trying to find/create a tool to ensure that patients are being seen by nurses that live geographically close to them to limit unnecessary drive time.\n\nOur nurses case manage so they are seeing the same patients longer term. So I have a lot of active patients to untangle.\n\nThanks!!\n",
    "url": "https://www.reddit.com/r/data/comments/1hcntho/mapping_service/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1734018067.0,
    "author": "Jumpy_Ad4564",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hcntho/mapping_service/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m2e939r",
        "body": "I could do that for you for free. I'm a data analyst/automation engineer but have a side business, so it would be great to work with a client for the first time in the US. Actually worked on something similar for a pool-maintenance company 2-3 years ago. Only catch is I would need you to be a reference for future prospects within the United States (I mostly work with companies in Canada at the moment). \n\nRegardless, here's what I need if you're interested:\n\n\\- What's the current process for dispatching? (how do you currently decide which nurse goes where)\n\n\\- Any other software involved within the process and does it offer an API?\n\nWhen previously automating for some big companies, I've learned it's best to first fully map the original process, then make the new one, and then come up with a plan on how to seamlessly migrate without downtime. \n\nWhat I'm currently thinking is get a list of nurses, and a list of addresses populated everyday (based on appointments), use the Google API to get the coordinates, then create groups of locations in an order that optimizes driving times, then randomly assign the groups to nurses (assuming they all come to a central location before going to their jobs for the day). This should pretty much accomplish what you want to do. I can also give you a front-end (probably a Google Sheet to make it easy and fast), where you can re-assign as things come up during the day. \n\nShoot me a DM",
        "score": 1,
        "created_utc": 1734385460.0,
        "author": "SneakyTurtle2002",
        "is_submitter": false,
        "parent_id": "t3_1hcntho",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1hcgrwv",
    "title": "Need advice from experienced data scientists and/or analysts",
    "selftext": "I'm 32 y/o bartender with 16 month old son. SE bootcamp grad with intermediate web development skills. Couldn't get a job with them (can't say I tried very hard). Decided to get a degree from University City of San Diego (top 12-13 CS and DS schools in the country). Currently in 3rd semester of community college taking Cacl, Data and algorithms classes with other bs classes. I was going for CS degree but lately I've been considering committing to DS. Here's my questions.\nI'm really f**** tired of bartending. How realistic is it for me to become a data analyst between now and my graduation? I've been doing a lot of reading about similarities between DA and DS. DS obviously more technical and requires advanced knowledge of statistics etc... which is why most employers prefer college grad. DA on the other hand hires anyone with irrelevant degree as long as the have the skills. \nDo you think it's better to study and try to find internship opportunities as DS or just go for the DA job. Which way will have a better outcome in your opinion?",
    "url": "https://www.reddit.com/r/data/comments/1hcgrwv/need_advice_from_experienced_data_scientists/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1733992650.0,
    "author": "CompetitiveLeader965",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hcgrwv/need_advice_from_experienced_data_scientists/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m1o2ect",
        "body": "Try to get a job as a data analyst. It's very common to go from data analysis to data science, so if you want to do that down the line, it's still possible. I'd learn a BI tool first and do a project with it so you have something to show employers.\n\n\nEdit: and learn SQL too.",
        "score": 2,
        "created_utc": 1733995059.0,
        "author": "goatsnboots",
        "is_submitter": false,
        "parent_id": "t3_1hcgrwv",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1hbz0m0",
    "title": "How to prepare for data engineering. ",
    "selftext": "Hi everyone, I’m currently working as a Data Analyst but looking to transition into a Data Engineer role. I’ve set a goal of 6 months to prepare and start applying for interviews. However, I’m feeling a bit unsure about where to begin.  \n\nIf anyone could share a preparation roadmap, it would be incredibly helpful. I’d also appreciate recommendations for free resources or any paid resources that are worth the investment. Thank you in advance for your guidance and support!",
    "url": "https://www.reddit.com/r/data/comments/1hbz0m0/how_to_prepare_for_data_engineering/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1733938543.0,
    "author": "Mony_10",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hbz0m0/how_to_prepare_for_data_engineering/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m2e9i4m",
        "body": "In the same boat. You can't really make projects for it because it's very much dependent on the business or software logic. Not a lot of good tutorials out there either that speak on how all of it looks in a practical setting.",
        "score": 1,
        "created_utc": 1734385591.0,
        "author": "SneakyTurtle2002",
        "is_submitter": false,
        "parent_id": "t3_1hbz0m0",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1hadrfs",
    "title": "FDH commands in R| DEA",
    "selftext": "Hi I am unable to call fdh() or fdh\\_efficiency() function in R, despite having installed all the relevnt packages like benchmarking, lpsolve. can someone please help?",
    "url": "https://www.reddit.com/r/data/comments/1hadrfs/fdh_commands_in_r_dea/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 0,
    "created_utc": 1733762024.0,
    "author": "Tight-Credit4319",
    "subreddit": "data",
    "permalink": "/r/data/comments/1hadrfs/fdh_commands_in_r_dea/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ha7cuk",
    "title": "data",
    "selftext": "i wanna get turkish gambling sites datas how can i reach them? pls inform me.\n",
    "url": "https://www.reddit.com/r/data/comments/1ha7cuk/data/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1733742465.0,
    "author": "Responsible_Goal_712",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ha7cuk/data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ha6b3t",
    "title": "How can I found datas on telegram",
    "selftext": "I wanna buy Turkish Gambling site datas.It was on breachforum but It is closed.Can somebody help me please",
    "url": "https://www.reddit.com/r/data/comments/1ha6b3t/how_can_i_found_datas_on_telegram/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1733737773.0,
    "author": "dimetilpropan",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ha6b3t/how_can_i_found_datas_on_telegram/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m1kgtb4",
        "body": "What kind of data do you need, and what amount of data are you looking for?",
        "score": 1,
        "created_utc": 1733943745.0,
        "author": "Academic_Thanks9425",
        "is_submitter": false,
        "parent_id": "t3_1ha6b3t",
        "depth": 0
      },
      {
        "id": "m5vo44p",
        "body": "contact me on tg dimetiltrp is my nick",
        "score": 1,
        "created_utc": 1736261813.0,
        "author": "dimetilpropan",
        "is_submitter": true,
        "parent_id": "t1_m1kgtb4",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1h9cwwz",
    "title": "Career Advice ",
    "selftext": "I build a robotics startup for 2 years. Dropped early this year because things weren't going in right direction. Last 7 months doing marketing for a Travel company. Now, I want to switch my career to data related field and have been learning PowerBi.\nAny advise where and what to start.",
    "url": "https://www.reddit.com/r/data/comments/1h9cwwz/career_advice/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1733639957.0,
    "author": "ArduousSma",
    "subreddit": "data",
    "permalink": "/r/data/comments/1h9cwwz/career_advice/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1h8d279",
    "title": "Senior Data Scientist paths",
    "selftext": "Currently a senior data scientist and potentially have the opportunity to move into a more business facing role of senior manager in a revenue management team focusing more on business analytics and enacting strategies quickly. Would this be a move that most sane people consider? Or would this be seen as a potential downgrade? What key factors would be good to consider as to reasoning to want to venture more into the business side of things as opposed to a more technical role of data scientist?",
    "url": "https://www.reddit.com/r/data/comments/1h8d279/senior_data_scientist_paths/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1733522651.0,
    "author": "Hairy_Warthog_1151",
    "subreddit": "data",
    "permalink": "/r/data/comments/1h8d279/senior_data_scientist_paths/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1h825kf",
    "title": "how i get crypto leads ???",
    "selftext": "Hello, my friend and I have started a small business for crypto traders. Now, we need some leads to contact them and determine if they are potential clients.",
    "url": "https://www.reddit.com/r/data/comments/1h825kf/how_i_get_crypto_leads/",
    "score": 0,
    "upvote_ratio": 0.25,
    "num_comments": 1,
    "created_utc": 1733494135.0,
    "author": "Gamir9",
    "subreddit": "data",
    "permalink": "/r/data/comments/1h825kf/how_i_get_crypto_leads/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m6hj8s9",
        "body": "Dm",
        "score": 1,
        "created_utc": 1736550487.0,
        "author": "Chakra7767",
        "is_submitter": false,
        "parent_id": "t3_1h825kf",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1h7n6pn",
    "title": "How's Msc management and Data analytics Postgraduate course in BPP University",
    "selftext": "Hello guys, anyone has been applied data analytics postgraduate course provide by BPP university? I'm an accounting practitioner and I feel data analytics tools like Alteryx, SPSS, SQL, and Tableau were extremely useful at work, anyone has ever graduate from that major?",
    "url": "https://www.reddit.com/r/data/comments/1h7n6pn/hows_msc_management_and_data_analytics/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1733441473.0,
    "author": "Level-Complex-9737",
    "subreddit": "data",
    "permalink": "/r/data/comments/1h7n6pn/hows_msc_management_and_data_analytics/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1h6qfbx",
    "title": "AI Agent Knowledge Base",
    "selftext": "Exploring the idea of building an API platform for knowledge bases — essentially a tool that allows companies to connect, query, and manage data from multiple sources.\n\nDoes anyone know of existing solutions in this space? I'd love to hear from folks working on similar problems or who have thoughts or insight here.",
    "url": "https://www.reddit.com/r/data/comments/1h6qfbx/ai_agent_knowledge_base/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1733344510.0,
    "author": "james_dub443",
    "subreddit": "data",
    "permalink": "/r/data/comments/1h6qfbx/ai_agent_knowledge_base/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1h6vjsb",
    "title": "Does your organization give any awareness of cyber threats to your employees?",
    "selftext": "While companies invest heavily in advanced technologies and systems to protect data, the human factor remains one of the most significant vulnerabilities in cybersecurity. Cybercriminals use human factors to get unauthorized access, steal information, and infect systems with malware. Even the best technology doesn’t help if the people are not educated, engaged, and empowered to recognize and respond to security threats.\n\nHere are some of the common human-caused cybersecurity breaches:\n\nPHISHING ATTACKS\n\nThis cyber attack typically involves deceptive emails, text messages, or websites that trick individuals into divulging sensitive information such as credit card numbers and passwords.\n\nSOCIAL ENGINEERING\n\nCybercriminals often use psychological manipulation techniques to trick individuals into actions that compromise security. Social engineering attacks target human emotions, exploiting trust, curiosity, fear, or the desire to help others.\n\nWEAK PASSWORD PRACTICES\n\nPasswords are a major weak point in cybersecurity, with many individuals using easy passwords, reusing them, or neglecting multifactor authentication.\n\nPOOR SOFTWARE MANAGEMENT\n\nUnregular software updates cause 60% of data breaches. Optimizing these processes should be a priority for all organizations.\n\nINSIDER THREATS\n\nThe 2023 Insider Threat Report by Ponemon Institute found a 44% increase in insider threats over the past two years, with the average incident costing $15.38 million.\n\nCompared to experienced cybersecurity specialists trained to anticipate risks, the average employee with a lack of awareness may overlook the signs of a potential cyberattack. Studies show that 82% of organizations have experienced a cyber attack due to human error in the past three years. \n\nOrganizations are now starting to understand the need for comprehensive training programs that focus not just on technology but also on awareness and cultivation of positive security behaviors. Teaching employees about the latest threats, instilling a culture of security, and encouraging open communication about potential risks are critical steps in safeguarding sensitive data.\n\nWe all need to remember that cybersecurity is not just about technology - it’s about people. By understanding and mitigating the human factors contributing to the knowledge gap in cybersecurity, organizations can better protect themselves against the ever-present threat of cyberattacks.\n\nDoes your organization give any awareness of cyber threats to your employees? Please share your experience. ",
    "url": "https://www.reddit.com/r/data/comments/1h6vjsb/does_your_organization_give_any_awareness_of/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 3,
    "created_utc": 1733357450.0,
    "author": "Syncplify",
    "subreddit": "data",
    "permalink": "/r/data/comments/1h6vjsb/does_your_organization_give_any_awareness_of/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m0gizqf",
        "body": "this is spam!",
        "score": 2,
        "created_utc": 1733357584.0,
        "author": "coolrivers",
        "is_submitter": false,
        "parent_id": "t3_1h6vjsb",
        "depth": 0
      },
      {
        "id": "m0gkmg8",
        "body": "We are not spam:) We are just curious about how companies approach data security training. Would love to hear your experience as well.",
        "score": 0,
        "created_utc": 1733358168.0,
        "author": "Syncplify",
        "is_submitter": true,
        "parent_id": "t1_m0gizqf",
        "depth": 1
      },
      {
        "id": "m0gldhj",
        "body": "reposting a generic article is spam",
        "score": 2,
        "created_utc": 1733358434.0,
        "author": "coolrivers",
        "is_submitter": false,
        "parent_id": "t1_m0gkmg8",
        "depth": 2
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1h6v2xb",
    "title": "How do I install an IPA file on iOS into an app?",
    "selftext": "",
    "url": "https://www.reddit.com/r/data/comments/1h6v2xb/how_do_i_install_an_ipa_file_on_ios_into_an_app/",
    "score": 1,
    "upvote_ratio": 0.99,
    "num_comments": 0,
    "created_utc": 1733356208.0,
    "author": "Olesmitgamer13",
    "subreddit": "data",
    "permalink": "/r/data/comments/1h6v2xb/how_do_i_install_an_ipa_file_on_ios_into_an_app/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1h5kaxt",
    "title": "Need advice on data integration, common repository so as to build dashboard on Powerbi",
    "selftext": "I'm a data analyst and have stepped into freelancing space recently. I work on sql, python and BI.\n\nI need to work on a project that has the below data sources.\n\n* Magento\n* Insider\n* Google Analytics\n* Adjust\n\nI'm new to data integration subject. Would like to know what are the different methods/tools which can help me with the integration from these data sources to a common repository with a daily refresh frequency.  \n\n\nKindly also suggest which type of repository from AWS vs Azure vs GCP will suite the best \\[ with pricings \\].",
    "url": "https://www.reddit.com/r/data/comments/1h5kaxt/need_advice_on_data_integration_common_repository/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1733221021.0,
    "author": "Nervous-Battle-3107",
    "subreddit": "data",
    "permalink": "/r/data/comments/1h5kaxt/need_advice_on_data_integration_common_repository/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1h5dwcv",
    "title": "Is there a list of countries' lithographic printing capability sorted by node size?",
    "selftext": "Once you exclude Taiwan and China, it's hard to find stats on where different countries are. It's all disconnected news stories ",
    "url": "https://www.reddit.com/r/data/comments/1h5dwcv/is_there_a_list_of_countries_lithographic/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1733195793.0,
    "author": "JandalfTheJrey",
    "subreddit": "data",
    "permalink": "/r/data/comments/1h5dwcv/is_there_a_list_of_countries_lithographic/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1h530nb",
    "title": "How to Get Free Automotive data on honda civic models sold in 2015, with make,model,price and mileage",
    "selftext": "i am trying to compile a dataset for all honda civic models of 2015, make,model,price,MSRP, and mileages they were sold in the year 2015, for me to be able to see , how the prices were sold different back in the day and by different states, but getting historical data is hard I have checked everywhere. does anyone have any clues or ideas?\n\npossibly in csv? or json? or API. and I can parse them my self, pleas and thank you.\n\nplease help reddit users.",
    "url": "https://www.reddit.com/r/data/comments/1h530nb/how_to_get_free_automotive_data_on_honda_civic/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1733166815.0,
    "author": "Independent_Ad_9759",
    "subreddit": "data",
    "permalink": "/r/data/comments/1h530nb/how_to_get_free_automotive_data_on_honda_civic/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1h40cw1",
    "title": "USDA database reformat help",
    "selftext": "Is there anyone who knows a lot about the CSV file organization on USDA central database? I’m a highschool student who needs helps because I don’t really understand what’s going on.",
    "url": "https://www.reddit.com/r/data/comments/1h40cw1/usda_database_reformat_help/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 6,
    "created_utc": 1733048335.0,
    "author": "likepotatoman",
    "subreddit": "data",
    "permalink": "/r/data/comments/1h40cw1/usda_database_reformat_help/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lzwudno",
        "body": "You can usually give federal agencies a call /email / chat message to get help with their data. [https://ask.usda.gov/s/contactsupport](https://ask.usda.gov/s/contactsupport)\n\nWhat’s the link where you found the data? What are you trying to do with the data (Eg  Are you looking for a specific statistic?)",
        "score": 1,
        "created_utc": 1733079566.0,
        "author": "bassai2",
        "is_submitter": false,
        "parent_id": "t3_1h40cw1",
        "depth": 0
      },
      {
        "id": "m0apr5r",
        "body": "Did you ask ChatGTP?",
        "score": 0,
        "created_utc": 1733275259.0,
        "author": "datadanno",
        "is_submitter": false,
        "parent_id": "t3_1h40cw1",
        "depth": 0
      },
      {
        "id": "m0itfwz",
        "body": "They told me to figure it out and all the info was in there (maybe I’m just bad idk) and it’s the downloadable csv file from USDA Central on foundation food and I’m trying to make it into a big list containing all the info on each food on calories protein fats carbs salt sugar and fiber per 100g tbh I kinda gave up and I’m typing everything out by hand, there’s like 350 foods but at the pace I was going with the csv it would be faster to type everything out",
        "score": 1,
        "created_utc": 1733397279.0,
        "author": "likepotatoman",
        "is_submitter": true,
        "parent_id": "t1_lzwudno",
        "depth": 1
      },
      {
        "id": "m0isln6",
        "body": "Yes and I more or less got « ion know man figure it out »",
        "score": 1,
        "created_utc": 1733396759.0,
        "author": "likepotatoman",
        "is_submitter": true,
        "parent_id": "t1_m0apr5r",
        "depth": 1
      },
      {
        "id": "m0jkyaw",
        "body": "What’s the link?",
        "score": 1,
        "created_utc": 1733409481.0,
        "author": "bassai2",
        "is_submitter": false,
        "parent_id": "t1_m0itfwz",
        "depth": 2
      },
      {
        "id": "m0jw1i1",
        "body": "Maybe a different data set would be more aligned to your ultimate objective? https://www.kaggle.com/datasets/syedjaferk/calories-in-food-items-per-100gm-ounce-serving\n\nhttps://www.kaggle.com/datasets?tags=4306-Nutrition\n\nSome times the best way to start a data project is by choosing the right (or least worst) data set, then by adjusting your research questions accordingly.",
        "score": 1,
        "created_utc": 1733413184.0,
        "author": "bassai2",
        "is_submitter": false,
        "parent_id": "t1_m0itfwz",
        "depth": 2
      }
    ],
    "comments_extracted": 6
  },
  {
    "id": "1h3y1j5",
    "title": "Portfolio",
    "selftext": "How to build a portfolio with SQL knowledge?\n",
    "url": "https://www.reddit.com/r/data/comments/1h3y1j5/portfolio/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 0,
    "created_utc": 1733038216.0,
    "author": "Ambitious-Table3573",
    "subreddit": "data",
    "permalink": "/r/data/comments/1h3y1j5/portfolio/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1h3gv0j",
    "title": "What do I get for my data?\n",
    "selftext": "",
    "url": "https://www.reddit.com/gallery/1h3gv0j",
    "score": 4,
    "upvote_ratio": 0.75,
    "num_comments": 1,
    "created_utc": 1732985809.0,
    "author": "TripPristine4376",
    "subreddit": "data",
    "permalink": "/r/data/comments/1h3gv0j/what_do_i_get_for_my_data/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "m00aqb0",
        "body": "you get to use the app",
        "score": 1,
        "created_utc": 1733127721.0,
        "author": "next_level_dev",
        "is_submitter": false,
        "parent_id": "t3_1h3gv0j",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1h38iza",
    "title": "SQL tutorial ",
    "selftext": "I want to share this Udemy course on sale right now for SQL problem solving\n\nhere's the link --> [https://www.udemy.com/course/sql-problem-solving-for-interviews/?couponCode=SQL\\_MADE\\_EASY](https://www.udemy.com/course/sql-problem-solving-for-interviews/?couponCode=SQL_MADE_EASY)\n\nHappy learning!",
    "url": "https://www.reddit.com/r/data/comments/1h38iza/sql_tutorial/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1732956634.0,
    "author": "Marco_Nashaat",
    "subreddit": "data",
    "permalink": "/r/data/comments/1h38iza/sql_tutorial/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1h2vke4",
    "title": "Help digitalizing my pet data ",
    "selftext": "I’m not extremely tech inclined. And I would be using this on my iPad. I have 2 fish tanks and 2 snakes. I want to digitalize all my inputs without using multiple apps. When it comes to my fish tanks, I’ve been inputting everything from PH level, to water temperature on a daily basis into Microsoft excel. Same with my snakes such as their humidity and monthly weight. I’ve also been utilizing my calendars app to track stuff like what day I did water changes or what day my snakes shed. Which is extremely inefficient with calendars being orientated towards reminders and not tracking past events. I’ve been using a notebook to write down random observations such as their behaviors or things I can do to improve their habitats. I want one place that can have multiple folders (one for each habitat/ animal. Each of these folders having a spreadsheet/chart, a calendar, and notebook for notes. Do any such softwares or apps exist? \n",
    "url": "https://www.reddit.com/r/data/comments/1h2vke4/help_digitalizing_my_pet_data/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1732913582.0,
    "author": "fishindachain",
    "subreddit": "data",
    "permalink": "/r/data/comments/1h2vke4/help_digitalizing_my_pet_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lzmvtwj",
        "body": "What are you willing to spend to automate this? \nProbably <$5 a month \n\nThe best ya gunna get is a free online course in automating g-sheets/ excel to help automate whatever input/ output you want, \n\nHonestly the cheapest most efficient way of you achieving your desired goal is through your phones automation, for example, shortcuts on Apple, you can build this type of thing quite quickly",
        "score": 2,
        "created_utc": 1732923777.0,
        "author": "DataIntegrateNow",
        "is_submitter": false,
        "parent_id": "t3_1h2vke4",
        "depth": 0
      },
      {
        "id": "lzmwu23",
        "body": "I’d prefer to buy a license rather than a monthly subscription. The whole apple shortcut thing, I’ll have to do some research on that. I’m also looking into goodnotes but they lack a spreadsheet I can convert to a chart or graph to visualize the data. I’ll be getting more tanks here soon, so I’m hoping to resolve this organization issue. I know this is entirely my fault and I’m sure there’s an easier way to do this. I’m just not very tech inclined",
        "score": 1,
        "created_utc": 1732924171.0,
        "author": "fishindachain",
        "is_submitter": true,
        "parent_id": "t3_1h2vke4",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1h14oca",
    "title": "NFL Data",
    "selftext": "Can anyone point me in the direction to get NFL data? Looking ideally for free, but am open to non free as well. What is the best data sets for NFL games current and historical?",
    "url": "https://www.reddit.com/r/data/comments/1h14oca/nfl_data/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1732715225.0,
    "author": "ShizzleD21",
    "subreddit": "data",
    "permalink": "/r/data/comments/1h14oca/nfl_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lz8zbnq",
        "body": "[https://www.kaggle.com/competitions/nfl-big-data-bowl-2025/data](https://www.kaggle.com/competitions/nfl-big-data-bowl-2025/data)  \nThere is a yearly competition for nfl data in kaggle, you can find different datasets there.",
        "score": 2,
        "created_utc": 1732719100.0,
        "author": "jack_of_all_masters",
        "is_submitter": false,
        "parent_id": "t3_1h14oca",
        "depth": 0
      },
      {
        "id": "lzdhobm",
        "body": "https://github.com/nflverse You can run as a Python library. Has tons of history, play by play, stats, injuries, everything. I’ve used this for a couple data science projects so far",
        "score": 2,
        "created_utc": 1732778439.0,
        "author": "lilclit",
        "is_submitter": false,
        "parent_id": "t3_1h14oca",
        "depth": 0
      },
      {
        "id": "lz9zmwg",
        "body": "[https://nflsavant.com/](https://nflsavant.com/)",
        "score": 1,
        "created_utc": 1732730564.0,
        "author": "Both-Blueberry2510",
        "is_submitter": false,
        "parent_id": "t3_1h14oca",
        "depth": 0
      },
      {
        "id": "lze508y",
        "body": "This OP!👌",
        "score": 1,
        "created_utc": 1732793424.0,
        "author": "AdEasy7357",
        "is_submitter": false,
        "parent_id": "t1_lz9zmwg",
        "depth": 1
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "1h0zmme",
    "title": "Economic Data from 1920s",
    "selftext": "I want to extract the data for economic parameters during the Great Depression period (1929 to 1939) for USA and Japan. Does anyone know which website will give me the exact data, something like TradeMap maybe but it only provides data since 1999",
    "url": "https://www.reddit.com/r/data/comments/1h0zmme/economic_data_from_1920s/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1732695651.0,
    "author": "Mysterious_Pace_1202",
    "subreddit": "data",
    "permalink": "/r/data/comments/1h0zmme/economic_data_from_1920s/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lz9zv14",
        "body": "Tried NBER and BOJ already ?",
        "score": 1,
        "created_utc": 1732730633.0,
        "author": "Both-Blueberry2510",
        "is_submitter": false,
        "parent_id": "t3_1h0zmme",
        "depth": 0
      },
      {
        "id": "lzc0vt5",
        "body": "BOJ had no data for the time period which I realised after downloading the CSV 🥲, NBER  was somewhat the same no data available",
        "score": 1,
        "created_utc": 1732754903.0,
        "author": "Mysterious_Pace_1202",
        "is_submitter": true,
        "parent_id": "t1_lz9zv14",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1h0p9jr",
    "title": "Looking for food menu related data.",
    "selftext": "Im working on a project where the aim is to provide food/ restaurant recs based around their desired meal budget. \n\ni've tried a few sources:\n\n1. MealMe - One of the most suggested. Comes with a heavy price tag which I cannot afford.  \n2. OpenMenu- I reached out to them but no response  \n3. Yelp Fusion API: This is what I'm currently using. The Fusion API unfortunately doesn't allow menu item information.\n\nThe other thing i've looked into is using Open Street Maps and to perform a search for the businesses and then scrape relevant Menu Data. This doesn't seem to be the most efficient as a lot the the data is not available on OSM.\n\n  \nAny guidance on how I could proceed would be appreciated!",
    "url": "https://www.reddit.com/r/data/comments/1h0p9jr/looking_for_food_menu_related_data/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1732661395.0,
    "author": "Blahblahblakha",
    "subreddit": "data",
    "permalink": "/r/data/comments/1h0p9jr/looking_for_food_menu_related_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1h0doia",
    "title": "Are projects with static datasets worth including in a portfolio? ",
    "selftext": "I'm doing a route optimization project with a dataset I found on Kaggle. Every time I ask ChatGPT anything about this project, it responds with some kind of Google traffic and weather API that it wants me to use. I have used APIs in other projects, but right now I'm doing route optimization in R. It's my first real modeling in R. I could easily do this in Python and maybe even add an API there, but alas, I'm trying to get some R fundamentals down. \n\nThe fact that ChatGPT will not stop suggesting APIs is causing me to second guess if any of my projects are portfolio worthy without it. Please give me your thoughts. ",
    "url": "https://www.reddit.com/r/data/comments/1h0doia/are_projects_with_static_datasets_worth_including/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1732632820.0,
    "author": "Cheap-Selection-2406",
    "subreddit": "data",
    "permalink": "/r/data/comments/1h0doia/are_projects_with_static_datasets_worth_including/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lz6xqmz",
        "body": "I would say it’s still worth it. Depending on the level and role but mostly what people like seeing is that you had a question, and took logical steps / problem solved your way to an answer.\n\nMy main portfolio piece in undergrad was a personal project where I just downloaded a bunch of inflation, unemployment, crime stats etc from FRED and ran a couple regressions on it. The regressions were crap but learning how to clean and aggregate the data, then feed it into a model was what they wanted to see. To this day that crash course in data cleaning has paid off in spades (joins, string manipulation, date alignment, etc.)\n\nEspecially if you’re jumping into an entry level, your boss or tech lead should be able to guide you through the tricky bits.\n\nOne caveat being if you’re trying to demonstrate experience with a specific piece of software. Knowing how to handle APIs is good, but there are plenty of other things you can showcase.",
        "score": 1,
        "created_utc": 1732680758.0,
        "author": "Auggernaut88",
        "is_submitter": false,
        "parent_id": "t3_1h0doia",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1h0bqm3",
    "title": "Usability of data with significant ceiling effect",
    "selftext": "Hello,\n\nI am currently writing my thesis about the effect of childhood adversity on sensitivity to feaful faces using a facial emotion recognition task. One outcome measure is accuracy, however there is a significant ceiling effect. 64% of all participants scored 100% accuracy. The distrubution is as follows: 1 participant scores 86%, 2 participants scored 90%, 14 scored 95% and 28 scored 100%. I can log transform the data or I can apply a two parts model in which the data is split in 100 or lower than 100, and the remaining variance (lower than 100 )is also modelled. However I dont know whether it even is useful to report the accuracy in my thesis, because even with a log transformation, or two parts model there still is a very significant ceiling effect. I could also only use reaction time in which there is no ceiling effect.\n\nThank you in advance!",
    "url": "https://www.reddit.com/r/data/comments/1h0bqm3/usability_of_data_with_significant_ceiling_effect/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1732627331.0,
    "author": "asap-lars",
    "subreddit": "data",
    "permalink": "/r/data/comments/1h0bqm3/usability_of_data_with_significant_ceiling_effect/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1h05mrl",
    "title": "Data",
    "selftext": "I'm looking to learn something in the data field.\nI don't have a degree, but I know SQL.\nI enjoy working with Excel and building databases.\nWhat course should I take, and what fields can I explore related to this topic?\n\n",
    "url": "https://www.reddit.com/r/data/comments/1h05mrl/data/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1732603121.0,
    "author": "Ambitious-Table3573",
    "subreddit": "data",
    "permalink": "/r/data/comments/1h05mrl/data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lza0ly1",
        "body": "Try to get a job if you already know sql or work on projects that involve data to showcase your skills to get that job.  \nCourses are overrated.",
        "score": 3,
        "created_utc": 1732730861.0,
        "author": "Both-Blueberry2510",
        "is_submitter": false,
        "parent_id": "t3_1h05mrl",
        "depth": 0
      },
      {
        "id": "lz1p197",
        "body": "I am planning to learn Data analysis. Not sure how it goes",
        "score": 2,
        "created_utc": 1732609844.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_1h05mrl",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1gzvcv8",
    "title": "Looking to create a multilingual exams dataset ",
    "selftext": "I’m looking to create a multilingual exams dataset — I want to collect exams from other countries ideally those with some multimodal components (diagrams, passages, etc). I’m looking for things like the Korean CSAT, French PASS, Japanese Kyotsu — and more ! \n\nPlease post raw PDFs of these exams (with answers) if you can. Your help is much appreciated. ",
    "url": "https://www.reddit.com/r/data/comments/1gzvcv8/looking_to_create_a_multilingual_exams_dataset/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1732572265.0,
    "author": "student25031998",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gzvcv8/looking_to_create_a_multilingual_exams_dataset/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gzncoe",
    "title": "How to Build an In-House Tool for Tracking EMV and VIT?",
    "selftext": "Does anyone have experience with Traackr or similar tools for tracking EMV and VIT?\n\nI’m planning to build an in-house version of Traackr to track EMV (Earned Media Value) and VIT (Vitality Score), but with added capabilities to break down the data by age group and ethnicity since my company prioritizes these insights.\n\nHow should I get started? What steps do I need to take?\n\nWould this be a difficult project? Will it require a lot of math or advanced analytics?\n\nAny guidance, tips, or resources would be greatly appreciated!\n\n",
    "url": "https://www.reddit.com/r/data/comments/1gzncoe/how_to_build_an_inhouse_tool_for_tracking_emv_and/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1732553262.0,
    "author": "Ok-Department-7482",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gzncoe/how_to_build_an_inhouse_tool_for_tracking_emv_and/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lza1k24",
        "body": "Think the math itself is easy. Tracking is the part I am not sure.  \nEMV =  CPM \\* Impressions on a post or something like that \\* Adjustment factor (optional or could include CTR or clicks here)\n\nCPM - as a v1 could be overall average CPM of paid media  \nImpressions - Do you track this  \nAdjustment factor - again optional and you could skip for v1\n\n[https://www.shopify.com/blog/how-to-calculate-earned-media-value](https://www.shopify.com/blog/how-to-calculate-earned-media-value)",
        "score": 2,
        "created_utc": 1732731153.0,
        "author": "Both-Blueberry2510",
        "is_submitter": false,
        "parent_id": "t3_1gzncoe",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1gz8ckf",
    "title": "Hello, does anyone know if there's a large public dataset on vicarious trauma anywhere?",
    "selftext": "I've checked Kaggle but can't seem to find one, i'd be intrigued to see some data on it. ",
    "url": "https://www.reddit.com/r/data/comments/1gz8ckf/hello_does_anyone_know_if_theres_a_large_public/",
    "score": 3,
    "upvote_ratio": 0.81,
    "num_comments": 1,
    "created_utc": 1732501383.0,
    "author": "TopazFlame",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gz8ckf/hello_does_anyone_know_if_theres_a_large_public/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gyal9g",
    "title": "Data on Free and Clear homeowners and their age",
    "selftext": "Hello,\n\nDoes anyone have a source to pull data on homeowners that own their property free and clear and are a specific age?",
    "url": "https://www.reddit.com/r/data/comments/1gyal9g/data_on_free_and_clear_homeowners_and_their_age/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1732397923.0,
    "author": "Horror_East2071",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gyal9g/data_on_free_and_clear_homeowners_and_their_age/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gy8x8n",
    "title": "Looking for Data Streaming Related Content Writer(s)",
    "selftext": "Hi Folks,   \n  \nWe are seeking a skilled content writer with expertise in data streaming technologies to create high-quality, engaging, and technically accurate content. The ideal candidate will have experience writing about tools such as Apache Kafka, Apache Flink, Spark Streaming, or similar platforms.  \n  \n**Responsibilities:**  \nCreate blog posts, tutorials, white papers, and case studies on data streaming topics.  \nSimplify complex concepts for technical and non-technical audiences.  \nStay updated on the latest trends in data engineering and streaming technologies.  \n  \n**Requirements:**  \nProven experience in writing technical content, preferably in data engineering or big data.  \nStrong understanding of data streaming concepts and tools.  \nAbility to deliver SEO-optimized, well-researched content.",
    "url": "https://www.reddit.com/r/data/comments/1gy8x8n/looking_for_data_streaming_related_content_writers/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1732393474.0,
    "author": "Plenty_Obligation151",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gy8x8n/looking_for_data_streaming_related_content_writers/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gxzal0",
    "title": "Where to find data on election results by brand and type of machine used to collect and tabulate?",
    "selftext": "",
    "url": "https://www.reddit.com/r/data/comments/1gxzal0/where_to_find_data_on_election_results_by_brand/",
    "score": 5,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1732367474.0,
    "author": "lostredditers",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gxzal0/where_to_find_data_on_election_results_by_brand/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gwud08",
    "title": "Speak AI - Take home excerise",
    "selftext": "Since they don't have a Glassdoor account, I decided to post here about my experience interviewing for a BizOps role at Speak AI.\n\nI was given a take-home exercise after completing two interview rounds. I was told this is the last round and after completing the exercise, I'd be able to present my work. I spent approximately 4 full days on this exercise and put in a lot of work including market research for multiple ideas. \n\nIt's been almost two weeks since I submitted it and have not heard back from them on any next steps. I'm not sure what to think of this? Was I completely scammed? Or am I exaggerating?",
    "url": "https://www.reddit.com/r/data/comments/1gwud08/speak_ai_take_home_excerise/",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1732233794.0,
    "author": "datagirl_123",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gwud08/speak_ai_take_home_excerise/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gwew22",
    "title": "Short term positions in data fields ",
    "selftext": "Hi everyone,\n\nI would like to have advices about what field to choose if you like changing jobs/company often.\n\nAs part of a professional retraining, I joined a data analysis bootcamp (3 months) and I am now a data science apprentice in a company (1 year and a half studying at school while also working in a company). \n\nI would like to know what kind of analytical jobs are available when you enjoy changing companies after about a year. I realise that after a year in a company, I become kind of bored of the people and the missions (I had several work experiences before turning to data science and this was already the case)\n\nI am thinking about becoming a freelancer to find short missions either in data analysis, data science, or even data engineering since I had a few DE related missions that I really enjoyed.\n\nIn your opinions, is the idea of changing jobs often realistic in this field? \nFrom what I have seen, it seems that data science jobs are not likely to be short term. But what about data analysis and data engineering? \n\nSorry for the long message, thanks for reading. ",
    "url": "https://www.reddit.com/r/data/comments/1gwew22/short_term_positions_in_data_fields/",
    "score": 3,
    "upvote_ratio": 0.81,
    "num_comments": 0,
    "created_utc": 1732191340.0,
    "author": "Jlgsvvc",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gwew22/short_term_positions_in_data_fields/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gw6vlv",
    "title": "Help please ",
    "selftext": "What would be the reason internet not getting connection after installing cat6a terminated on cat6a jacks ? ",
    "url": "https://www.reddit.com/r/data/comments/1gw6vlv/help_please/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1732159993.0,
    "author": "OwnAd5017",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gw6vlv/help_please/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gvjwlw",
    "title": "Need a roadmap for entry level data engineering roles",
    "selftext": "Need a roadmap for an entry level data engineer role. \n\nI have 18 months of experience in a service based company. Unfortunately due to the bad mass hiring procedures and scarcity of jobs in India, I got pulled into an project and role not of my selection. I want to work in data engineering field instead. \n\nMy work experience is in a Product Experience Management tool similar to Syndigo or Stibo. \nDefinitely some ETL procedures and skills I have learned by handling retail datasets, but its more of an integration/configuration work with some need of SQL. \n\nI have good technical knowledge of data and data warehousing concepts as I had industry internship on it for 6 months. I have basic handson’s of Informatica power centre, Datastage, Talend and Abnitio. But the basics mostly from that internship. \nOther than that currently have Azure DP 900, preparing for PL-900 cause I thought my retail data skills will be more usefull with PowerBI and then will give Azure Data Engineer one hopefully. \n\nI have programming knowledge in SQL and Python. Planning to learn spark a bit as I have worked on Azure databricks for some hackathon usecases. \n\nDefinately have to make some proper data engineering projects as well. \n\nOther than that is there any more suggestions? Or do you think I am in the right path to switch my job role to some data engineering role within 1 year or so? ",
    "url": "https://www.reddit.com/r/data/comments/1gvjwlw/need_a_roadmap_for_entry_level_data_engineering/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1732086531.0,
    "author": "National-Owl-9987",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gvjwlw/need_a_roadmap_for_entry_level_data_engineering/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "ly851bz",
        "body": "have you tried warehouse nativa PA tools such as Mitzu?",
        "score": 1,
        "created_utc": 1732179965.0,
        "author": "Ambrus2000",
        "is_submitter": false,
        "parent_id": "t3_1gvjwlw",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1guv4ix",
    "title": "French State Open Data platform data.gouv.fr demo",
    "selftext": "The French Open Data platform [data.gouv.fr](http://data.gouv.fr/) is organizing a public demo to show the latest and future planned features of the platform, which includes harvesting geographic data, high-value data, opening up the platform to restricted data, providing data through APIs, etc.\n\nDemo is on November 20, 2024, from 1pm. to 2pm UTC (all in French), and registration to attend is here: [https://tally.so/r/mV1LAJ](https://tally.so/r/mV1LAJ)",
    "url": "https://www.reddit.com/r/data/comments/1guv4ix/french_state_open_data_platform_datagouvfr_demo/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1732016177.0,
    "author": "bolinocroustibat",
    "subreddit": "data",
    "permalink": "/r/data/comments/1guv4ix/french_state_open_data_platform_datagouvfr_demo/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gufcl6",
    "title": "Where I can download documents/pdfs/txt files related to same topic ?",
    "selftext": "",
    "url": "https://www.reddit.com/r/data/comments/1gufcl6/where_i_can_download_documentspdfstxt_files/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1731963483.0,
    "author": "RstarPhoneix",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gufcl6/where_i_can_download_documentspdfstxt_files/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gu6r2g",
    "title": "Any good Laptop for DS?",
    "selftext": "Hi!\n\nI am willing to buy a new laptop during this Black Friday or Christmass vacation, and I wanted to ask opinions on MacBooks Air, Lenovo ThinkPads and more...\n\nI do an MSc in Data Science and work as a digital analyst doing data projects in which most of the programming is on the cloud.\n\nBudget ~= 1000$",
    "url": "https://www.reddit.com/r/data/comments/1gu6r2g/any_good_laptop_for_ds/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1731942285.0,
    "author": "OutrageousTheme976",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gu6r2g/any_good_laptop_for_ds/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gs3cac",
    "title": "I created a graph-based optimizer that not only works, but it also actually beats Adam, like very badly!",
    "selftext": "I prove it thoroughly using SMOL LLM models. The secret? The graph is not what you think it is, humans. Code, full explanation, and more in this video. The Rhizome Optimizer is MIT licensed.\n\n[https://youtu.be/OMCRRueMhdI](https://youtu.be/OMCRRueMhdI)\n\n# LLM ABC's Defined:\n\n1. **Nodes**:\n   * **Definition**: These are the fundamental units of knowledge or **disparate concepts** within the model. Think of them as the atomic building blocks, representing individual words, phrases, or even abstract ideas.\n   * **Function**: Nodes act as anchors in the model's conceptual space. By optimizing how nodes interact, the model can form more coherent and meaningful connections.\n2. **Edges**:\n   * **Definition**: The **relationships** between nodes, representing the **patterns** and connections that link concepts together. These edges capture the dependencies, associations, and context between nodes.\n   * **Function**: Edges are crucial for forming meaning. By tuning the quality and weight of these connections, the model can enhance its understanding of the relationships between disparate concepts, making its output more coherent and contextually accurate.\n3. **Clusters**:\n   * **Definition**: The **shapes** formed by interconnected nodes and edges. These clusters represent emergent structures, patterns of meaning, or thematic groupings. The shape itself is information, carrying meaning based on its form, density, and fluidity.\n   * **Function**: Clusters capture higher-level abstractions by grouping related concepts. The **fluid nature** of these clusters allows the model to dynamically adjust its understanding, enabling adaptive reasoning across various contexts.\n\n# The Interplay Between Nodes, Edges, and Clusters:\n\n* **Nodes** are isolated concepts, but they gain meaning through **Edges**, which bind them into relationships.\n* **Clusters** are the emergent structures formed from nodes connected by edges. They can adapt and transform, much like fluid, depending on the strength and context of the relationships.\n\n# Application in the Rhizome Optimizer:\n\n* In optimizing neural networks, rather than solely focusing on reducing loss, the Rhizome Optimizer will aim to **enhance the quality of edges** and **optimize the structure of clusters**. This can lead to richer conceptual integration and a more adaptive learning process.\n* By treating clusters as fluid structures, the model can dynamically reshape its understanding, making it better at generalizing across contexts.",
    "url": "https://www.reddit.com/r/data/comments/1gs3cac/i_created_a_graphbased_optimizer_that_not_only/",
    "score": 2,
    "upvote_ratio": 0.67,
    "num_comments": 0,
    "created_utc": 1731695902.0,
    "author": null,
    "subreddit": "data",
    "permalink": "/r/data/comments/1gs3cac/i_created_a_graphbased_optimizer_that_not_only/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1grdolo",
    "title": "Wars & Their Causes",
    "selftext": "Hi guys,\n\nThis might be a long shot but I'm writing a book and I'm wondering if anyone has put together a list of all the wars throughout human history with their dates and categorised their causes,\n\nIf not I'm going to put it together myself but thought I'd reach out first just incase someone has already done it and can save me the leg work,\n\nHappy to reference you in the book and any help is much appreciated,\n\nAll the best,",
    "url": "https://www.reddit.com/r/data/comments/1grdolo/wars_their_causes/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1731614271.0,
    "author": "jpearcewords",
    "subreddit": "data",
    "permalink": "/r/data/comments/1grdolo/wars_their_causes/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1grf5ao",
    "title": "Goodbye Databases",
    "selftext": "",
    "url": "https://x.com/breckyunits/status/1857136517451317682",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 0,
    "created_utc": 1731618048.0,
    "author": "breck",
    "subreddit": "data",
    "permalink": "/r/data/comments/1grf5ao/goodbye_databases/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gqgdt4",
    "title": "Free SQL course on Udemy",
    "selftext": "Hello everyone!\n\nI created a course for SQL problem solving on Udemy and I wanted to share it with the community for free, so here's the [coupon](https://www.udemy.com/course/sql-problem-solving-for-interviews/?couponCode=2079C1E1AD7714314A5B) click on it and the free coupon should be already applied.\n\nDon't forget to leave a review and get back to me with any feedback you have! \n\nFree spots are limited and only available for the next 5 days, so get it now and start learning!\n\nHappy learning!",
    "url": "https://www.reddit.com/r/data/comments/1gqgdt4/free_sql_course_on_udemy/",
    "score": 6,
    "upvote_ratio": 1.0,
    "num_comments": 5,
    "created_utc": 1731514121.0,
    "author": "Marco_Nashaat",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gqgdt4/free_sql_course_on_udemy/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lwycn1s",
        "body": "Where's the link?",
        "score": 1,
        "created_utc": 1731521594.0,
        "author": "random_py",
        "is_submitter": false,
        "parent_id": "t3_1gqgdt4",
        "depth": 0
      },
      {
        "id": "m8o6b62",
        "body": "Says its 13.99",
        "score": 1,
        "created_utc": 1737606283.0,
        "author": "Mzkazmi",
        "is_submitter": false,
        "parent_id": "t3_1gqgdt4",
        "depth": 0
      },
      {
        "id": "lwygjgp",
        "body": "[https://www.udemy.com/course/sql-problem-solving-for-interviews/?couponCode=2079C1E1AD7714314A5B](https://www.udemy.com/course/sql-problem-solving-for-interviews/?couponCode=2079C1E1AD7714314A5B)\n\nor click the word \"coupon\" in the post, it should be highlighted.",
        "score": 2,
        "created_utc": 1731522768.0,
        "author": "Marco_Nashaat",
        "is_submitter": true,
        "parent_id": "t1_lwycn1s",
        "depth": 1
      },
      {
        "id": "m8pd8dv",
        "body": "The coupon was valid for only 5 days. I'll be in touch if I have more free coupons",
        "score": 1,
        "created_utc": 1737629638.0,
        "author": "Marco_Nashaat",
        "is_submitter": true,
        "parent_id": "t1_m8o6b62",
        "depth": 1
      },
      {
        "id": "lwyv8mm",
        "body": "Thank you very much",
        "score": 1,
        "created_utc": 1731527201.0,
        "author": "random_py",
        "is_submitter": false,
        "parent_id": "t1_lwygjgp",
        "depth": 2
      }
    ],
    "comments_extracted": 5
  },
  {
    "id": "1gqpt6o",
    "title": "Looking for Datasets on Unemployment,Inflation and Unclaimed Job Openings [International and in the USA]",
    "selftext": "I am currently doing a research paper and have been using BLS,OECD and The World Bank for information on these topics.I woud love to find alternatives to get a more non-bias american view ,as well as cross reference.",
    "url": "https://www.reddit.com/r/data/comments/1gqpt6o/looking_for_datasets_on_unemploymentinflation_and/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1731537744.0,
    "author": "Far-Item6455",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gqpt6o/looking_for_datasets_on_unemploymentinflation_and/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gq4ebj",
    "title": "Data Collection for Criminal Legal Systems",
    "selftext": "Hello! As part of a fact-finding effort for a local legal nonprofit, I hoped to humbly ask a few questions around any data collection and reporting systems that community members here have heard about or have interacted with.\n\nThe basic context is a legal non profit wants to advocate for building an anonymous data collection system that reports demographic details around dimensions like race, socioeconomic status, employment status, geolocation, to be able to serve clients better. An example use case: many of the county residents lost their homes in a catastrophic fire, and they want to be able to analyze how it impacted different demographic segments. If you've read this far, thank you! Here are a few general questions:\n\n* Any experience using data in such a capacity?\n* If so, do you know if it was an off the shelf platform/service or built by an IT department or contracted out to data engineers, devs, etc.? (If off the shelf, can you share the name if you remember it?)\n* Do you remember any unusual but helpful reporting dimensions?\n* Are you aware of county or organization currently employing a data collection system in a similar context?\n* Other than PII (Personally Identifiable Information), do any other concerns come to mind implementing data solutions around client advocacy in a criminal justice system?\n\nAgain, thanks for taking time to read this, and more thanks in advance for any responses here! Best to you!",
    "url": "https://www.reddit.com/r/data/comments/1gq4ebj/data_collection_for_criminal_legal_systems/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 0,
    "created_utc": 1731470802.0,
    "author": "Strict-Basil5133",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gq4ebj/data_collection_for_criminal_legal_systems/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gptdkv",
    "title": "Can it be that all groups of women shifted towards Trump from 2020 to 2024? I could not really find this data in AP VoteCast.",
    "selftext": "",
    "url": "https://i.redd.it/a07h2v23xi0e1.jpeg",
    "score": 2,
    "upvote_ratio": 0.67,
    "num_comments": 1,
    "created_utc": 1731440342.0,
    "author": "andWan",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gptdkv/can_it_be_that_all_groups_of_women_shifted/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lwsrffs",
        "body": "Source: https://x.com/Nowooski/status/1856364197178908885\n\nEarlier source: https://x.com/PatrickRuffini/status/1856338938656346240\n\nFrom the bio of the second tweet: „author of Party of the People: Inside the Multiracial Populist Coalition Remaking the GOP.“ This makes me question if this graph is real. But I had problems to navigate the AP VoteCast page.",
        "score": 1,
        "created_utc": 1731440608.0,
        "author": "andWan",
        "is_submitter": true,
        "parent_id": "t3_1gptdkv",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1gpwmdd",
    "title": "Which is worse?",
    "selftext": "Inaccurate data or no data at all?",
    "url": "https://www.reddit.com/r/data/comments/1gpwmdd/which_is_worse/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1731448400.0,
    "author": "---Skip_lntro---",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gpwmdd/which_is_worse/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lwy2m59",
        "body": "Inaccurate data is far worse. At least when data is missing you can't be led astray into making terrible decisions.",
        "score": 1,
        "created_utc": 1731518561.0,
        "author": "Guavifo",
        "is_submitter": false,
        "parent_id": "t3_1gpwmdd",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1goptcm",
    "title": "Why Choose (or Not Choose) Sapienza University for a Master’s in Data Science?",
    "selftext": "Hello everyone,\n\nI’m considering pursuing a Master’s in Data Science at Sapienza University for Fall 2025. However, I’m unsure if it’s the right choice for me. Here’s a bit about me: I’m from a Central Asian country, and initially, I wanted to do my Master’s in Germany. Unfortunately, my credits (I have a Bachelor's in Economics and Management) aren’t sufficient to qualify for Data Science programs there. I have 2 years of international experience, which I think adds value, but I’m still not sure if Sapienza is the best fit.\n\nSo, I’m wondering:\n\n1. Why would you recommend Sapienza University for Data Science?\n2. What are the reasons someone might want to avoid this university for the same program?\n3. Additionally, how does Sapienza help with internships, especially for international students looking to intern at big tech companies like Meta, Google, or Bloomberg?\n\nI’d appreciate any advice or insights from people who’ve been through this!\n\nThanks in advance!",
    "url": "https://www.reddit.com/r/data/comments/1goptcm/why_choose_or_not_choose_sapienza_university_for/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1731323462.0,
    "author": "Responsible_Ad_6595",
    "subreddit": "data",
    "permalink": "/r/data/comments/1goptcm/why_choose_or_not_choose_sapienza_university_for/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lwkh1tm",
        "body": "I too wanted to know!!!",
        "score": 1,
        "created_utc": 1731326841.0,
        "author": "Least_Stage_9714",
        "is_submitter": false,
        "parent_id": "t3_1goptcm",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1go9anb",
    "title": "How to find all youtube videos with a specific word in the title?",
    "selftext": "How to find all youtube videos with a specific word in the title?",
    "url": "https://www.reddit.com/r/data/comments/1go9anb/how_to_find_all_youtube_videos_with_a_specific/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1731268581.0,
    "author": "SelectionSad4840",
    "subreddit": "data",
    "permalink": "/r/data/comments/1go9anb/how_to_find_all_youtube_videos_with_a_specific/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gmlzf1",
    "title": "Election data by county",
    "selftext": "I’m trying to make some election maps that show how counties voted, but I need actual election results, anyone have a way to pull election results county by county?",
    "url": "https://www.reddit.com/r/data/comments/1gmlzf1/election_data_by_county/",
    "score": 6,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1731081873.0,
    "author": "derrzerr",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gmlzf1/election_data_by_county/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lwv6012",
        "body": "I would also very much love to see how each individual county voted, but I can't easily find this data, either.",
        "score": 1,
        "created_utc": 1731470258.0,
        "author": "Guavifo",
        "is_submitter": false,
        "parent_id": "t3_1gmlzf1",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1gmn3uw",
    "title": "Property damage due to hurricanes",
    "selftext": "Does anyone know where to get specifically and purely the amount of property damage measured in USD, in the US, due to hurricanes?  All the figures I find are (reasonably) convoluted with a less firm measure of lost worker productivity and so on.  I'd like to try to get a fairly firm number, like just the measure of property damage alone.  \n\nLikewise for wildfire damage, by the way, if anyone has a source handy.  ",
    "url": "https://www.reddit.com/r/data/comments/1gmn3uw/property_damage_due_to_hurricanes/",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1731084704.0,
    "author": "axiom_tutor",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gmn3uw/property_damage_due_to_hurricanes/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1glef1c",
    "title": "Anyone want to export their Walmart Order History?\n",
    "selftext": "Looking for someone to test an app I have that allows you to export your Walmart Order history.\n\n",
    "url": "https://www.reddit.com/r/data/comments/1glef1c/anyone_want_to_export_their_walmart_order_history/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 10,
    "created_utc": 1730941307.0,
    "author": "Frequent-Arm5530",
    "subreddit": "data",
    "permalink": "/r/data/comments/1glef1c/anyone_want_to_export_their_walmart_order_history/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lvv0aje",
        "body": "I don’t have Walmart order history but I’m curious, how does it work?",
        "score": 1,
        "created_utc": 1730963274.0,
        "author": "Colton200456",
        "is_submitter": false,
        "parent_id": "t3_1glef1c",
        "depth": 0
      },
      {
        "id": "lvv10cr",
        "body": "It's a Chrome extension that exports order history from sites like TEMU, Instacart, eBay, Amazon, and Walmart. \n\nIt's pretty simple to use. Once you have it installed you visit the site and then you will have the option to export the order history for a period (2024, 2023, etc)",
        "score": 2,
        "created_utc": 1730963693.0,
        "author": "Frequent-Arm5530",
        "is_submitter": true,
        "parent_id": "t1_lvv0aje",
        "depth": 1
      },
      {
        "id": "lvv18vk",
        "body": "So then through your chrome extension, it tracks the log in or just scrapes whatever page you’re on? What if you have multiple pages of order history for a single year?",
        "score": 1,
        "created_utc": 1730963834.0,
        "author": "Colton200456",
        "is_submitter": false,
        "parent_id": "t1_lvv10cr",
        "depth": 2
      },
      {
        "id": "lvv1ifz",
        "body": "It doesn't track the login, once you leave the tab open it will fetch the data for multiple pages of order history in the background.",
        "score": 1,
        "created_utc": 1730963992.0,
        "author": "Frequent-Arm5530",
        "is_submitter": true,
        "parent_id": "t1_lvv18vk",
        "depth": 3
      },
      {
        "id": "lvv1yds",
        "body": "API’s and JS?",
        "score": 1,
        "created_utc": 1730964256.0,
        "author": "Colton200456",
        "is_submitter": false,
        "parent_id": "t1_lvv1ifz",
        "depth": 4
      },
      {
        "id": "lvv256f",
        "body": "Both",
        "score": 1,
        "created_utc": 1730964372.0,
        "author": "Frequent-Arm5530",
        "is_submitter": true,
        "parent_id": "t1_lvv1yds",
        "depth": 5
      },
      {
        "id": "lvv29eq",
        "body": "So do you just store the year as a variable and fetch’s pages until you find the last (well, first) purchase for that year?",
        "score": 1,
        "created_utc": 1730964441.0,
        "author": "Colton200456",
        "is_submitter": false,
        "parent_id": "t1_lvv256f",
        "depth": 6
      },
      {
        "id": "lvv33m2",
        "body": "In a nutshell, that is it. From a high level, it is:\n\n1. Storing the period as you said\n2. Identifying how many orders are in that period\n3. Fetching the pages for the orders\n4. Parsing the pages\n5. Preparing for export\n\nApart from the above, there are aspects such as:\n\n1. Managing communication between multiple tabs and the extension\n2. Managing situations with bot blockers\n3. Managing concurrent requests\n4. Throttling requests\n5. Managing data cache\n6. Managing export resumptions if there is a disruption (like a close tab etc)\n\nThere are a few more, but I can remember this from the top of my head now.",
        "score": 1,
        "created_utc": 1730964947.0,
        "author": "Frequent-Arm5530",
        "is_submitter": true,
        "parent_id": "t1_lvv29eq",
        "depth": 7
      },
      {
        "id": "lvv3uqb",
        "body": "Apologies if I’m asking too many questions, just ignore me when you get tired, I do C# SQL for a school district, so being able to think in a different language and use is always a nice little spice up 🤣\n\nSo when you go to the order history, do you first input what year you want to pull?",
        "score": 1,
        "created_utc": 1730965409.0,
        "author": "Colton200456",
        "is_submitter": false,
        "parent_id": "t1_lvv33m2",
        "depth": 8
      },
      {
        "id": "lvv62we",
        "body": "😅 No problem, I welcome all the questions. Since you have a technical background I could go more in-depth with concerns, pros, and cons with decisions. I have experience with C# and SQL.  \n  \nIf anything you can DM me directly and we chat there. I'm signing off for the night. You can check out the [extension](https://chromewebstore.google.com/detail/ecommerce-order-history-d/gnemaookpcekfeidmgocagofgijpoced) if you want.\n\nSo it's not necessarily going to the order page, but knowing what the order page URL or API call is and then pre-formatting the request and then sending an XML HTTP request. From there parsing the HTML in the background.",
        "score": 2,
        "created_utc": 1730966795.0,
        "author": "Frequent-Arm5530",
        "is_submitter": true,
        "parent_id": "t1_lvv3uqb",
        "depth": 9
      }
    ],
    "comments_extracted": 10
  },
  {
    "id": "1gkzizq",
    "title": "Practice with Election data set?",
    "selftext": "Hello,\n\nI want to practice applying my data analytics and visualization skills with election data. \nDoes anyone know where I can access the largest and most robust election dataset as possible? \n\nThanks in advance. ",
    "url": "https://www.reddit.com/r/data/comments/1gkzizq/practice_with_election_data_set/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1730902678.0,
    "author": "Accomplished-You9482",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gkzizq/practice_with_election_data_set/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lvpxn1d",
        "body": "For polls data \"Five Thirty Eight\".",
        "score": 2,
        "created_utc": 1730905031.0,
        "author": "PracticalPlenty7630",
        "is_submitter": false,
        "parent_id": "t3_1gkzizq",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1gktytz",
    "title": "Need Help Finding Drug (Medicine) Info for a project of mine.",
    "selftext": "Can anyone help me with the following? Where do I find data from India such as:\n\nDrug Name (I have this)\n\n* Salt Composition\n* Uses & Benefits\n* Side Effects\n* How to Use\n* How the Drug Works\n* Safety Advice\n\nThe screenshot is from Tata 1MG.\n\n(**Mods can delete this post if it is not relevant to this sub.**)\n\n[Tata 1MG](https://preview.redd.it/xg5dtgcft8zd1.jpg?width=1335&format=pjpg&auto=webp&s=9a596ee59b7c7e9c1d50d025376a46ec13b05f74)\n\n",
    "url": "https://www.reddit.com/r/data/comments/1gktytz/need_help_finding_drug_medicine_info_for_a/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1730882281.0,
    "author": "Busy-Competition-786",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gktytz/need_help_finding_drug_medicine_info_for_a/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lvp1m3g",
        "body": "[removed]",
        "score": 1,
        "created_utc": 1730895100.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_1gktytz",
        "depth": 0
      },
      {
        "id": "lvphes7",
        "body": "Oh, ok. Thanks for letting me know.",
        "score": 1,
        "created_utc": 1730900266.0,
        "author": "Busy-Competition-786",
        "is_submitter": true,
        "parent_id": "t1_lvp1m3g",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1gkt7cr",
    "title": "Finding free climatological data (time-series)",
    "selftext": "Hello! I'm looking for data sources for climatological data (temperature, rainfall, humidity, etc) because I'm conducting research in Mindanao, Philippines. I have tried accessing JAXA, but they have not had data in my specific research areas since 2010. ",
    "url": "https://www.reddit.com/r/data/comments/1gkt7cr/finding_free_climatological_data_timeseries/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1730879018.0,
    "author": "DesperateAd1956",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gkt7cr/finding_free_climatological_data_timeseries/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gkg8zw",
    "title": "Book review: Web Scraping with Python ",
    "selftext": "Hi everyone! Hope this is allowed. Wanted to share a book I've just finished reading and found super useful as a data analyst trying to get into data engineering.\n\nIt's called \"Web Scraping With Python\"\n\nI've written up a review of it, you can find on my [blog](https://medium.com/@sergioramos3.sr/self-taught-reviews-web-scraping-with-python-by-ryan-mitchell-fe4899cb868e)\n\nWould love you guys' thoughts!",
    "url": "https://www.reddit.com/r/data/comments/1gkg8zw/book_review_web_scraping_with_python/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1730837984.0,
    "author": "0sergio-hash",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gkg8zw/book_review_web_scraping_with_python/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gkcugk",
    "title": "Software for large sets of data",
    "selftext": "Hello to all my software engineers/sales reps, data analysts, etc. I'm being asked to create a project plan that requires a software solution that can pull/ handle a good amount of data from government systems. I am also going to need to provide monthly reports to one of our customers. After researching, looks like etl software for data management and some type of reporting software for the actual reporting piece. Can someone please provide insight on this? Especially on software pricing and best softwares to go with?",
    "url": "https://www.reddit.com/r/data/comments/1gkcugk/software_for_large_sets_of_data/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1730829478.0,
    "author": "Bblstar123",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gkcugk/software_for_large_sets_of_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gk7ap8",
    "title": "free SQL course on Udmey",
    "selftext": "Hello everyone,\n\nI created a SQL problem-solving course on Udemy to help people in data fields prepare for technical interviews since SQL is a big part of it and you can have it for free [here](https://www.udemy.com/course/sql-problem-solving-for-interviews/?couponCode=5F8332202C6CA7BF327B)\n\nI'll be glad to get your feedback and I'd really appreciate it if you leave a review!!\n\nHappy learning!",
    "url": "https://www.reddit.com/r/data/comments/1gk7ap8/free_sql_course_on_udmey/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1730815148.0,
    "author": "Marco_Nashaat",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gk7ap8/free_sql_course_on_udmey/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gjxddu",
    "title": "98% of companies experienced ML project failures last year, with poor data and lackluster cost-performance the primary causes",
    "selftext": "",
    "url": "https://info.sqream.com/hubfs/data%20analytics%20leaders%20survey%202024.pdf",
    "score": 4,
    "upvote_ratio": 0.75,
    "num_comments": 1,
    "created_utc": 1730776855.0,
    "author": "Some-Technology4413",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gjxddu/98_of_companies_experienced_ml_project_failures/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gjbppp",
    "title": "Need 2 data sets. Food consumption and chronic disease.",
    "selftext": "Hello,\n\nI have a python data mining project, My proposed idea to my professor is \"Food consumption in relation with chronic disease\". To be able to do this project i need 2 data sets which i was not able to find easily as this is my first time touching on this subject.\n\nWhat i need if you could please supply me with 2 datasets or guide me where to get them.\n\n1-Food consumption\n\n2-Chronich disease\n\nAcross the world or a certain population like for example Africa, Asia ,Or USA.\n\n\n\nThanks in advance ;-).",
    "url": "https://www.reddit.com/r/data/comments/1gjbppp/need_2_data_sets_food_consumption_and_chronic/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1730718519.0,
    "author": "Bulky-Newspaper-857",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gjbppp/need_2_data_sets_food_consumption_and_chronic/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gj677n",
    "title": "Is there a (data-related) python package you want to see built? (I'll build and open source it)",
    "selftext": "Hi data friends!\n\nI'm looking for ideas on what python package to build. I'm thinking of a wrapper for public data APIs along with functions useful to manipulate the data, though I'm open to other ideas. Is there anything that you would find useful in your work that I could help build?\n\nI hope to build something useful (a package that people will actually pip install and use) to build up mt Github and practice my development skills. I'll update you once I've built it.\n\nDisclaimer: I am still early in my career, so the complexity of what I am able to build is limited.\n\nThank you for your suggestions!",
    "url": "https://www.reddit.com/r/data/comments/1gj677n/is_there_a_datarelated_python_package_you_want_to/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1730694498.0,
    "author": "jaquezmun",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gj677n/is_there_a_datarelated_python_package_you_want_to/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gj5rm0",
    "title": "Need help (dashboard)",
    "selftext": "I created a dashboard using streamlit in which theres a table element created using html, the table's cells contains all the visual and inner pivot tables inside it. \nThe problem is that i want to export this table and its contents as is to a word or pdf or export it to a image format. \nTo accomplish this i tried using html2canvas, but it won't work i don't know why.\n\nPlease suggest work arounds for this one.\nI know theres a built in print opt that streamlit offers but the point is i want to export only the visual table.",
    "url": "https://www.reddit.com/r/data/comments/1gj5rm0/need_help_dashboard/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1730692941.0,
    "author": "Hot-Description601",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gj5rm0/need_help_dashboard/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lvdc4t9",
        "body": "I don't know what streamlit is but if it is used to build websites, you could try using the build in print function of the browser. You can style it with css. Pretty annoying to work with but it should work for simple stuff.\n\n\nhttps://www.mediaevent.de/css/pagedmedia.html",
        "score": 1,
        "created_utc": 1730738119.0,
        "author": "Exact-Flounder1274",
        "is_submitter": false,
        "parent_id": "t3_1gj5rm0",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1giikcn",
    "title": "Automated logging for personal data",
    "selftext": "Hi, everyone! This is probably being asked a lot. I’m interested in tracking a variety of data categories in my daily life, but I’m struggling to keep everything organized without spending tons of time on manual logging. I've been logging for years on sheets but it is inconsistent and can get very overwhelming.\n\nI've thought about integrating apps / forms into a central log or using voice commands for quick notes, but I wonder if there's a better way to handle a larger range of categories with minimal effort. Does anyone have any experience with automating tracking of many categories from their life into a central dataset, calories, work hours, times peeing, conversations rated, number of drinks at a night out.... Really whatever.... Just very curious on how to make it simple and easy.\n\nFor those who track a lot of personal data, how do you manage it all? Would love any tips or insight",
    "url": "https://www.reddit.com/r/data/comments/1giikcn/automated_logging_for_personal_data/",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 1,
    "created_utc": 1730623494.0,
    "author": "Ynchr",
    "subreddit": "data",
    "permalink": "/r/data/comments/1giikcn/automated_logging_for_personal_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lv5gnc8",
        "body": "I use apple shortcuts combined with Data Jar app for my similar needs",
        "score": 2,
        "created_utc": 1730623741.0,
        "author": "Kindly_Wind_7261",
        "is_submitter": false,
        "parent_id": "t3_1giikcn",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1gi7lg7",
    "title": "Anyone know of an open source (free) api to access historical polling data?",
    "selftext": "",
    "url": "https://www.reddit.com/r/data/comments/1gi7lg7/anyone_know_of_an_open_source_free_api_to_access/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1730584805.0,
    "author": "ng_guru",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gi7lg7/anyone_know_of_an_open_source_free_api_to_access/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gh9lra",
    "title": "Who's getting polled?",
    "selftext": "The latest poll says...\n\nI'm not sure if this is the right sub for this, but I have a question: where do these news outlets & such get their polling data? For example with election day approaching, all of the news outlets are reporting that \"49% of voters...\", or \"so & so leads by however many points...\" etc.; even when you hear stats on preferences- for say, toothpaste (9/10 dentists agree) or regional halloween candy preferences as reported by NBC lol- Exactly WHERE is this data derived? How?  BUT DEFINITELY I'm specifically curious about all of this political polling.  I've never in my life been asked anything about anything for a poll. I don't know anyone who has! I don't know anybody that knows anybody that ever has. Lol so where are they getting this info?  Who are they asking? How? Where? ",
    "url": "https://www.reddit.com/r/data/comments/1gh9lra/whos_getting_polled/",
    "score": 2,
    "upvote_ratio": 0.75,
    "num_comments": 3,
    "created_utc": 1730478751.0,
    "author": "Birds_arent_real444",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gh9lra/whos_getting_polled/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lux4w0i",
        "body": "So the methodology is going to vary depending on who is collecting the data, but political polling usually relies on voter contact lists when surveying. This data is based on publicly available voter registration data which can include fields such as phone number, party affiliation, etc. Depending on how representative the data is, it may be supplemented by survey panel data (lists of people who sign up to take surveys). News websites may supplement the data with data from their readers/viewers.\n\nWhy haven't you been polled? It could be a variety of reasons. If you don't typically vote or haven't voted recently, you won't be on a voter contact list or your contact info may be out of date. If you don't typically take surveys or subscribe to news websites, you're likely not on any supplemental contact lists. If you're one of those people who doesn't answer phone calls from an out-of-state number, you may have been contacted but didn't pick up. Finally, the sample size they use for these polls is typically in the thousands, so there's just a good chance of not being called, especially if you fall into a demographic group that is easier to get sample from (i.e., older people as they are more likely to pick up a random phone call).",
        "score": 2,
        "created_utc": 1730495075.0,
        "author": "xtremeggnog",
        "is_submitter": false,
        "parent_id": "t3_1gh9lra",
        "depth": 0
      },
      {
        "id": "luxwlzx",
        "body": "Have you ever been polled?",
        "score": 1,
        "created_utc": 1730504795.0,
        "author": "Birds_arent_real444",
        "is_submitter": true,
        "parent_id": "t1_lux4w0i",
        "depth": 1
      },
      {
        "id": "luxy4yg",
        "body": "Yes, but I also work in market research so I am more actively engaged in answering unknown phone calls and taking surveys as it gives me a better understanding of my job.",
        "score": 1,
        "created_utc": 1730505353.0,
        "author": "xtremeggnog",
        "is_submitter": false,
        "parent_id": "t1_luxwlzx",
        "depth": 2
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1gh5uaq",
    "title": "Help",
    "selftext": "\nSo I took pictures last night on my Nikon Coolpix S1800. I go to look back at them- they’re all missing.  I got a notice earlier in the night that the memory drive was full so I deleted  some- not all of them and continued taking pictures. This morning I go to download them from my sd and I can’t find them. So I downloaded a recovery software and they were no where to be found, but old pictures from years ago were recovered. Is there any way for me to get those pictures from last night back? I’m so desprate ",
    "url": "https://www.reddit.com/r/data/comments/1gh5uaq/help/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1730468963.0,
    "author": "ImplementOver4101",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gh5uaq/help/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gh684n",
    "title": "What do you like to document, track, measure, or capture?",
    "selftext": "",
    "url": "https://www.reddit.com/r/data/comments/1gh684n/what_do_you_like_to_document_track_measure_or/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1730470004.0,
    "author": "Kaiser_design",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gh684n/what_do_you_like_to_document_track_measure_or/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gf7wyk",
    "title": "NEED HELP ASAP: G-RAID 1 Full",
    "selftext": "So I have the G-Technology G-Drive 40B set to RAID-1, meaning I have 2X 20TB HDDs in there that are a pure copy of one another.\n\nSo they are now full of my video/photo backups. I'm wanting to know if I can still use the enclosure with 2X NEW 20TB HDD's? Meaning, I want to know if it is okay to remove both FULL 2X OLD 20TB HDD's and keep them in storage if I ever need the media on them again.\n\n(Emphasis on keeping both as is so that I have 2X for redundancy). Then am I able to put 2X NEW 20TB HDD's in this same enclosure so I have a fresh RAID-1 to put NEW backups on?\n\nThen theoretically can I remove the 2X NEW HDD's and swap in the 2X OLD HDD's if I need to access my old files!?\n\nNote: I'm pretty new to RAID Storages, and I want to emphasize that I'm not asking to rebuild any HDD, just purely if it's safe/advisable to be able to use this enclosure as a 2X HDD bay where I can swap between 2 sets of 2 drives (total 4, and potentially more in the future) to be able to access media.",
    "url": "https://i.redd.it/ij4gjeoqyrxd1.jpeg",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 1,
    "created_utc": 1730242316.0,
    "author": "jshabnto",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gf7wyk/need_help_asap_graid_1_full/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lvmhpji",
        "body": "Theoretically I guess. unless you encrypt them with NTFS and format the system and have no recovery key. a friend did that. Look up raid swappabe?",
        "score": 1,
        "created_utc": 1730855752.0,
        "author": "marklar7",
        "is_submitter": false,
        "parent_id": "t3_1gf7wyk",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1gcwvxr",
    "title": "Bar chart race dataset",
    "selftext": "Where can I find datasets for a bar chart race? I've been looking for at least an hour and got no clue where can I find a proper one.",
    "url": "https://www.reddit.com/r/data/comments/1gcwvxr/bar_chart_race_dataset/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1729982455.0,
    "author": "Pulik3",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gcwvxr/bar_chart_race_dataset/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gccz8u",
    "title": "Dumb question about phone data",
    "selftext": "I have a phone plan with text, talk, and data. I also have an M3000-DFB6 Mifi that I use with my computer because I use a lot of data working online. I have a 100GB limit and I rarely run out. Computer and phone are not the same carrier. I usually use my landlord's Spectrum internet on the phone.\n\nQuestion: if I watch Netflix on my phone, using the wifi on the Mifi, am I using my phone plan's data, or the data from the Mifi?",
    "url": "https://www.reddit.com/r/data/comments/1gccz8u/dumb_question_about_phone_data/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1729916389.0,
    "author": "LadyA052",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gccz8u/dumb_question_about_phone_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gbhxpa",
    "title": "Is 91gb of downloaded data on an iPhone normal for one week?",
    "selftext": "Is this normal data usage ",
    "url": "https://www.reddit.com/r/data/comments/1gbhxpa/is_91gb_of_downloaded_data_on_an_iphone_normal/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1729816853.0,
    "author": null,
    "subreddit": "data",
    "permalink": "/r/data/comments/1gbhxpa/is_91gb_of_downloaded_data_on_an_iphone_normal/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "ltsejeh",
        "body": "For an everyday person? No.\n\nFor a porn addict? Also probably no.",
        "score": 2,
        "created_utc": 1729909849.0,
        "author": "ryan0585",
        "is_submitter": false,
        "parent_id": "t3_1gbhxpa",
        "depth": 0
      },
      {
        "id": "ltsko59",
        "body": "What could possibly use that much data she used 16gb in 1 hr",
        "score": 1,
        "created_utc": 1729912238.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t1_ltsejeh",
        "depth": 1
      },
      {
        "id": "ltsm42k",
        "body": "Facetiousness of my past answer aside, here's something I found online:\n\n\"According to Netflix, you use about 1GB of data per hour for streaming a TV show or movie in standard definition and up to 3GB of data per hour when streaming HD video.\"\n\nSo, streaming 91 GB in a week means 13 GB per day, or a little over 4 hours of HD video per day (13 hours of SD video). If one wasn't connected to WiFi and watching movies/shows from their phone each day (could be that someone doesn't know they're not connected to WiFi), I could see that happening.\n\nYou mention \"she\" in your response... I'm thinking here a daughter, girlfriend, wife, or something else? My advice is to ask and remind whoever it is to use WiFi, if that's the problem. If it's data usage on WiFi, don't worry about it, unless I'm missing something.\n\nEdit: As for 16GB on 1 hour, that sounds like downloading vs streaming. Some streaming services (like Netflix) allow you to download content to watch later. As you don't use the data at the (more limited) rate that it buffers/is viewed, you can download content faster than you would if you were watching it. Also possible other (likely video) content is being downloaded, IMO.",
        "score": 3,
        "created_utc": 1729912833.0,
        "author": "ryan0585",
        "is_submitter": false,
        "parent_id": "t1_ltsko59",
        "depth": 2
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1gbcobs",
    "title": "Multi-modal model for Unstructured data ",
    "selftext": "Hi, we are currently building a multi-modal model for accurate data extraction from unstructured data (such as PDFs, text, and images) aimed at enterprise applications in finance, retail and healthcare. We are already in design partnership with a couple of firms. Looking to add a few more. Please dm if you want us to make your data LLM ready and build custom workflows on top of it.",
    "url": "https://www.reddit.com/r/data/comments/1gbcobs/multimodal_model_for_unstructured_data/",
    "score": 2,
    "upvote_ratio": 0.75,
    "num_comments": 0,
    "created_utc": 1729802086.0,
    "author": "Confident_Dinner_872",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gbcobs/multimodal_model_for_unstructured_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gb5bd6",
    "title": "Seeking Recommendations for Gathering Data for Social Network Analysis",
    "selftext": "Hi everyone,\n\nI'm interested in conducting network analysis on a social network using graph theory. Could anyone recommend methods or tools for extracting data from social networks? Are there specific APIs or scraping techniques that are effective? Any advice on best practices would also be appreciated!\n\nThanks in advance!",
    "url": "https://www.reddit.com/r/data/comments/1gb5bd6/seeking_recommendations_for_gathering_data_for/",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1729783602.0,
    "author": "djoule53",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gb5bd6/seeking_recommendations_for_gathering_data_for/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "ltlg0ni",
        "body": "When I first got into the SNA space...a little over 10 years ago now...the large social network companies (particularly LinkedIn, Facebook and Twitter) were far more open in terms of their APIs and access to data. For both good and bad reasons that has changed so that it's more challenging to gain access to data. Twitter was relatively free and open until the price of API access increased significantly under Musk.   \n  \nThese days I would suggest starting with something like Mastodon where the data is available. In terms of tools for accessing the data, I would suggest you look at three:  \n\n1. [Communalytic](https://communalytic.org/) (see these docs for example: https://communalytic.org/docs/mastodon-data-structure/)\n2. [NodeXL](https://nodexl.com/)\n3. [VOSON](http://vosonlab.net/)\n\nOr....you could write some Python code (use ChatGPT if you need to) to collect the data via the Mastodon API. Or, just create a simple web crawler to collect the network data. \n\nOnce you have the data you could upload it into a tool like [Polinode](https://www.polinode.com) to analyse the networks and share them with others (full disclosure: I'm the founder of Polinode). You may also enjoy this webinar from a few years ago about pushing data from NodeXL to Polinode: [https://www.youtube.com/watch?v=6syIwTVbrt0](https://www.youtube.com/watch?v=6syIwTVbrt0)",
        "score": 2,
        "created_utc": 1729811251.0,
        "author": "androma",
        "is_submitter": false,
        "parent_id": "t3_1gb5bd6",
        "depth": 0
      },
      {
        "id": "lubm4ne",
        "body": "You used to be able to pull a lot of Twitter data with quite ease. Not, you need to rely on 3rd party solutions. Check [apify.com](http://apify.com) for their actors that can do that for you without you having to write the code.",
        "score": 2,
        "created_utc": 1730194078.0,
        "author": "Significant_Shop2751",
        "is_submitter": false,
        "parent_id": "t3_1gb5bd6",
        "depth": 0
      },
      {
        "id": "ltvoz4v",
        "body": "Thank you, I will look up your recommendations and your tool. :)",
        "score": 1,
        "created_utc": 1729963812.0,
        "author": "djoule53",
        "is_submitter": true,
        "parent_id": "t1_ltlg0ni",
        "depth": 1
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1gayys3",
    "title": "Getting data from sites like Twitch, YouTube, etc. for university project",
    "selftext": "I am currently doing a Data Science degree at university, and for our Visualisation class, we have been permitted to acquire the data for the project ourselves and decide on the research topic. \n\nI am very interested in content creators, streamers and content-consumers. So i figured I wanted to try and create some beautiful visualisation using data from something like YouTube, Twitch, TikTok or similar. \n\nHowever, I have a question that i am hoping someone can help me with. \n\nI am unsure how to get data of these platforms? I am specifically thinking about sites like [Twitchtracker.com](http://Twitchtracker.com) and [Track YouTube analytics, future predictions, & live subscriber counts - Social Blade](https://socialblade.com/youtube/). How do these sites ingest the data from the platforms? \n\nDo they just do continual scraping of the sites, and then create their data products that way, or do they use the API provided by the sites? \n\nI am unsure, because i tried reading a little bit into the API provided by YouTube and Twitch, but they seem like they a specifically targeted toward channel owners, and it made me wonder If its even possible to get the data from twitch about other channels if you are not the owner of the content, ie. \n\nIn the example about twitch, some interesting data could be:   \nStream time, games streamed, followers, following, etc. \n\nThank you kindly!",
    "url": "https://www.reddit.com/r/data/comments/1gayys3/getting_data_from_sites_like_twitch_youtube_etc/",
    "score": 2,
    "upvote_ratio": 0.67,
    "num_comments": 0,
    "created_utc": 1729764049.0,
    "author": "BadBroBobby",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gayys3/getting_data_from_sites_like_twitch_youtube_etc/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gaysfb",
    "title": "Downloading data as csv or xlsx",
    "selftext": "Hey, I am looking at data from celebrity private jet tracker. Com\nDoes somebody know if and how I can extract the data as a csv or xlsx format? \nIt's for an essay at uni\nThanks :) ",
    "url": "https://www.reddit.com/r/data/comments/1gaysfb/downloading_data_as_csv_or_xlsx/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1729763299.0,
    "author": "julebest",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gaysfb/downloading_data_as_csv_or_xlsx/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "ltn6vos",
        "body": "If you know a little coding you can use python with the requests and beautifulsoup libraries to scrape the data off the site.",
        "score": 1,
        "created_utc": 1729837436.0,
        "author": "WhatAGuy765",
        "is_submitter": false,
        "parent_id": "t3_1gaysfb",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1gb1hwn",
    "title": "Data Assimilation (Particle Filtering)",
    "selftext": "Anybody knows how to run multiple parameter estimation using particle Filter?",
    "url": "https://www.reddit.com/r/data/comments/1gb1hwn/data_assimilation_particle_filtering/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1729773092.0,
    "author": "fafafa3412",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gb1hwn/data_assimilation_particle_filtering/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gahffr",
    "title": "What's the consensus on how Snapchat stores and sees our data?",
    "selftext": "I know this question might be overdone. But I know that in many instances they can provide meta data, and even the content of snaps by eavesdropping if notified by a warrant before the snap is sent.\nHowever I wonder if when people say our data and snaps are never truly deleted do they mean the actual picture and words. Or just the meta data exposing we HAD a conversation or exchange. I can't imagine Snapchat servers would be able to pull up the actual content of a snap I sent a week ago. I do believe the meta data is there about the photo.",
    "url": "https://www.reddit.com/r/data/comments/1gahffr/whats_the_consensus_on_how_snapchat_stores_and/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1729707323.0,
    "author": "lubbom",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gahffr/whats_the_consensus_on_how_snapchat_stores_and/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1gak40h",
    "title": "Hi, I wanted to engage in some amateur journalism and am curious about scraping information from the web and doing entity analysis",
    "selftext": "I'm looking for guidance on conducting a research project that investigates some behaviors I've observed in the video game streaming community, particularly concerning authenticity and perceived excitement. I've noticed an influx of overly positive reviews for certain products that seem uninspiring, raising questions about potential conflicts of interest at play in the generation of content.\n\nI want to explore how many gaming companies have shifted their C-suite to include primarily ex-Hollywood professionals, suggesting that aggressive marketing may be overshadowing creative direction and quality. My plan is to scrape YouTube titles related to these companies' games before and after the shift and analyze the positive versus negative language used in those titles.\n\nWhile this research won’t establish causation, I suspect it may reveal a troubling trend in the gaming industry that mirrors the film industry, where budgets are increasingly diverted from actual game development to advertising. This shift could boost sales in the short term but harm longevity and replay-ability. I’d love any advice or resources on how to approach this project effectively!\n\nBULLETTED BREAKDOWN;\n\nI'm seeking guidance on conducting a research project focused on behaviors in the video game streaming community. Here are the key points:\n\n* **Observation**: I’ve noticed certain behaviors in the streaming community that raise questions about authenticity and excitement.\n* **Concerns**: Many products receive overwhelmingly positive impressions despite seeming uninspiring, suggesting potential conflicts of interest.\n* **Research Idea**:\n   * Investigate how many gaming companies have shifted their C-suite to primarily ex-Hollywood executives.\n   * This shift may indicate that aggressive marketing is taking precedence over creative direction and quality.\n   * Plan to scrape YouTube titles related to these companies’ games before and after the leadership change.\n   * Conduct an entity analysis of positive vs. negative language used in those titles.\n* **Hypothesis**: Although this won’t prove causation, I suspect it may reveal a troubling trend in the gaming industry, similar to the film industry, where budgets are diverted from game development to advertising.\n\nI’d appreciate any advice or resources on how to approach this project effectively!",
    "url": "https://www.reddit.com/r/data/comments/1gak40h/hi_i_wanted_to_engage_in_some_amateur_journalism/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1729714354.0,
    "author": "DeffN0tAndy",
    "subreddit": "data",
    "permalink": "/r/data/comments/1gak40h/hi_i_wanted_to_engage_in_some_amateur_journalism/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ga1og1",
    "title": "Data Quality Checker",
    "selftext": "Upload a CSV, drag and drop field types, quickly analyze data to see what rows are invalid (click the respective percent to view the invalid rows for the respective column)   \n  \nI realized looking at data quality isn't as streamlined as it could be, etc standardized initial quality assessment. I made this early stage POC tool that helps get a quick view of data quality based on field types.  \n\nWould this be valuable for the data science community? Are there any additional features that would improve it? What would make a tool like this more valuable? \n\n[https://checkalyze.github.io/](https://checkalyze.github.io/)\n\nThank you for any feedback. ",
    "url": "https://www.reddit.com/r/data/comments/1ga1og1/data_quality_checker/",
    "score": 1,
    "upvote_ratio": 0.99,
    "num_comments": 9,
    "created_utc": 1729656343.0,
    "author": null,
    "subreddit": "data",
    "permalink": "/r/data/comments/1ga1og1/data_quality_checker/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "ltavyru",
        "body": "yep, as new possibile features on data type:  \n\\- url  \n\\- tags (as particular characters data)",
        "score": 1,
        "created_utc": 1729664123.0,
        "author": "nelsonmau",
        "is_submitter": false,
        "parent_id": "t3_1ga1og1",
        "depth": 0
      },
      {
        "id": "ltbs8db",
        "body": "Interesting. Are you also thinking of running custom test cases to be more effective.",
        "score": 1,
        "created_utc": 1729684182.0,
        "author": "srikon",
        "is_submitter": false,
        "parent_id": "t3_1ga1og1",
        "depth": 0
      },
      {
        "id": "ltbxbp2",
        "body": "Validating data types, acceptable values in the column etc.",
        "score": 1,
        "created_utc": 1729686397.0,
        "author": "srikon",
        "is_submitter": false,
        "parent_id": "t3_1ga1og1",
        "depth": 0
      },
      {
        "id": "ltawahc",
        "body": "what do you mean tags as particular characters data?",
        "score": 1,
        "created_utc": 1729664328.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t1_ltavyru",
        "depth": 1
      },
      {
        "id": "ltbw8gd",
        "body": "like custom data fields that can be applied?",
        "score": 1,
        "created_utc": 1729685939.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t1_ltbs8db",
        "depth": 1
      },
      {
        "id": "ltbz2vd",
        "body": "i see. unless a supported field type, i think that would take a custom field apply from the user since it’s arbitrary — which i still don’t think is something that exists as a data quality assessment tool. this is intended to be preliminary, quick light and simple — obviously not a full data analysis suite",
        "score": 1,
        "created_utc": 1729687115.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t1_ltbxbp2",
        "depth": 1
      },
      {
        "id": "ltawfoj",
        "body": "kind of classification",
        "score": 1,
        "created_utc": 1729664418.0,
        "author": "nelsonmau",
        "is_submitter": false,
        "parent_id": "t1_ltawahc",
        "depth": 2
      },
      {
        "id": "ltawhni",
        "body": "what would be an example of this?",
        "score": 1,
        "created_utc": 1729664453.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t1_ltawfoj",
        "depth": 3
      },
      {
        "id": "ltaxczh",
        "body": "I have a csv, I have a column with strings that I use for classification (e.g., \"tag1, tag2, tag3\"), and I want to check the consistency of the values within this column, right?",
        "score": 1,
        "created_utc": 1729665003.0,
        "author": "nelsonmau",
        "is_submitter": false,
        "parent_id": "t1_ltawhni",
        "depth": 4
      }
    ],
    "comments_extracted": 9
  },
  {
    "id": "1g9xhqn",
    "title": "API and connect to google sheets",
    "selftext": "Hii! I'm not really sure if I'm in the right sub. Can you all help me on how I can connect an API to my Google Sheets/Excel? I use a chrome extension for API but feel free to suggest free API. So technically I need the following:\n- number of views, likes, and comments\n- used captions\n- upload date\n- creator's name\n\nAll of these are from different sources or links. I don't know how to make a workflow out of it. \n\n",
    "url": "https://www.reddit.com/r/data/comments/1g9xhqn/api_and_connect_to_google_sheets/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1729643216.0,
    "author": "fleureahhh__",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g9xhqn/api_and_connect_to_google_sheets/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1g8tgr2",
    "title": "Buyer intent data enrichment ",
    "selftext": "I have lists already.  Can anyone recommend a service that will enrich my data by buyer intent",
    "url": "https://www.reddit.com/r/data/comments/1g8tgr2/buyer_intent_data_enrichment/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1729526991.0,
    "author": "rocketman11111",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g8tgr2/buyer_intent_data_enrichment/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1g89hbv",
    "title": "Building a CSV file ingestion pipeline where uploaded statement column headers constantly keep changing?",
    "selftext": "I have a use case that I am working on where customers normally upload financial statements from payment aggregators and banks. Now, I have my own internal financial model and I am trying to find a way to handle this inconsistent data and map the data to my financial model. I would like to understand what would be a good way to create a mapping such that I can handle this problem well and scale/support multiple customers.\n\nFYI - The uploaded statement goes to S3 for storage and then I am using Snowflakes to store the data in a table. My issue is the changing column headers that varies across different processors/banks.",
    "url": "https://www.reddit.com/r/data/comments/1g89hbv/building_a_csv_file_ingestion_pipeline_where/",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 3,
    "created_utc": 1729459442.0,
    "author": "Django-Ninja",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g89hbv/building_a_csv_file_ingestion_pipeline_where/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lsz96x8",
        "body": "i had some similar problems. i di not solve it in prod, my plan was to load stuff as json row, and then write sql+json query with lots of case clauses to map that shit into my model. \n\nloading csv row as json , solves problem that you dont have to create new table for each csv schema, then you can solve schema differences with snowflakes variant type and json functions.\n\nthis way you can have one table for raw, then your transofmration can be one or multiple queries that extracts particular format into your model.",
        "score": 1,
        "created_utc": 1729501585.0,
        "author": "throw_mob",
        "is_submitter": false,
        "parent_id": "t3_1g89hbv",
        "depth": 0
      },
      {
        "id": "lt0jxb7",
        "body": "I actually have a similar problem that I imposed a working solution in prod for. I use python and read in the data file to a dataframe. I then have a separate mapping document saved in a location available to the client team and myself, which maps the fields on the incoming file to the fields in our system. The client team can update these fields as needed, but it is unfortunately specific to each client.\n\nFor generic purposes, if your system is set up in such a way that you have some sort of description field in your tables you can use to match up the incoming data with, you can simply have a mapping for each possible incoming field. Im not sure if that would fit your needs, but I would imagine there wouldn’t be an infinite number of fields possible for them to send to you right? The number surely would be finite?",
        "score": 1,
        "created_utc": 1729522921.0,
        "author": "AppropriateBeing9526",
        "is_submitter": false,
        "parent_id": "t3_1g89hbv",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1g88b6e",
    "title": "Above ground storage tanks",
    "selftext": "Where can I find data on the quantity and location of above ground petroleum storage tanks in the US and Canada?",
    "url": "https://www.reddit.com/r/data/comments/1g88b6e/above_ground_storage_tanks/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1729456320.0,
    "author": "NefariousnessWeak475",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g88b6e/above_ground_storage_tanks/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1g7ctbc",
    "title": "Future of big data",
    "selftext": "",
    "url": "https://i.redd.it/0fdz15f9qqvd1.jpeg",
    "score": 9,
    "upvote_ratio": 0.84,
    "num_comments": 3,
    "created_utc": 1729355663.0,
    "author": "cursingpeople",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g7ctbc/future_of_big_data/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lsphiv3",
        "body": "How old is this?",
        "score": 3,
        "created_utc": 1729356204.0,
        "author": "BackstabAssist3",
        "is_submitter": false,
        "parent_id": "t3_1g7ctbc",
        "depth": 0
      },
      {
        "id": "lsz3fkx",
        "body": "It talks about 2020 so must be a few years before. I remember how religiously I learned the 5 v’s of f big data and all the crazy big query, mapR and what not , so many hours into it that just dissolved into nothing when this stuff became Saas . Now, no one cares about it unless you are a 17 yo guru with 3 doctorates in comsci , genetics and ai.",
        "score": 1,
        "created_utc": 1729497485.0,
        "author": "mullerjannie",
        "is_submitter": false,
        "parent_id": "t3_1g7ctbc",
        "depth": 0
      },
      {
        "id": "lsz312d",
        "body": "lol yes my thoughts too, feels like back in. 2016",
        "score": 1,
        "created_utc": 1729497200.0,
        "author": "mullerjannie",
        "is_submitter": false,
        "parent_id": "t1_lsphiv3",
        "depth": 1
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1g6akzr",
    "title": "How to filter real emails vs bot emails?",
    "selftext": "My boss asked me to find the ratio between genuine emails vs bot emails collected from the discount plugin on Shopify. \nI can see there are overall 3k+ emails and I'm working on combining each csv file into on sheet (suggestions are welcome).\n\nBut I want to know how I can figure out which emails are real and not temp mails from the database?",
    "url": "https://www.reddit.com/r/data/comments/1g6akzr/how_to_filter_real_emails_vs_bot_emails/",
    "score": 2,
    "upvote_ratio": 0.67,
    "num_comments": 0,
    "created_utc": 1729228129.0,
    "author": "wolfandthesheep31",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g6akzr/how_to_filter_real_emails_vs_bot_emails/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1g63h8n",
    "title": "Converting verticle list to table in Sheets",
    "selftext": "Hi all, I have a large data set that is currently a vertical list in Sheets (each data point is an individual cell, all in column A) and I need help turning it into a table with 6 columns. I've tried a couple different transposition and array formula codes and I can't seem to get it to work :( any help would be greatly appreciated!",
    "url": "https://www.reddit.com/r/data/comments/1g63h8n/converting_verticle_list_to_table_in_sheets/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1729204788.0,
    "author": "Own-Sorbet1776",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g63h8n/converting_verticle_list_to_table_in_sheets/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lsgnc4x",
        "body": "I don't understand what you mean. Like row 1, 7, 13... Should go in column A then 2,8,14 in column B?\nOr do you have the right number of rows but the 6 pieces of information are consecutive in one cell? Or separated by something?",
        "score": 1,
        "created_utc": 1729216127.0,
        "author": "PracticalPlenty7630",
        "is_submitter": false,
        "parent_id": "t3_1g63h8n",
        "depth": 0
      },
      {
        "id": "lt3d8zj",
        "body": "Info on the format of the data (is there unique traits that determine which of the 6 columns each record should be moved to?) would be useful.\n\nYou can always add some if statements to the neighboring cell with the following pseudo logic IFS(<1stCellWithData> contains \"<unique value for column n1>\", \"<column n1 name>\", <2ndCellWithData> contains \"<unique value for column n2>\",\"<column n2 name>\", ... <6thCellWithData> contains \"<unique value for column n6>\",\"<column n6 name>\"). Then apply that to the entire column.\n\nThen, in the first cell of each column, add an xlookup(\"column name\", <range of data neighboring cells with if results>,<range off data>).\n\nThen select the entire table, copy, paste as values so the new dataset is not dependent on the formulas. Clean up as needed.",
        "score": 1,
        "created_utc": 1729555161.0,
        "author": "Impressive_Roll1840",
        "is_submitter": false,
        "parent_id": "t3_1g63h8n",
        "depth": 0
      },
      {
        "id": "lt8yn39",
        "body": "Can you post a sample SS on how the data looks like and how you want it? Do you want to split one column into 6? For eg:; data in col 1  to col 6? \nHow do you want to achieve? \nIf so, find out the separator and split the column using the separator I.e., separators could be tab. Space or comma.",
        "score": 1,
        "created_utc": 1729636194.0,
        "author": "Amazing-Cupcake-3597",
        "is_submitter": false,
        "parent_id": "t3_1g63h8n",
        "depth": 0
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1g615nj",
    "title": "A question ",
    "selftext": "I apologize if this is a) stupid, or b) has been asked before. \n\nWith the sheer amount of data we have on the histories of civilizations and the different variables that led to their rises and downfalls, shouldn’t there be an almost objective answer to how a society should govern itself?\n\nEconomics, for example. Shouldn’t we have enough sheer data on different economic systems and their success rates to have a definitive answer for the perfect system?",
    "url": "https://www.reddit.com/r/data/comments/1g615nj/a_question/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1729198436.0,
    "author": "sora_979",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g615nj/a_question/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lt3fph2",
        "body": "I feel it.\n\nBut having data and data being usable are two different things. Not that much of human created data has been digitalized. Even if it were, formatting/encoding/database-structure/maintenance could cause limitations on access for different institutions or at all.\n\nThere are also many cultural factors in why this is not currently possible, but I would have to speculate.\n\nThe economic reasons are more important. Data has value. So by nature, it would be costly to access privately held data on a large scale. Especially as such an effort would drive up demand further than AI already has. It would also require a good faith coordinated effort across many nations to engage in a long term project.\n\nMy biggest point on this would be that Economics, while a valuable and important field of study, is still in its infancy. Richard Thaler won the 2017 nobel prize research showing that people don't always act in their own best interest. My understanding is that was a large assumption underpinning much of economic research/application. Economics is not the only field to continue to suffer from early assumptions (e.g. psychology)\n\nAnd lastly there is data literacy. Policy could not be set based on such a complex data set when we are already pretty bad at setting policy on the limited data sets we do possess.\n\nEverything is a work in progress always. The perfect system does not exist.",
        "score": 2,
        "created_utc": 1729556072.0,
        "author": "Impressive_Roll1840",
        "is_submitter": false,
        "parent_id": "t3_1g615nj",
        "depth": 0
      },
      {
        "id": "lt5k18z",
        "body": "Ah, you’re right. I work so often with data sets I forgot to include the human element. Thank you!",
        "score": 1,
        "created_utc": 1729595265.0,
        "author": "sora_979",
        "is_submitter": true,
        "parent_id": "t1_lt3fph2",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1g4qnp2",
    "title": "Very messy location data",
    "selftext": "Hi there,\n\nI'm currently using some publicly available data to expand my data analytics skills. There are over 80k rows in the table and I've challenged myself to try and clean this up.\n\nIt seems no clear prompt was given for the operating location field and some are just countries, some are street addresses, some have multiple countries and some have a combination of all of the above!\n\nCan anyone recommend how to clean this data up? \n\nMany thanks in advance!",
    "url": "https://i.redd.it/ot9t77p5k1vd1.jpeg",
    "score": 15,
    "upvote_ratio": 0.86,
    "num_comments": 31,
    "created_utc": 1729050939.0,
    "author": "trooynathan",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g4qnp2/very_messy_location_data/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "ls5x8xk",
        "body": "Oh, simple:\n\nTry not to let any of the visuals have a need for the location data, thereby letting that field banish into the hellscape it is, and ultimately, ignored😬",
        "score": 10,
        "created_utc": 1729060706.0,
        "author": "Colton200456",
        "is_submitter": false,
        "parent_id": "t3_1g4qnp2",
        "depth": 0
      },
      {
        "id": "ls6i96w",
        "body": "Taking a screen shot is a lost art.",
        "score": 7,
        "created_utc": 1729074981.0,
        "author": "RedRedditor84",
        "is_submitter": false,
        "parent_id": "t3_1g4qnp2",
        "depth": 0
      },
      {
        "id": "ls5kqwn",
        "body": "Oof at least this is self imposed and not required for work. If anything this is a cautionary tale for people who design and develop forms. If I HAD to do this I would start cleaning the messiest data manually until I could at least be sure that there were no spelling mistakes. Then get a list of suburbs and wildcard search. Clean up where there are no matches. Unless you literally cleanse every single row you'd probably never get anything better than suburb and state from this\n\nYou can't do this kind of fixing in powerbi. Cleanse the data then re-ingest",
        "score": 6,
        "created_utc": 1729053332.0,
        "author": "Cyraga",
        "is_submitter": false,
        "parent_id": "t3_1g4qnp2",
        "depth": 0
      },
      {
        "id": "ls5w1kj",
        "body": "I would try Google's Address Validation API to identify rows that should be removed from the dataset or corrected:\n\nhttps://googleapis.dev/python/addressvalidation/latest/\n\nNext step would be to find an API to take an address and split it into country, city, street etc, I assume google has that too",
        "score": 2,
        "created_utc": 1729059921.0,
        "author": "fartGesang",
        "is_submitter": false,
        "parent_id": "t3_1g4qnp2",
        "depth": 0
      },
      {
        "id": "ls68574",
        "body": "Have you got longitudes and latitudes for each location? If so you’ll have more luck with that. Thinking some way you could get AI to run through the Lon/Lat data and give you the city/state/country but that wouldn’t be a job for Power Query and it would take a veeery long time. \n\nHow granular is your analysis going to be? if it’s just on a country level you might be okay. Beyond that you’re going to have a very hard time with that column.",
        "score": 2,
        "created_utc": 1729068241.0,
        "author": "Rosskillington",
        "is_submitter": false,
        "parent_id": "t3_1g4qnp2",
        "depth": 0
      },
      {
        "id": "ls5or0j",
        "body": "What is the objective here?\n\nDo you need street address?\n\nDo you want to preserve maximum info?",
        "score": 1,
        "created_utc": 1729055501.0,
        "author": "maplemaple2024",
        "is_submitter": false,
        "parent_id": "t3_1g4qnp2",
        "depth": 0
      },
      {
        "id": "ls61th9",
        "body": "Hey, \nFrom my PoV, I won’t use PBI to clean up such a messy data. What’s your data volume. How many records do you have in this dataset?",
        "score": 1,
        "created_utc": 1729063779.0,
        "author": "Amazing-Cupcake-3597",
        "is_submitter": false,
        "parent_id": "t3_1g4qnp2",
        "depth": 0
      },
      {
        "id": "ls6k62h",
        "body": "Not sure what your end goal is, but I build a simple python script to loop through each and fix. Maybe Gemini-1.5-Flash-002(super cheap or free credits) or llama3.2 (local and free).  Both models do json outputs very well and are free or very low cost.",
        "score": 1,
        "created_utc": 1729076077.0,
        "author": "dannyboy2042",
        "is_submitter": false,
        "parent_id": "t3_1g4qnp2",
        "depth": 0
      },
      {
        "id": "ls6u2wc",
        "body": "You said you're interested in the country + province/state/territory\nI would:\n1) put the data in a pandas data frame in Python or in a SQL table via extracting a  CSV from power BI\n2) I would had 2 empty columns: country; state\n3) with text function I would search and fill those two columns and ex. if the text contains upper(location) LIKE '%AUSTRA%' then Country='Australia'\n4) continue null value  by null value until there are no more lines with empty values for your two columns",
        "score": 1,
        "created_utc": 1729080964.0,
        "author": "PracticalPlenty7630",
        "is_submitter": false,
        "parent_id": "t3_1g4qnp2",
        "depth": 0
      },
      {
        "id": "ls7btq6",
        "body": "Firstly, go in excel and split every word, you can do this by text to columns. This is in data tab make sure you click on delimited and then check space delimiter and uncheck every thing else . \nThis will separate your state and countries which can give a good start to work with",
        "score": 1,
        "created_utc": 1729087847.0,
        "author": "Fancy_Contact_8078",
        "is_submitter": false,
        "parent_id": "t3_1g4qnp2",
        "depth": 0
      },
      {
        "id": "ls8unqe",
        "body": "It looks like there's longitude and latitude. You might be able to get a specific location from that?",
        "score": 1,
        "created_utc": 1729105345.0,
        "author": "Zanoth13",
        "is_submitter": false,
        "parent_id": "t3_1g4qnp2",
        "depth": 0
      },
      {
        "id": "ls5hr0q",
        "body": "Can you derived the actual location by doing a script that gets the coordinates (lat, long)? Although I'm not sure if M language is capable of that",
        "score": 1,
        "created_utc": 1729051792.0,
        "author": "Kind_Cow7817",
        "is_submitter": false,
        "parent_id": "t3_1g4qnp2",
        "depth": 0
      },
      {
        "id": "ls6krxh",
        "body": "Thanks for your post, I have exactly the same problem. Does someone know how is possible to solve using python or SQL?",
        "score": 1,
        "created_utc": 1729076414.0,
        "author": "c8rd",
        "is_submitter": false,
        "parent_id": "t3_1g4qnp2",
        "depth": 0
      },
      {
        "id": "ls5grsc",
        "body": "To add: I'm using power bi",
        "score": 0,
        "created_utc": 1729051307.0,
        "author": "trooynathan",
        "is_submitter": true,
        "parent_id": "t3_1g4qnp2",
        "depth": 0
      },
      {
        "id": "ls65808",
        "body": "hahaha perfect response",
        "score": 1,
        "created_utc": 1729066196.0,
        "author": "trooynathan",
        "is_submitter": true,
        "parent_id": "t1_ls5x8xk",
        "depth": 1
      },
      {
        "id": "ls6nz03",
        "body": "Agreed. PRTSC button on Windows, or Command-shift-3 on Mac.... if anybody is curious. I'm bot sure about other OS's.\n\nI suppose there are some instances where the software could disable this function.",
        "score": 1,
        "created_utc": 1729078088.0,
        "author": "scottdave",
        "is_submitter": false,
        "parent_id": "t1_ls6i96w",
        "depth": 1
      },
      {
        "id": "ls6p3o3",
        "body": "I'll give you the credit but I don't use reddit on my PC for some weird reason",
        "score": 1,
        "created_utc": 1729078646.0,
        "author": "trooynathan",
        "is_submitter": true,
        "parent_id": "t1_ls6i96w",
        "depth": 1
      },
      {
        "id": "ls5mwqj",
        "body": "so what do you do about people inputting garbage?",
        "score": 1,
        "created_utc": 1729054482.0,
        "author": "andylikescandy",
        "is_submitter": false,
        "parent_id": "t1_ls5kqwn",
        "depth": 1
      },
      {
        "id": "ls5pvme",
        "body": "The general objective is to clean the location as best as possible. This isn't for a job/university. I'm just practicing.\n\nPreservation of maximum information is not required. Ideally I would keep state/territory/province & country",
        "score": 0,
        "created_utc": 1729056135.0,
        "author": "trooynathan",
        "is_submitter": true,
        "parent_id": "t1_ls5or0j",
        "depth": 1
      },
      {
        "id": "ls656i8",
        "body": "Hi there, thanks for the response.\nThere are over 40,000 records in this dataset.",
        "score": 1,
        "created_utc": 1729066166.0,
        "author": "trooynathan",
        "is_submitter": true,
        "parent_id": "t1_ls61th9",
        "depth": 1
      },
      {
        "id": "ls7c0eq",
        "body": "Also, what are you trying to achieve after cleaning this data ? Next steps depend on that",
        "score": 1,
        "created_utc": 1729087911.0,
        "author": "Fancy_Contact_8078",
        "is_submitter": false,
        "parent_id": "t1_ls7btq6",
        "depth": 1
      },
      {
        "id": "ls5ifkd",
        "body": "That seems like a logical solution. However, some of the coordinates are for the country, whereas others are more localized.\n\n(Using Power BI)",
        "score": 0,
        "created_utc": 1729052135.0,
        "author": "trooynathan",
        "is_submitter": true,
        "parent_id": "t1_ls5hr0q",
        "depth": 1
      },
      {
        "id": "ls6yoc3",
        "body": "There’re so many variables here…how big is your list of distinct values? If it were me I’d dump my list into an LLM, ask it to return lat/long coordinates for each location inputted and call it a day. Otherwise you have to write some wild decision tree in python or sql to try and determine a location based on differing levels of specificity (address level? County level? Province level?) AND you have to deal with bullshit like abbreviations…it’s just a ton of headache at that point",
        "score": 1,
        "created_utc": 1729082926.0,
        "author": "CheeseDog_",
        "is_submitter": false,
        "parent_id": "t1_ls6krxh",
        "depth": 1
      },
      {
        "id": "ls7lfnu",
        "body": "Win+Shift+S is the modern way on Windows!",
        "score": 1,
        "created_utc": 1729091054.0,
        "author": "RedRedditor84",
        "is_submitter": false,
        "parent_id": "t1_ls6nz03",
        "depth": 2
      },
      {
        "id": "ls5q1za",
        "body": "This is publicly available data from a government body I'm just using to practice with. \n\nObviously it would be ideal to have separate fields for address, postcode, suburb, state/territory and country.",
        "score": 3,
        "created_utc": 1729056239.0,
        "author": "trooynathan",
        "is_submitter": true,
        "parent_id": "t1_ls5mwqj",
        "depth": 2
      },
      {
        "id": "lsaqtlf",
        "body": "Make it impossible to input garbage",
        "score": 1,
        "created_utc": 1729129210.0,
        "author": "Cyraga",
        "is_submitter": false,
        "parent_id": "t1_ls5mwqj",
        "depth": 2
      },
      {
        "id": "ls5rdfv",
        "body": "'Clean the location' is subjective.\n\nexample-\n\n1. If the data is for house prices, you don't need street address. Locality/Neighborhood information is enough. \n\n2. if this is crime/accidents reported, then you need hierarchy.\n\n3. If you want to see producs orderred, and plan routes/warehouse capacities. Then filling missing data becomes crucial and you need street addresses.\n\n  \nDraft a problem statement/objective and then tackle the problem.\n\nComment other column names",
        "score": 3,
        "created_utc": 1729057022.0,
        "author": "maplemaple2024",
        "is_submitter": false,
        "parent_id": "t1_ls5pvme",
        "depth": 2
      },
      {
        "id": "lsd308d",
        "body": "Easy solved. You've got perfectly fine Lat Long columns there.\n\nhttp://download.geonames.org/export/zip/\n\nGet the AU dataset\n\nNew worksheet  \n\nInsert data  \n\nVLOOKUP( )  \n\nUse the Lat and Long columns and replace the Post Code names in your main sheet.",
        "score": 1,
        "created_utc": 1729173024.0,
        "author": "No_Vermicelliii",
        "is_submitter": false,
        "parent_id": "t1_ls5pvme",
        "depth": 2
      },
      {
        "id": "ls6vlv4",
        "body": "Okay! Here is what I’ll try to do:\n1. Use python to load the data\n2. Clean and transform using the basic functions (for eg: splitting of columns based on character)\n3. Use value_counts() to understand the data points in each column \n4. Remove the null values and encrypted characters. \n5. Finally slice the data frame based on the use case and retain only the columns I need.\n6. Export the data as a csv file (which can then be loaded to PBI).\nThe above points are very basic and can be done by even non python users. That’s the only intention.\n\nPoweBI is my bread and butter. I use it at work everyday. Unfortunately I will not use PBI to clean up my dataset. Hence I’m refraining from suggesting you the PBI steps as it will increase the load on your data model and decrease the efficiency :)\n\nHope it helps. Happy learning!",
        "score": 2,
        "created_utc": 1729081637.0,
        "author": "Amazing-Cupcake-3597",
        "is_submitter": false,
        "parent_id": "t1_ls656i8",
        "depth": 2
      },
      {
        "id": "lsbi4vr",
        "body": "Does inputting a random address from across town (or whatever arbitrary location) count as garbage?",
        "score": 1,
        "created_utc": 1729141322.0,
        "author": "andylikescandy",
        "is_submitter": false,
        "parent_id": "t1_lsaqtlf",
        "depth": 3
      },
      {
        "id": "lsbj9jz",
        "body": "It's poor quality data, but I can't tell so I don't mind 😅",
        "score": 1,
        "created_utc": 1729141947.0,
        "author": "Cyraga",
        "is_submitter": false,
        "parent_id": "t1_lsbi4vr",
        "depth": 4
      }
    ],
    "comments_extracted": 31
  },
  {
    "id": "1g52e90",
    "title": "Whats the most eficient process or platform for finding and exporting data on commercial real estate owners in a specific state, and over 10k square feet?",
    "selftext": "CoStar is suepr expensive and other services dont allow you to export all properties. eg, Reonomy found several hundred properties but only lets you export 5 at a time into excel.  \n\nDoes anyone know of a service or a hack for identifying all commercial properties in a given state that are greater than 10k sf, that will give me:\n\n- Owner name  \n- Facility maintenance director name (If possible)  \n- Phone number  \n- Email address  \n- APN of property",
    "url": "https://www.reddit.com/r/data/comments/1g52e90/whats_the_most_eficient_process_or_platform_for/",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 2,
    "created_utc": 1729093275.0,
    "author": "Cultural-Bathroom01",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g52e90/whats_the_most_eficient_process_or_platform_for/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lt3gcnj",
        "body": "Some County Auditor websites have API's, but that would be limited by county and not all counties will provide such services.",
        "score": 1,
        "created_utc": 1729556311.0,
        "author": "Impressive_Roll1840",
        "is_submitter": false,
        "parent_id": "t3_1g52e90",
        "depth": 0
      },
      {
        "id": "lujs217",
        "body": "Actovia will give property and owner data. Pretty much all of the above. Able to bulk export.",
        "score": 1,
        "created_utc": 1730307218.0,
        "author": "Such_Pie8596",
        "is_submitter": false,
        "parent_id": "t3_1g52e90",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1g4whrx",
    "title": "Switching from developer to Data roles",
    "selftext": "I want to switch from software development to data analyst or data engineering role and I just want to know that in India, let's say I am in Kolkata, so what kind of package I might get with the data analyst role and if I want to switch to data engineering then what might be the salary I can get? As I have started with python and SQL, and planning to learn some other tools which are necessary to go either path that I mentioned earlier. I am working in an MNC for 3 years. ",
    "url": "https://www.reddit.com/r/data/comments/1g4whrx/switching_from_developer_to_data_roles/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1729075865.0,
    "author": "Sourav7996",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g4whrx/switching_from_developer_to_data_roles/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1g4gr06",
    "title": "Hey Data Enthusiasts! 👋 Let’s Talk About Data Engineering and Growth Opportunities",
    "selftext": "Hi everyone! I’m Alejandro, a **Data Engineering expert with over 20 years of experience** working on everything from **real-time pipelines and cloud integrations** to **advanced data analytics.** I’m here to connect with like-minded folks and share something exciting with you all.\n\nWe recently launched a growing community at **DAR Analytics** – a space designed to **learn, collaborate, and solve real-world data challenges together**. Whether you’re new to the field or an experienced pro, there’s something for everyone.\n\n💼 **What you’ll find in our community:**\n\n* In-depth **blogs** breaking down complex concepts in data engineering.\n* **Real-world use cases** tailored for startups, helping solve challenges from Day 1.\n* A thriving **community hosted on Skool** for discussions, projects, and continuous learning.\n\nThe best part? **It’s a place where practical insights meet real growth**—no fluff, just actionable knowledge. If you want to connect with other data professionals, discuss industry trends, or dive into projects that make a difference, this is the right place for you.\n\n🔗 **Check us out:** [daranalytics.com](http://www.daranalytics.com)\n\n[https://www.skool.com/data-team-7833/about](https://www.skool.com/data-team-7833/about)\n\n**Let’s collaborate, learn, and grow together.** I'd love to hear your experiences, challenges, and thoughts about the ever-evolving data space! 🚀\n\n#DataEngineering #Analytics #BusinessGrowth #DataCommunity #LearnTogether #DARAnalytics",
    "url": "https://www.reddit.com/r/data/comments/1g4gr06/hey_data_enthusiasts_lets_talk_about_data/",
    "score": 0,
    "upvote_ratio": 0.2,
    "num_comments": 0,
    "created_utc": 1729022087.0,
    "author": "ale-3385",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g4gr06/hey_data_enthusiasts_lets_talk_about_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1g42lal",
    "title": "How about if the results of glmm and sem don't fit the general laws of nature? ",
    "selftext": "For example, in the northern hemisphere, elevation factors and species richness show a negative correlation based on GLMM and SEM? What might be the cause of this? The amount of data? Model construction errors?",
    "url": "https://www.reddit.com/r/data/comments/1g42lal/how_about_if_the_results_of_glmm_and_sem_dont_fit/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1728979186.0,
    "author": "Ekubo019",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g42lal/how_about_if_the_results_of_glmm_and_sem_dont_fit/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1g3wvm7",
    "title": "Found an app that doesn't store your data and is really easy to use",
    "selftext": "https://preview.redd.it/hj8zaayzttud1.png?width=2214&format=png&auto=webp&s=3b79ee0a73baed020ecdf7afe145088b5773499c\n\niOS App: [https://apps.apple.com/us/app/plansense/id6503712817](https://apps.apple.com/us/app/plansense/id6503712817)",
    "url": "https://www.reddit.com/r/data/comments/1g3wvm7/found_an_app_that_doesnt_store_your_data_and_is/",
    "score": 0,
    "upvote_ratio": 0.43,
    "num_comments": 2,
    "created_utc": 1728957411.0,
    "author": null,
    "subreddit": "data",
    "permalink": "/r/data/comments/1g3wvm7/found_an_app_that_doesnt_store_your_data_and_is/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lrzobc5",
        "body": "Just say you made it",
        "score": 5,
        "created_utc": 1728965629.0,
        "author": "duniyadnd",
        "is_submitter": false,
        "parent_id": "t3_1g3wvm7",
        "depth": 0
      },
      {
        "id": "lrzsdmx",
        "body": "Is it discovery or invention !!!",
        "score": 2,
        "created_utc": 1728967767.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_1g3wvm7",
        "depth": 0
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1g2s00j",
    "title": "I shared a 1+ Hour Streamlit Course on YouTube - Learn to Create Python Data/Web Apps Easily",
    "selftext": "Hello, I just shared a Python Streamlit Course on YouTube. Streamlit is a Python framework for creating Data/Web Apps with a few lines of Python code. I covered a wide range of topics, started to the course with installation and finished with creating machine learning web apps. I am leaving the link below, have a great day!\n\n[https://www.youtube.com/watch?v=Y6VdvNdNHqo&list=PLTsu3dft3CWiow7L7WrCd27ohlra\\_5PGH&index=10](https://www.youtube.com/watch?v=Y6VdvNdNHqo&list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&index=10)",
    "url": "https://www.reddit.com/r/data/comments/1g2s00j/i_shared_a_1_hour_streamlit_course_on_youtube/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1728832807.0,
    "author": "onurbaltaci",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g2s00j/i_shared_a_1_hour_streamlit_course_on_youtube/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1g2o3p3",
    "title": "What happens to your data after you die?",
    "selftext": "It could be anything - your photos, passwords, apps, instagram, payroll, etc. Does it get stored somewhere? How would someone get access to it e.g. a close family member?\n\nDo you guys really care about what happens to/who sees your data after you die?",
    "url": "https://www.reddit.com/r/data/comments/1g2o3p3/what_happens_to_your_data_after_you_die/",
    "score": 1,
    "upvote_ratio": 0.67,
    "num_comments": 3,
    "created_utc": 1728820943.0,
    "author": "reila_333",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g2o3p3/what_happens_to_your_data_after_you_die/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lrpnjc9",
        "body": "My partner has access to everything. I assume she would continue to use it or dispose of it. Either way i wont care lol.",
        "score": 1,
        "created_utc": 1728823930.0,
        "author": "k4zetsukai",
        "is_submitter": false,
        "parent_id": "t3_1g2o3p3",
        "depth": 0
      },
      {
        "id": "lrq5mu7",
        "body": "The person managing your affairs will get your death certificate and/or some documents saying they are in charge of your estate. They have to contact companies one by one to tell them you've died, and what happens next is up to the company's policies.\n\nThey can contact companies like banks, insurance companies, companies you got loans from etc to close down those accounts. Companies will keep records for a number of years determined by regulations and then delete them.\n\nFor things like your Facebook and Google accounts, they may make some of the information available to your estate (photos and emails and so on) or they may simply delete the account. It depends on the company and it's policies and there aren't many laws that regulate this, just the terms of service when you open the account.\n\nThis does mean that informing companies you've died is sometimes a bad idea. I know someone who wanted to keep their brother's account in an online game they played together as a memorial but when they informed the company he had died, they closed the account. Real kick in the balls but that was their policy.",
        "score": 1,
        "created_utc": 1728830988.0,
        "author": "fang_xianfu",
        "is_submitter": false,
        "parent_id": "t3_1g2o3p3",
        "depth": 0
      },
      {
        "id": "lsk25cg",
        "body": "Well depending on access and what u have in your will, in reality your dead sooo those random facts that are you, just become someone else’s data.",
        "score": 1,
        "created_utc": 1729272068.0,
        "author": "Dirt-Repulsive",
        "is_submitter": false,
        "parent_id": "t3_1g2o3p3",
        "depth": 0
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1g223zv",
    "title": "I don't know where to post, if someone can point me to the right sub reddit that would be great. But.. Is there any way to recover data from this, onto a pc or USB drive, or SD card? Just to get access to it",
    "selftext": "",
    "url": "https://i.redd.it/tbtu0i81ccud1.jpeg",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1728745538.0,
    "author": "MousseIndependent310",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g223zv/i_dont_know_where_to_post_if_someone_can_point_me/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lrkql3p",
        "body": "r/datarecovery will be more up your alley for this",
        "score": 1,
        "created_utc": 1728745949.0,
        "author": "ChevyRacer71",
        "is_submitter": false,
        "parent_id": "t3_1g223zv",
        "depth": 0
      },
      {
        "id": "lrl5o15",
        "body": "Ahh, okay, thank you!",
        "score": 1,
        "created_utc": 1728751118.0,
        "author": "MousseIndependent310",
        "is_submitter": true,
        "parent_id": "t1_lrkql3p",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1g1jvl2",
    "title": "Adobe found a Legal loophole to show your First & Last Name when you go to a website",
    "selftext": "This is a Measure Summit presentation from Charles Farina, VP Digital Strategy, Adswerve showing the latest marketing tools from Adobe Customer Journey Analytics.\n\nPlease skip to 32:30 in the video to see what I'm referring to: [https://measuresummit.com/access/speaker/charles-farina-2024/](https://measuresummit.com/access/speaker/charles-farina-2024/)\n\nOr go to the Loom link I made: [https://www.loom.com/share/09dcd35b203a4c59a2069af19c94aae4](https://www.loom.com/share/09dcd35b203a4c59a2069af19c94aae4)\n\n  \nHow is this even legal??",
    "url": "https://www.reddit.com/r/data/comments/1g1jvl2/adobe_found_a_legal_loophole_to_show_your_first/",
    "score": 5,
    "upvote_ratio": 0.86,
    "num_comments": 0,
    "created_utc": 1728680124.0,
    "author": "WantMoreCookies",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g1jvl2/adobe_found_a_legal_loophole_to_show_your_first/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1g14ne4",
    "title": "DAMA certification",
    "selftext": "Hi there,\n\nData consultant here, working for several businesses during the past 10 years. Mostly on Data Analyst, Data Governance & Database administration missions. \n\nLooking to pass the first level of DAMA certification program (CDMP associate). Any feedback on the certification ? On the exam? Bullshit certification or worth it? \nhttps://cdmp.info/about/\n\nThanks for the feedbacks !",
    "url": "https://www.reddit.com/r/data/comments/1g14ne4/dama_certification/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1728633482.0,
    "author": "Illustrious-Fan4485",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g14ne4/dama_certification/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1g14aoc",
    "title": "Fresh Software Engineering Graduate - How Easy is it to Transition to Data Analysis?",
    "selftext": "Hey everyone,\n\nI’m a fresh graduate with a Bachelor's degree in Software Engineering, and I’m interested in transitioning into data analysis. I have a solid foundation in programming (Java, Python, SQL) and have done some basic work with data manipulation and visualization.\n\nI wanted to ask: how easy is it for someone with my background to break into the data analysis field? Are there any specific skills or tools I should focus on learning? And what’s the job market like right now for entry-level data analysts?\n\nAny advice or personal experiences would be greatly appreciated!\n\nThanks!",
    "url": "https://www.reddit.com/r/data/comments/1g14aoc/fresh_software_engineering_graduate_how_easy_is/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1728631768.0,
    "author": "Fishingforfish2292",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g14aoc/fresh_software_engineering_graduate_how_easy_is/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": true,
    "locked": false,
    "comments": [
      {
        "id": "lrdqu4k",
        "body": "Congratulations on your degree!\n\nIt shouldn't be a huge lift to add data analysis on top of a software engineering background that already includes SQL.\n\nThere's a decent chance you can move straight into doing some projects, and learning the necessary tools on the fly. Look up some portfolio projects others have, and build your own that do similar things. \n\nThe actual on-the-job work can vary widely, but you'll often at least see job descriptions mention things like Looker or Power BI, and sometimes Python/Pandas.\n\nIt can definitely help to have a strong statistics foundation. \n\nAs low hanging fruit, get a Google Analytics certification to start, lurk here and in the data analyst world on LinkedIn, and build a network of data analysts to talk to about their work and see if it's what interests you.",
        "score": 1,
        "created_utc": 1728633425.0,
        "author": "oldmaninnyc",
        "is_submitter": false,
        "parent_id": "t3_1g14aoc",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1g0vzrd",
    "title": "Looking for free bulk image OCR?",
    "selftext": "Hello, I have thousands of image files that all follow the same format, and I'd like to extract the data from about 20 fields in the images. I currently have 500 images but anticipate gathering many more. Do you know of any free image OCRs with high accuracy and that allow customization of which fields of pixels on the image to pull from? I'll be compiling all of the data into a CSV and there's too much data to split it myself, which is why it's important I find an OCR where I can specify which pixels on the image to look at for each data point. Thank you in advance!",
    "url": "https://www.reddit.com/r/data/comments/1g0vzrd/looking_for_free_bulk_image_ocr/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1728602119.0,
    "author": "yaggirl341",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g0vzrd/looking_for_free_bulk_image_ocr/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lrfnh5o",
        "body": "Google pinpoint. I use it all the time to scrape PDF pages that have the same format. You just gotta request access, but after that, you don’t have to code and all you do draw a box around the areas you’d like. You’ll want to use the key-pair feature",
        "score": 1,
        "created_utc": 1728664448.0,
        "author": "jacksparrow914",
        "is_submitter": false,
        "parent_id": "t3_1g0vzrd",
        "depth": 0
      },
      {
        "id": "ls1o94v",
        "body": "Thx. Didnt know that we have this tool",
        "score": 1,
        "created_utc": 1729003777.0,
        "author": "Mentally_Chaos",
        "is_submitter": false,
        "parent_id": "t1_lrfnh5o",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1g108k6",
    "title": "Nikkei 225 Dividend Yield Data",
    "selftext": "I was looking for Nikkei 225 Dividend Yield historical data (1980-2023) but could scarcely find anything. \n\nI figured I could calculate it myself by dividing the Dividend Point Index data presented by Nikkei and the closing value of the index. However, that data is available only for a limited number of years.\n\nIs there any place I could scrap this data from? ",
    "url": "https://www.reddit.com/r/data/comments/1g108k6/nikkei_225_dividend_yield_data/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1728615624.0,
    "author": "Outrageous_Wedding84",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g108k6/nikkei_225_dividend_yield_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lrd2vvc",
        "body": "https://alfred.stlouisfed.org/series?seid=NIKKEI225&t&utm_source=perplexity#",
        "score": 1,
        "created_utc": 1728619018.0,
        "author": "Both-Blueberry2510",
        "is_submitter": false,
        "parent_id": "t3_1g108k6",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1g0h7rs",
    "title": "Am I Underpaid as a New Data Scientist?",
    "selftext": "I recently started my first Data Scientist role at a non-profit, earning $30K a year part-time. While I’m still working towards my degree, I have a Google Data Analytics certification and some personal project experience. After just two months, I’ve been told my work has made a big difference compared to the previous Data Scientist, and I’m responsible for creating reports and supporting key billing processes.\n\nHowever, I’m consistently working beyond my scheduled hours, including weekends, to keep up with the workload. Given that the average entry-level salary for Data Scientists is around $80K or more, even at non-profits, I’m starting to feel like $30K is far too low. Is it time to ask for a raise?",
    "url": "https://www.reddit.com/r/data/comments/1g0h7rs/am_i_underpaid_as_a_new_data_scientist/",
    "score": 7,
    "upvote_ratio": 0.82,
    "num_comments": 9,
    "created_utc": 1728561927.0,
    "author": "Hopeful_Article_8808",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g0h7rs/am_i_underpaid_as_a_new_data_scientist/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lr9oj5n",
        "body": "People usually have masters or PhD to begin with that salary, your salary is low at the moment but if you get the experience and move to full time, your salary will increase significantly. Google certification alone doesn’t help, work on kaggle to showcase your skills",
        "score": 5,
        "created_utc": 1728575755.0,
        "author": "Slight-Tangerine-164",
        "is_submitter": false,
        "parent_id": "t3_1g0h7rs",
        "depth": 0
      },
      {
        "id": "lr8xmgw",
        "body": "How did you even get a job part-time as a data scientist without a degree yet in this job market? Was it an internship?",
        "score": 8,
        "created_utc": 1728566445.0,
        "author": "TheManUtd10",
        "is_submitter": false,
        "parent_id": "t3_1g0h7rs",
        "depth": 0
      },
      {
        "id": "lr8oqix",
        "body": "People that underpay by that much aren't going to suddenly find the extra $70k you should be earning.\n\nApply for a different job.",
        "score": 5,
        "created_utc": 1728562769.0,
        "author": "oldmaninnyc",
        "is_submitter": false,
        "parent_id": "t3_1g0h7rs",
        "depth": 0
      },
      {
        "id": "lrae7m9",
        "body": "It is also going to depend on how many hours and you should factor in other benefits as well.  Part time can mean 5 hours or 35 hours, there isn’t really a a standard. \n\nAt my employer you would not have the title of Data Scientist until you were full-time and had a Master’s degree. This would also be on a lower pay scale than a Data Scientist.",
        "score": 1,
        "created_utc": 1728584059.0,
        "author": "MyBlueSunshines",
        "is_submitter": false,
        "parent_id": "t3_1g0h7rs",
        "depth": 0
      },
      {
        "id": "lrekks8",
        "body": "How many hours are you working? If it’s 20 per week then your pay is the equivalent of $60k salary. \n\nHowever, if you’re really worried that you’re underpaid, start applying elsewhere and see if you can get a better offer. If you can or can’t, there’s your answer.",
        "score": 1,
        "created_utc": 1728650936.0,
        "author": "data_story_teller",
        "is_submitter": false,
        "parent_id": "t3_1g0h7rs",
        "depth": 0
      },
      {
        "id": "lrif9vb",
        "body": "I hear people say \"I'm worth x but only being paid .7x\" all the time. And I often tell them they are worth what the market will support. If you feel underpaid, market info can let you know if you should try negotiating with your current employer, but when that fails, you are worth what another employer is willing to offer you.",
        "score": 1,
        "created_utc": 1728701079.0,
        "author": "linchpyn",
        "is_submitter": false,
        "parent_id": "t3_1g0h7rs",
        "depth": 0
      },
      {
        "id": "lrtuhnn",
        "body": "That sound like enough? Yes you’re underpaid. That’s enough for a data analyst in an internship w/out a bachelors. You’re being taken advantage of, people in the role of “data scientist” typically have a masters degree or phd, and would scoff at an offer of 30k. If you value your own education, don’t do this work for such a trivial amount. For context, servers who work full time usually make somewhere between 50-80k a year.",
        "score": 1,
        "created_utc": 1728878107.0,
        "author": "ShittyPissyDick",
        "is_submitter": false,
        "parent_id": "t3_1g0h7rs",
        "depth": 0
      },
      {
        "id": "lr8nzh8",
        "body": "Yes yes it is.",
        "score": 0,
        "created_utc": 1728562429.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_1g0h7rs",
        "depth": 0
      },
      {
        "id": "lrug9cd",
        "body": "Servers who work full time and make 50-80k also likely work a lot more hours than someone who says they have a part time job. We have no idea how many hours OP is working. If they're working 34 hours a week, then yeah definitely underpaid. If it's 10 then they're doing very well for essentially no qualifications especially in this market",
        "score": 1,
        "created_utc": 1728892616.0,
        "author": "snmnky9490",
        "is_submitter": false,
        "parent_id": "t1_lrtuhnn",
        "depth": 1
      }
    ],
    "comments_extracted": 9
  },
  {
    "id": "1g0l8b4",
    "title": "Project for Interview",
    "selftext": "Hello, I started a new career as a Data Analyst and would like to ask about a project for an interview. I was given data for one year, and in the instructions it said, \"We expect a growth of 10% every year for every product\", I spoke with a data scientist mock interviewer and he said, it's not good to do this graph since the data is too small and not enough to back up 10 years. I would like to know other people's thoughts on this since I am presenting this tomorrow during the interview.\n\nhttps://preview.redd.it/cdbf0od26ytd1.png?width=1532&format=png&auto=webp&s=42684be269146ee83172f5eae55e2ec6930cd860\n\n",
    "url": "https://www.reddit.com/r/data/comments/1g0l8b4/project_for_interview/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1728573517.0,
    "author": "jennercr7",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g0l8b4/project_for_interview/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1g05g78",
    "title": "Looking for a Paraquat Applicator/Farmers Database",
    "selftext": "Hey 👋🏻,\n\nI’m currently working on a project and I’m trying to get my hands on a database that tracks farmers or applicators who have used Paraquat. I’m particularly interested in any datasets that could provide info on usage patterns, application history, or anything related to this herbicide.\n\nI’ve done some basic searches but haven’t had much luck finding something concrete. Does anyone here know where I might be able to find such a dataset? Whether it’s publicly available, or even something I’d need to purchase or request through an organization, any lead would be super helpful.\n\nThanks in advance for any tips or suggestions! 👨‍🌾",
    "url": "https://www.reddit.com/r/data/comments/1g05g78/looking_for_a_paraquat_applicatorfarmers_database/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1728516960.0,
    "author": "alb53",
    "subreddit": "data",
    "permalink": "/r/data/comments/1g05g78/looking_for_a_paraquat_applicatorfarmers_database/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "luxp5jr",
        "body": "The only state that publishes pesticide use data, to my knowledge, is California. It’s been years since I’ve used it, but try looking for California Pesticide Information Portal (CalPIP). You’d probably be able to get pounds applied per county per time period, but you are not going to get PII (i.e., who specifically applied it). For national use data, you could look at EPA’s paraquat registration review docket at regulations.gov. You’d probably find a use and benefits assessment, but the most quantitative data you’d get would be something like “x% of Crop A were reported to have been sprayed with paraquat between YYYY-YYYY.” Source: used to work in pesticide consulting.",
        "score": 1,
        "created_utc": 1730502119.0,
        "author": "EducationalKnee2386",
        "is_submitter": false,
        "parent_id": "t3_1g05g78",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1fz98vp",
    "title": "Average weekly gas prices by city",
    "selftext": "Hello, is there a database or website where I can download the data of average weekly gas prices by US city since 2018? I need Omaha, Nebraska, specifically.",
    "url": "https://www.reddit.com/r/data/comments/1fz98vp/average_weekly_gas_prices_by_city/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "created_utc": 1728418048.0,
    "author": "Local-Message1826",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fz98vp/average_weekly_gas_prices_by_city/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lr14l47",
        "body": "Did a Google search for [historical gas prices Omaha] \n\nGot this\nhttps://neo.ne.gov/programs/stats/inf/107.htm",
        "score": 2,
        "created_utc": 1728438343.0,
        "author": "oldmaninnyc",
        "is_submitter": false,
        "parent_id": "t3_1fz98vp",
        "depth": 0
      },
      {
        "id": "lr1648b",
        "body": "Thank you brother",
        "score": 1,
        "created_utc": 1728439004.0,
        "author": "Local-Message1826",
        "is_submitter": true,
        "parent_id": "t1_lr14l47",
        "depth": 1
      }
    ],
    "comments_extracted": 2
  },
  {
    "id": "1fyqlpx",
    "title": "How to score a lat-long point basis density of other surrounding points?",
    "selftext": "Hey guys! Absolute newbie to statistics and data analysis reaching out for help here. I have a lat-long data set of all the retail outlets I service in my state. How do I go about assigning an outlet density score to each one of those outlets basis the density of outlets in a 3 km radius around each outlet?",
    "url": "https://www.reddit.com/r/data/comments/1fyqlpx/how_to_score_a_latlong_point_basis_density_of/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1728358108.0,
    "author": "eyeof_ra",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fyqlpx/how_to_score_a_latlong_point_basis_density_of/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1fyow1x",
    "title": "CDMP Studying",
    "selftext": "Hey! Im a senior analyst working in Data Management and Data Quality, thinking of doing my CDMP certificate. I'm kinda hesitant but ive read that it's good for career growth and knowledge. I've been looking at the DMBOK V2 Revised edition online as a free pdf download to take an idea and start studying to see if i like it, but didnt find a link. Can anyone send me the book or advise where they found it? I would like to hear your honest opinion on this certificate please",
    "url": "https://www.reddit.com/r/data/comments/1fyow1x/cdmp_studying/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1728352645.0,
    "author": "ChocolateLeast3350",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fyow1x/cdmp_studying/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lwozypf",
        "body": "Hi, have you taken the DQ exam yet? What is the DQ in real life all about (works, criteria, standards that related to this field)? Thank you.",
        "score": 1,
        "created_utc": 1731381984.0,
        "author": "thanhdinh2302",
        "is_submitter": false,
        "parent_id": "t3_1fyow1x",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1fxnnmc",
    "title": "Do Data Visualisation in plain language ",
    "selftext": "Datahorse simplifies the process of creating visualizations like scatter plots, histograms, and heatmaps through natural language commands. \n\nWhether you're new to data science or an experienced analyst, it allows for easy and intuitive data visualization.\n\nhttps://github.com/DeDolphins/DataHorse",
    "url": "https://v.redd.it/f3rt4zhyl6td1",
    "score": 26,
    "upvote_ratio": 1.0,
    "num_comments": 5,
    "created_utc": 1728240391.0,
    "author": "Ifearmyselfandyou",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fxnnmc/do_data_visualisation_in_plain_language/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lqoogr7",
        "body": "Nice. I'm going to test it",
        "score": 2,
        "created_utc": 1728251905.0,
        "author": "Content-Ad-1246",
        "is_submitter": false,
        "parent_id": "t3_1fxnnmc",
        "depth": 0
      },
      {
        "id": "lqq9hk4",
        "body": "Does it need a LLM API key or how does the LLM part work?",
        "score": 1,
        "created_utc": 1728274281.0,
        "author": "nyquant",
        "is_submitter": false,
        "parent_id": "t3_1fxnnmc",
        "depth": 0
      },
      {
        "id": "lqupvon",
        "body": "yea youre still pushing this project. its a bad idea for anything other than quick careless glances. llms still do weird shit that you need to check",
        "score": 0,
        "created_utc": 1728341254.0,
        "author": "bonferoni",
        "is_submitter": false,
        "parent_id": "t3_1fxnnmc",
        "depth": 0
      },
      {
        "id": "lqq9rvc",
        "body": "I guess there is that:\n\nverbose = False\nmutable = False\nmodel = ‘llama3-8b-8192’\ngroq_api_key = os.getenv(“DATAHORSE_API_KEY”)\nclient = Groq(api_key=groq_api_key)",
        "score": 1,
        "created_utc": 1728274421.0,
        "author": "nyquant",
        "is_submitter": false,
        "parent_id": "t1_lqq9hk4",
        "depth": 1
      },
      {
        "id": "lqwfukx",
        "body": "We are currently in final stages of own own LLM that we be open source.",
        "score": 1,
        "created_utc": 1728367507.0,
        "author": "Ifearmyselfandyou",
        "is_submitter": true,
        "parent_id": "t1_lqq9hk4",
        "depth": 1
      }
    ],
    "comments_extracted": 5
  },
  {
    "id": "1fxisso",
    "title": "MSDS or MSAI/ML?",
    "selftext": "Hey everyone, I'm trying to decide between two different master's programs and could use some advice. One is a master's in data science, and the other is a master's in AI/ML. I'm having a hard time figuring out which would be more beneficial in the long run.\n\nhttps://cdso.utexas.edu/msds\n\nhttps://cdso.utexas.edu/msai\n\nFor context, I have some experience in both areas and want to enhance my career for more advanced work in data analytics, science, or AI. Which do you think would be a better option in terms of future job prospects and practical applications? I live in the US and can relocate.\n\n\nThanks in advance for your input!",
    "url": "https://www.reddit.com/r/data/comments/1fxisso/msds_or_msaiml/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1728227794.0,
    "author": "AggressiveAd69x",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fxisso/msds_or_msaiml/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1fwo2m1",
    "title": "Insta data",
    "selftext": "Hi all\nWell I am little new to programming. I got one idea recently, want to know is there some way, I can analyse the instagram/YouTube scrolling.(Insta preferably)\nI mean I want to know what people usually scroll these days.?\nIs it remotely possible to get that data? Of any user or a large userbase?",
    "url": "https://www.reddit.com/r/data/comments/1fwo2m1/insta_data/",
    "score": 6,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1728126963.0,
    "author": "Consistent_Rise_7339",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fwo2m1/insta_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lqfus0t",
        "body": "Those data is not openly available to public not even to the companies directly. It's like privacy protection. So companies get these info as least as possible like simple group. ex:cars,bikes, trucks- vehicles like that .. not even full information",
        "score": 2,
        "created_utc": 1728127384.0,
        "author": "Dragon-king-7723",
        "is_submitter": false,
        "parent_id": "t3_1fwo2m1",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1fw2t41",
    "title": "Is the Data Industry Thriving? Insights and Career Advice",
    "selftext": "I'm looking for information about the job market in the data field, especially in the context of business studies. I have solid knowledge of SQL and a basic level in Python and Java. I would like to know what job opportunities exist and what additional skills might be useful to improve my employment prospects.\n\nAdditionally, I'm interested in knowing if the market is good at the moment, as I'm considering improving my technical skills but I'm not sure if it's worth it. Does anyone have experience in this field or can offer any advice on how to advance in my career? I appreciate any suggestions or resources you can share.\n\nThanks in advance!",
    "url": "https://www.reddit.com/r/data/comments/1fw2t41/is_the_data_industry_thriving_insights_and_career/",
    "score": 5,
    "upvote_ratio": 0.86,
    "num_comments": 3,
    "created_utc": 1728057949.0,
    "author": "Fabulous_Shoulder446",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fw2t41/is_the_data_industry_thriving_insights_and_career/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lqm8a77",
        "body": "It would be useful to start distinguishing between the data industry and the data analytics industry. With the advent of mobile phone data and sensors and scrapping, the data has become and industry in itself. But you are clearly interested in the data analytics industry",
        "score": 2,
        "created_utc": 1728223520.0,
        "author": "rlopez7",
        "is_submitter": false,
        "parent_id": "t3_1fw2t41",
        "depth": 0
      },
      {
        "id": "lqcdl83",
        "body": "I am intrested also 🤛🏻✌🏻",
        "score": 1,
        "created_utc": 1728067965.0,
        "author": "Resident-Coconut172",
        "is_submitter": false,
        "parent_id": "t3_1fw2t41",
        "depth": 0
      },
      {
        "id": "lqpvmxl",
        "body": "Not sure if it's a data thing or the current job market. I have 15 years experience and am barely getting interviews for jobs in my specific industry.  Maybe my resume is weak. Who knows?  I hear the job market is rough right now, though.\n\nI recommend that you apply for jobs while you study. If someone likes you and offers you a job while you are studying, that's great!  If not, you will be on your way to having more skills and becoming a more attractive candidate. \n\nI like to solve practice data problems on Stratascratch.  Make sure you filter on the \"free\" problems.\n\nKaggle has free machine learning tutorials.  They also have competitions. It's a good way to practice and you can include those workbooks in your work portfolio.",
        "score": 1,
        "created_utc": 1728268191.0,
        "author": "KingGeorgeClooney",
        "is_submitter": false,
        "parent_id": "t3_1fw2t41",
        "depth": 0
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1fvoncb",
    "title": "Any1 got data about the GDP growth rate from Jan 2010 to Aug 2024? i need the quarterly version not the annual",
    "selftext": "",
    "url": "https://www.reddit.com/r/data/comments/1fvoncb/any1_got_data_about_the_gdp_growth_rate_from_jan/",
    "score": 4,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1728008083.0,
    "author": "True-Reserve4307",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fvoncb/any1_got_data_about_the_gdp_growth_rate_from_jan/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lqmpfhi",
        "body": "https://www.statista.com/statistics/188185/percent-change-from-preceding-period-in-real-gdp-in-the-us/",
        "score": 2,
        "created_utc": 1728229325.0,
        "author": "Both-Blueberry2510",
        "is_submitter": false,
        "parent_id": "t3_1fvoncb",
        "depth": 0
      },
      {
        "id": "lqas1ef",
        "body": "Check FRED?",
        "score": 1,
        "created_utc": 1728049292.0,
        "author": "jcoffi",
        "is_submitter": false,
        "parent_id": "t3_1fvoncb",
        "depth": 0
      },
      {
        "id": "lqmpozg",
        "body": "https://www.bea.gov/news/2024/gross-domestic-product-third-estimate-corporate-profits-revised-estimate-and-gdp-0",
        "score": 1,
        "created_utc": 1728229409.0,
        "author": "Both-Blueberry2510",
        "is_submitter": false,
        "parent_id": "t3_1fvoncb",
        "depth": 0
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1fvjpg6",
    "title": "screen time dataset",
    "selftext": "i want the screen time data of mobile phone users from 2019 - 2022. where can i get this dataset? also i need to get app screen time data as well",
    "url": "https://www.reddit.com/r/data/comments/1fvjpg6/screen_time_dataset/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1727993366.0,
    "author": "Sweaty_Philosopher69",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fvjpg6/screen_time_dataset/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1fvimf2",
    "title": "Snapchat data",
    "selftext": "Why isn’t my Snapchat data showing up from saved chats of someone that I do not have added anymore? I know it used to show data from unadded people.",
    "url": "https://www.reddit.com/r/data/comments/1fvimf2/snapchat_data/",
    "score": 0,
    "upvote_ratio": 0.33,
    "num_comments": 0,
    "created_utc": 1727990446.0,
    "author": "Comfortable-Crew2821",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fvimf2/snapchat_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1ftixka",
    "title": "Seeking Recommendations for Evaluating Imputation Quality in a Large Dataset",
    "selftext": "Hello, everyone!\n\nI’m currently working on a dataset with 852 columns, where 304 are continuous and the remaining are categorical. The dataset contains 29,000 missing values—15,000 in continuous columns and 14,000 in ordinal columns. For the ordinal columns, I’ve opted for mode imputation since other methods produce float values or unwanted entries.\n\nFor the continuous columns, I’ve been experimenting with several imputation techniques, including MICE, KNN, Matrix, Mean, MISSForest, Bayesian Ridge, and BPCA.\n\nNow, I want to evaluate the quality of the imputations from these various methods to determine which one provides the best results for my analysis.\n\nI’m looking for suggestions on methods or metrics I could use to assess imputation quality. Any recommendations or insights would be greatly appreciated!\n\nThank you in advance!\n\n ",
    "url": "https://www.reddit.com/r/data/comments/1ftixka/seeking_recommendations_for_evaluating_imputation/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1727770156.0,
    "author": "Electronic-Willow701",
    "subreddit": "data",
    "permalink": "/r/data/comments/1ftixka/seeking_recommendations_for_evaluating_imputation/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1fsvidk",
    "title": "Have you ever used a Web3 framework for your data privacy? ",
    "selftext": "I think self-sovereign applications in Web3 are way more useful for data control, but I don’t know if there are any specific apps or projects out there. If anyone has used one or knows about it, I’d appreciate it if you could drop a comment for me to check out ",
    "url": "https://www.reddit.com/r/data/comments/1fsvidk/have_you_ever_used_a_web3_framework_for_your_data/",
    "score": 6,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1727702801.0,
    "author": "oldwhiteblackie",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fsvidk/have_you_ever_used_a_web3_framework_for_your_data/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1fsyqrg",
    "title": "How to understand data and graphs?",
    "selftext": "I am a product manager working in a fintech. I am uncomfortable working with data sets, like even when I have lots of datasets, i dont understand how to create sense out of it? I get intimidated easily and I have data analysts so I always escape from the discomfort.  \nWhat can I do to crunch data, such that I get the EUREKA! moments from it easily. Atleast not sure shot then hit and trial still works? \n\nI need to look at two different graphs and create a coherence....JUST REALLY WANT TO GET OUT OF THIS COMFORT ZONE ",
    "url": "https://www.reddit.com/r/data/comments/1fsyqrg/how_to_understand_data_and_graphs/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 3,
    "created_utc": 1727711161.0,
    "author": "AndThenThereWereNonw",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fsyqrg/how_to_understand_data_and_graphs/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lprvprm",
        "body": "Just think of data as gossip and graphs as visual gossip - they're just telling a story with numbers and pictures! It's like getting the latest scoop on what's going on. So grab a cup of tea and let's dive into the juicy details!",
        "score": 2,
        "created_utc": 1727763312.0,
        "author": "creppersdreef",
        "is_submitter": false,
        "parent_id": "t3_1fsyqrg",
        "depth": 0
      },
      {
        "id": "lprqvy0",
        "body": "I don't know about the others, but for me you need to be more specific so that I may be able to advise you. Like, telling about a specific moment in the past when you felt that.",
        "score": 1,
        "created_utc": 1727760376.0,
        "author": "Nat0ne",
        "is_submitter": false,
        "parent_id": "t3_1fsyqrg",
        "depth": 0
      },
      {
        "id": "lqnupp2",
        "body": "Start with 1 dataset. \nEither the easiest one or most important one.\nTry to glance through rows and columns and get a sense of data. Is it marketing data, product usage, sales, finance etc.\nMost datasets have only 3 types of columns\nMeasure - numbers\nDimension - text or id \nDate - date, week, month \n\nPull a sample from that dataset into Google sheet. See if you can summarize numbers by date or measure.\nSummarize could be sum, count or average \nYou will start seeing patterns. \nGrowing or decreasing\nOne higher than other etc.\n\nThis has been oversimplified for a reddit comment level. But hope you get the idea and can get started somewhere. Good luck",
        "score": 1,
        "created_utc": 1728242410.0,
        "author": "Both-Blueberry2510",
        "is_submitter": false,
        "parent_id": "t3_1fsyqrg",
        "depth": 0
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1fsswpj",
    "title": "Taxes paid by income bracket, impossible to find!",
    "selftext": "I want to find statistics regarding how much of the total tax income comes from each tax bracket. for example the top 10% earners pay 45% of the total tax revenu. This is pertaining to Switzerland only. I can find how much people are in each bracket but nothing linking to the actual tax amount paid.",
    "url": "https://www.reddit.com/r/data/comments/1fsswpj/taxes_paid_by_income_bracket_impossible_to_find/",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1727694553.0,
    "author": "Professional-Fox6121",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fsswpj/taxes_paid_by_income_bracket_impossible_to_find/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1fsrtkh",
    "title": "(x-post r/datasets) Looking for dataset with stress measures and eating disorder severity",
    "selftext": "Hi all,\n\nIf this is not appropriate here, please delete or perhaps tip me in the direction of other more relevant subreddits.   \n  \nPerhaps someone can point me in the right direction: I have been combing through different (open) datasets to find a dataset that includes both a measure of eating disorder severity and a measure of (experienced) stress, especially a measure of what caused stress (so is the experienced stress mostly due to for example work, or social, or due to the eating disorder).\n\nI work as a neuro and behavioural scientist in the eating disorder field, focusing on the effects of stress on the course of an eating disorder. We already know that stress makes eating disorders worse, but we don’t know well if this is mostly due to stressors that are specific to the eating disorder itself (e.g. stress due to having to eat, or due to binges) or due to more general stressors, such as social stressors or work. This is clinically relevant and as including patients in a study to examine this takes a lot of time and burdens patients again, I’m seeing if there are datasets that includes these data.\n\nHopefully someone has an idea, thanks in advance!",
    "url": "https://www.reddit.com/r/data/comments/1fsrtkh/xpost_rdatasets_looking_for_dataset_with_stress/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1727690141.0,
    "author": "joop_niknil",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fsrtkh/xpost_rdatasets_looking_for_dataset_with_stress/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1fsfdus",
    "title": "Daily Dose of Data Science",
    "selftext": "",
    "url": "https://blog.dailydoseofds.com/?r=1i8m5n&utm_campaign=referrals-subscribe-page-share-screen&utm_medium=web",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1727645620.0,
    "author": "RiskAdministrative38",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fsfdus/daily_dose_of_data_science/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1fs96s0",
    "title": "Can I turn this map’s data into a spreadsheet?",
    "selftext": "Sorry if this is the wrong group, let me know if there's a better subreddit.\n\nIm trying to turn the data that this map pulls from into an excel spreadsheet, including name and location. Is that even possible?",
    "url": "https://www.atlasobscura.com/articles/all-places-in-the-atlas-on-one-map",
    "score": 0,
    "upvote_ratio": 0.5,
    "num_comments": 3,
    "created_utc": 1727629505.0,
    "author": null,
    "subreddit": "data",
    "permalink": "/r/data/comments/1fs96s0/can_i_turn_this_maps_data_into_a_spreadsheet/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lpjcw8m",
        "body": "berserk plough ten imagine waiting capable unwritten rotten like piquant\n\n *This post was mass deleted and anonymized with [Redact](https://redact.dev)*",
        "score": 2,
        "created_utc": 1727637618.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t3_1fs96s0",
        "depth": 0
      },
      {
        "id": "lq1qci8",
        "body": "Maybe post a link to the map and not just an image. Assuming it is a web map then the data is coming in to the browser",
        "score": 1,
        "created_utc": 1727907055.0,
        "author": "dtdv",
        "is_submitter": false,
        "parent_id": "t3_1fs96s0",
        "depth": 0
      },
      {
        "id": "lpq57i7",
        "body": "Thank you!",
        "score": 2,
        "created_utc": 1727736847.0,
        "author": null,
        "is_submitter": false,
        "parent_id": "t1_lpjcw8m",
        "depth": 1
      }
    ],
    "comments_extracted": 3
  },
  {
    "id": "1fs12jz",
    "title": "Help Regarding FYP",
    "selftext": "Hello everyone, I am a senior at my suniversity starting my Final Year Project. The topic I'm considering revolves around measuring the value of data and finding a way to enrich said data's value particularly in the agricultural sector. However I'm still trying to find my bearing. Any help regarding the project or recommendation of sources that kight be helpful will be greatly appreciated. TIA",
    "url": "https://www.reddit.com/r/data/comments/1fs12jz/help_regarding_fyp/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1727604497.0,
    "author": "Tolstoy6",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fs12jz/help_regarding_fyp/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lph5cz3",
        "body": "Find some datasets to work with. Doing a quick google search for \"agriculture datasets\" brings up some data you can download from the USDA website. There might be something in that data you can analyze and extrapolate on.",
        "score": 1,
        "created_utc": 1727608643.0,
        "author": "ivilkee",
        "is_submitter": false,
        "parent_id": "t3_1fs12jz",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  },
  {
    "id": "1fqfbmf",
    "title": "News Networks - Distribution of topics",
    "selftext": "I’ve started wondering about the breakdown of topics reported by networks/shows, and how it’s changed over time. I did an initial Googling, but didn’t find anything recent… the research/reporting right now seems to be on the source of news, not necessarily the topics. Anyone know of any quality data on this? Or a better place to look? It’s just for funsies, nothing academic or professional. Prompted by struggling to find news coverage of the hurricane tonight, noticing my usual channels are only showing political news these days.",
    "url": "https://www.reddit.com/r/data/comments/1fqfbmf/news_networks_distribution_of_topics/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1727410066.0,
    "author": "Thiseffingguy2",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fqfbmf/news_networks_distribution_of_topics/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1fpsu52",
    "title": "Learn data with a peer",
    "selftext": "Hello,\n\nI intend to start learning data tools and i was thinking it would be better to do so with a friend.\n\nI wont start from scratch as i already code in python and have a significant xp in sql.\n\nAnyone interested ? The idea is to learn together, exchange tricks ideas and tricks..\n",
    "url": "https://www.reddit.com/r/data/comments/1fpsu52/learn_data_with_a_peer/",
    "score": 5,
    "upvote_ratio": 0.86,
    "num_comments": 7,
    "created_utc": 1727345952.0,
    "author": "almostlowcostman",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fpsu52/learn_data_with_a_peer/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lp53dxv",
        "body": "I’m interested, dm me, I have some knowledge in data I won’t be starting from scratch either.",
        "score": 1,
        "created_utc": 1727413953.0,
        "author": "KatCelest",
        "is_submitter": false,
        "parent_id": "t3_1fpsu52",
        "depth": 0
      },
      {
        "id": "lpcjqtk",
        "body": "I'm up for it",
        "score": 1,
        "created_utc": 1727535732.0,
        "author": "Odd-Employ-7127",
        "is_submitter": false,
        "parent_id": "t3_1fpsu52",
        "depth": 0
      },
      {
        "id": "lpcm5pj",
        "body": "R we going steady",
        "score": 1,
        "created_utc": 1727536580.0,
        "author": "Odd-Employ-7127",
        "is_submitter": false,
        "parent_id": "t3_1fpsu52",
        "depth": 0
      },
      {
        "id": "lp7qcno",
        "body": "Done",
        "score": 1,
        "created_utc": 1727457736.0,
        "author": "almostlowcostman",
        "is_submitter": true,
        "parent_id": "t1_lp53dxv",
        "depth": 1
      },
      {
        "id": "lphhi9i",
        "body": "Could i join you as well",
        "score": 1,
        "created_utc": 1727614912.0,
        "author": "Retardedmanager",
        "is_submitter": false,
        "parent_id": "t1_lp7qcno",
        "depth": 2
      },
      {
        "id": "lq3qmth",
        "body": "Would love to join in learning data as I am strengthening and filling gaps in my knowledge especially in analytics using python",
        "score": 1,
        "created_utc": 1727940399.0,
        "author": "Numerous_1m",
        "is_submitter": false,
        "parent_id": "t1_lphhi9i",
        "depth": 3
      }
    ],
    "comments_extracted": 6
  },
  {
    "id": "1fpwwbl",
    "title": "Idiot trying to self-educate to finish a project",
    "selftext": "Hi all,\n\nI'm looking into how to create a relationship database using excel, spite, and about 180-200 different groups. After reaching out to a few professors, l've been told the most efficient thing I should be doing instead is create an \"edge list\".\n\nProblem is, I barely know what means after 2 days of looking into it and my sociogram would need 2 weight values as these relationships between groups are either very one-sided (i.e. either someone hates someone else who likes them in turn OR there's a clearly defined relationship dynamic but it's weighted at \"O\" on my scale to indicate how it's totally unknown what the reciprocated opinion/ relationship stance is).\n\nThere's also the issue that I believe I'd need to make another similar matrix to highlight how members have switched over to other groups, stolen from someone, or even just if they have a business relationship either as a supplier, distributor, or client.\n\nPlease help. I don't even know what software I should be picking, I'm just using Gephi because it was free and there's a small online textbook I found with labs.",
    "url": "https://www.reddit.com/r/data/comments/1fpwwbl/idiot_trying_to_selfeducate_to_finish_a_project/",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1727359254.0,
    "author": "SarcasticJackass177",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fpwwbl/idiot_trying_to_selfeducate_to_finish_a_project/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1fptlmu",
    "title": "A list of all available pronouns for instagram",
    "selftext": "Just thought this might fit here, if not just remove it please.\nFeel free to adjust or extend my list, i'd be glad to see more words/phrases 😁",
    "url": "https://www.reddit.com/u/Daniel0210/s/GsUNajHcOX",
    "score": 1,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1727349038.0,
    "author": "Daniel0210",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fptlmu/a_list_of_all_available_pronouns_for_instagram/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1fpbhfr",
    "title": "As an active data analyst job-seeker, this made me cackle. I might adjust my approach to job applications & write a SQL version of my next cover letter lol (not my OC).",
    "selftext": "Job a",
    "url": "https://i.redd.it/1samno4vzzqd1.jpeg",
    "score": 23,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "created_utc": 1727288613.0,
    "author": "PuzzleheadedAsk6787",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fpbhfr/as_an_active_data_analyst_jobseeker_this_made_me/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lowud41",
        "body": "If you can inject enough SQL into your resume upload, you just may be able to convince the ATS to straight out offer you the job. Lol.",
        "score": 6,
        "created_utc": 1727295526.0,
        "author": "space-ish",
        "is_submitter": false,
        "parent_id": "t3_1fpbhfr",
        "depth": 0
      },
      {
        "id": "lp15t07",
        "body": "HR or AI will send this straight to junk mail before anyone who understands it will ever see it.",
        "score": 3,
        "created_utc": 1727363838.0,
        "author": "Hotel_Joy",
        "is_submitter": false,
        "parent_id": "t3_1fpbhfr",
        "depth": 0
      },
      {
        "id": "lp3u7eh",
        "body": "A genius idea like this must be sent to the hiring manager directly! Maybe attached the query result with it, a table view of a resume might be rhetorical ULTIMATE resume… I doff my cap to you, good person..",
        "score": 2,
        "created_utc": 1727395173.0,
        "author": "Salty-Necessary-7302",
        "is_submitter": false,
        "parent_id": "t3_1fpbhfr",
        "depth": 0
      },
      {
        "id": "lp16mgz",
        "body": "😂",
        "score": 1,
        "created_utc": 1727364095.0,
        "author": "PuzzleheadedAsk6787",
        "is_submitter": true,
        "parent_id": "t1_lowud41",
        "depth": 1
      }
    ],
    "comments_extracted": 4
  },
  {
    "id": "1fpmgxt",
    "title": "Documentation hard/software",
    "selftext": "I understand this may not be the best thread, but for the potion on metadata, and also, simply trying to orginize a high volume of content,  I figure it maybe beneficial to reach out here.\n\nGoal:  Mobile, Lightweight and frictionless (process) dor documentation, expression and story telling.\n\nDetails:  I am looking, effectively for a cheap light weight suite of equipment and software for documentation.  (Days, routines, thoughts, ideas, data for measuring/tracking, etc. . .)  Preferred to be based around my phone (Samsung) to keep things cheap and light.  \n\nBudget $100.\n\nThings in mind:\n- Divinchie resolve (desktop editor) (free)\n- Notion (logging) (free)\n- Google keep notes (quick capture (text)) (free)\n- kinmaster (mobile video edits) ($?)\n- \n\n\nA fast note list below:\n\nEdc phone vlog kit:\n- tri/mono pod (flex/grip legs?) ($20?)\n- light ($25?)\n- mic (s? $?)\n- . . .\n\nMedia, Back ups, edits, transfers:\n- back up option (software/hardware)\n- simple fast video edits\n- top hard/software to transfer phone -> desktop\n- \n\nOther:\n- gen automation:\n- - Tagging, metadata, transcribe, group/album, media,\n- capture software\n- - Photo\n- - Video\n- - Audio (transcribe, summary, clean audio)\n- - - Audio saved to podcasting software (making easy to access, functions as a back up, and gives \"play\" features such as speed, cut silences etc. . .)\n- - Text (good formatting + speech to text)\n// ability to capture all via 1 software?\n",
    "url": "https://www.reddit.com/r/data/comments/1fpmgxt/documentation_hardsoftware/",
    "score": 3,
    "upvote_ratio": 1.0,
    "num_comments": 0,
    "created_utc": 1727319425.0,
    "author": "Kaiser_design",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fpmgxt/documentation_hardsoftware/",
    "is_self": true,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [],
    "comments_extracted": 0
  },
  {
    "id": "1fpe3y2",
    "title": "August 2024 ADU and Solar Trends: ADU permitting had positive 32% YoY growth and Solar had negative 22% YoY growth",
    "selftext": "",
    "url": "https://www.reddit.com/gallery/1fpe3y2",
    "score": 2,
    "upvote_ratio": 1.0,
    "num_comments": 1,
    "created_utc": 1727295261.0,
    "author": "buildzoom_data",
    "subreddit": "data",
    "permalink": "/r/data/comments/1fpe3y2/august_2024_adu_and_solar_trends_adu_permitting/",
    "is_self": false,
    "distinguished": null,
    "stickied": false,
    "over_18": false,
    "spoiler": false,
    "locked": false,
    "comments": [
      {
        "id": "lowtksh",
        "body": "More visualizations from in-house building permit data during the month of August. Permit activity for ADUs remained the same between July and August 2024 but still saw a positive increase over last year, with 32% growth. However, compared to previous months, the rate of growth is decreasing. For reference, year-over-year growth was 46% and 40% in June and July, respectively.\n\nSolar installation activity decreased between July and August. Year-over-year growth was negative as well, though the extent of this year-over-year growth decline continues to slow down.",
        "score": 1,
        "created_utc": 1727295284.0,
        "author": "buildzoom_data",
        "is_submitter": true,
        "parent_id": "t3_1fpe3y2",
        "depth": 0
      }
    ],
    "comments_extracted": 1
  }
]